# Nginx èˆ‡ FastAPI é…ç½®æŒ‡å—
  
## ç›®éŒ„
- [æ¶æ§‹æ¦‚è¿°](#æ¶æ§‹æ¦‚è¿° )
- [Nginx é…ç½®è©³è§£](#nginx-é…ç½®è©³è§£ )
- [è² è¼‰å‡è¡¡æ©Ÿåˆ¶](#è² è¼‰å‡è¡¡æ©Ÿåˆ¶ )
- [API ç«¯é»æ˜ å°„](#api-ç«¯é»æ˜ å°„ )
- [FastAPI æ‡‰ç”¨è¨­ç½®](#fastapi-æ‡‰ç”¨è¨­ç½® )
- [FastAPI è©³ç´°é…ç½®èˆ‡å¯¦ä½œ](#fastapi-è©³ç´°é…ç½®èˆ‡å¯¦ä½œ )
- [Redis å®Œæ•´é…ç½®èˆ‡æ¶æ§‹](#redis-å®Œæ•´é…ç½®èˆ‡æ¶æ§‹ )
  - [Redis ä¼ºæœå™¨é…ç½®](#1-redis-ä¼ºæœå™¨é…ç½® )
  - [Redis æŒä¹…åŒ–ç­–ç•¥](#2-redis-æŒä¹…åŒ–ç­–ç•¥-aof )
  - [Redis è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥](#3-redis-è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥ )
  - [Redis é€£ç·šæ± é…ç½®](#4-redis-é€£ç·šæ± é…ç½® )
  - [Redis Stream è©³ç´°æ©Ÿåˆ¶](#5-redis-stream-è©³ç´°æ©Ÿåˆ¶ )
  - [Redis å¿«å–å±¤è©³è§£](#6-redis-å¿«å–å±¤è©³è§£ )
  - [Redis é›™é‡è§’è‰²](#7-redis-é›™é‡è§’è‰²stream-vs-cache )
  - [Redis èˆ‡ PostgreSQL å”ä½œæ¨¡å¼](#8-redis-èˆ‡-postgresql-å”ä½œæ¨¡å¼ )
- [Worker å·¥ä½œæµç¨‹](#worker-å·¥ä½œæµç¨‹ )
- [Worker è©³ç´°å¯¦ä½œ](#worker-è©³ç´°å¯¦ä½œ )
- [Worker å®Œæ•´ç”Ÿå‘½é€±æœŸ](#worker-å®Œæ•´ç”Ÿå‘½é€±æœŸ )
- [FastAPI èˆ‡ Worker å”ä½œæ©Ÿåˆ¶](#fastapi-èˆ‡-worker-å”ä½œæ©Ÿåˆ¶ )
- [è«‹æ±‚è™•ç†æµç¨‹](#è«‹æ±‚è™•ç†æµç¨‹ )
- [æ‰¹é‡è™•ç†èˆ‡æ•ˆèƒ½å„ªåŒ–](#æ‰¹é‡è™•ç†èˆ‡æ•ˆèƒ½å„ªåŒ– )
- [æ€§èƒ½å„ªåŒ–é…ç½®](#æ€§èƒ½å„ªåŒ–é…ç½® )
- [å¥åº·æª¢æŸ¥èˆ‡ç›£æ§](#å¥åº·æª¢æŸ¥èˆ‡ç›£æ§ )
- [æ“´å±•èˆ‡ç¶­è­·](#æ“´å±•èˆ‡ç¶­è­· )
- [ç¸½çµ](#ç¸½çµ )
  
## æ¶æ§‹æ¦‚è¿°
  
æœ¬æ—¥èªŒæ”¶é›†ç³»çµ±æ¡ç”¨ Nginx ä½œç‚ºåå‘ä»£ç†å’Œè² è¼‰å‡è¡¡å™¨ï¼Œå¾Œç«¯éƒ¨ç½²å¤šå€‹ FastAPI å¯¦ä¾‹ä¾†è™•ç†æ—¥èªŒæ”¶é›†è«‹æ±‚ï¼Œä¸¦ä½¿ç”¨ç¨ç«‹çš„ Worker æœå‹™å°‡æ—¥èªŒéåŒæ­¥è™•ç†ä¸¦æŒä¹…åŒ–åˆ° PostgreSQL æ•¸æ“šåº«ã€‚æ•´é«”æ¶æ§‹å¦‚ä¸‹ï¼š
  
```
å¤–éƒ¨è«‹æ±‚ â†’ Nginx (è² è¼‰å‡è¡¡) â†’ FastAPI å¯¦ä¾‹ 1 â†’ Redis (éšŠåˆ—) â†’ Worker â†’ PostgreSQL
                            â†’ FastAPI å¯¦ä¾‹ 2                        â†’ PostgreSQL
                            â†’ ... (æ›´å¤šå¯¦ä¾‹)
```
  
ä»¥ä¸‹æ˜¯ç³»çµ±æ¶æ§‹çš„è©³ç´°åœ–ç¤ºï¼š
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client        â”‚    â”‚    Nginx        â”‚    â”‚  Redis (Queue)  â”‚
â”‚   Requests      â”‚â—„â”€â”€â–ºâ”‚   (Reverse      â”‚â—„â”€â”€â–ºâ”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Proxy)        â”‚    â”‚                 â”‚
                       â”‚                 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                              â”‚                         â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                    â”‚         â”‚         â”‚               â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚  FastAPI    â”‚  â”‚  â”‚  FastAPI    â”‚    â”‚   Worker    â”‚
             â”‚  Instance 1 â”‚  â”‚  â”‚  Instance 2 â”‚    â”‚             â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
                    â”‚ Storage Layer   â”‚                   â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                   â”‚
                    â”‚  â”‚ PostgreSQL â”‚ â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
## Nginx é…ç½®è©³è§£
  
### 1. åŸºæœ¬é…ç½®
  
```nginx
events {
    worker_connections 4096;    # æå‡ä»¥æ”¯æ´æ›´é«˜ä¸¦ç™¼
}
```
  
- `worker_connections`: è¨­å®šæ¯å€‹ worker é€²ç¨‹å¯è™•ç†çš„æœ€å¤§é€£ç·šæ•¸ï¼Œå¾é»˜èªçš„ 1024 æå‡è‡³ 4096 ä»¥æ”¯æ´é«˜ä½µç™¼è«‹æ±‚ã€‚
  
### 2. ä¸Šæ¸¸æœå‹™é…ç½® (Upstream)
  
```nginx
upstream fastapi_backend {
    least_conn;  # æœ€å°‘é€£ç·šæ•¸æ¼”ç®—æ³•ï¼ˆé©åˆé•·é€£ç·šï¼‰
  
    server fastapi-1:8000 weight=1 max_fails=3 fail_timeout=30s;
    server fastapi-2:8000 weight=1 max_fails=3 fail_timeout=30s;
  
    keepalive 128;  # æå‡é€£ç·šæ± ä»¥æ”¯æ´æ›´é«˜ä¸¦ç™¼
}
```
  
- `least_conn`: ä½¿ç”¨æœ€å°‘é€£ç·šæ•¸ç®—æ³•ï¼Œé©åˆé•·é€£ç·šå ´æ™¯
- `server`: å®šç¾©å¾Œç«¯ FastAPI æœå‹™åœ°å€å’Œç«¯å£
- `weight`: æœå‹™å™¨æ¬Šé‡ï¼ˆé»˜èªç‚º 1ï¼‰
- `max_fails` å’Œ `fail_timeout`: å®¹éŒ¯é…ç½®
- `keepalive`: HTTP é€£ç·šæ± å¤§å°ï¼Œæå‡é€£ç·šè¤‡ç”¨æ•ˆç‡
  
### 3. æ—¥èªŒæ ¼å¼é…ç½®
  
```nginx
log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                '$status $body_bytes_sent "$http_referer" '
                '"$http_user_agent" upstream: $upstream_addr '
                'response_time: $upstream_response_time';
```
  
- è¨˜éŒ„è©³ç´°çš„è«‹æ±‚è³‡è¨Šï¼ŒåŒ…æ‹¬ä¸Šæ¸¸æœå‹™å™¨åœ°å€å’ŒéŸ¿æ‡‰æ™‚é–“ï¼Œä¾¿æ–¼æ€§èƒ½åˆ†æå’Œå•é¡Œæ’æŸ¥ã€‚
  
### 4. é™æµé…ç½®
  
```nginx
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10000r/s;
limit_conn_zone $binary_remote_addr zone=conn_limit:10m;
```
  
- `rate=10000r/s`: æ¯ç§’æœ€å¤šè™•ç† 10,000 å€‹è«‹æ±‚ï¼ˆç‚ºå£“åŠ›æ¸¬è©¦èª¿é«˜ï¼‰
- `zone=conn_limit`: é™åˆ¶å–®å€‹ IP çš„é€£ç·šæ•¸
  
## è² è¼‰å‡è¡¡æ©Ÿåˆ¶
  
### æ ¸å¿ƒé…ç½®è§£æ
  
æœ¬ç³»çµ±çš„è² è¼‰å‡è¡¡æ ¸å¿ƒé…ç½®ä½æ–¼ `nginx/nginx.conf` çš„ upstream å€å¡Šï¼š
  
```nginx
upstream fastapi_backend {
    least_conn;  # è² è¼‰å‡è¡¡æ¼”ç®—æ³•
  
    server fastapi-1:8000 weight=1 max_fails=3 fail_timeout=30s;
    server fastapi-2:8000 weight=1 max_fails=3 fail_timeout=30s;
  
    keepalive 128;  # é€£ç·šæ± å¤§å°
}
```
  
### è² è¼‰å‡è¡¡æ¼”ç®—æ³•è©³è§£
  
#### 1. Least Connections (æœ€å°‘é€£ç·šæ•¸) æ¼”ç®—æ³•
  
Nginx ä½¿ç”¨ `least_conn` æŒ‡ä»¤å¯¦ç¾æœ€å°‘é€£ç·šæ•¸æ¼”ç®—æ³•ï¼š
  
**å·¥ä½œåŸç†ï¼š**
```
è«‹æ±‚åˆ°é” â†’ æª¢æŸ¥æ‰€æœ‰å¾Œç«¯æœå‹™å™¨çš„æ´»èºé€£ç·šæ•¸ â†’ é¸æ“‡é€£ç·šæ•¸æœ€å°‘çš„æœå‹™å™¨ â†’ è½‰ç™¼è«‹æ±‚
```
  
**å…·é«”æµç¨‹ï¼š**
1. ç•¶æ–°è«‹æ±‚åˆ°é”æ™‚ï¼ŒNginx æª¢æŸ¥ upstream æ± ä¸­æ‰€æœ‰æœå‹™å™¨
2. è¨ˆç®—æ¯å€‹æœå‹™å™¨çš„ç•¶å‰æ´»èºé€£ç·šæ•¸ï¼ˆè€ƒæ…®æ¬Šé‡ï¼‰
3. é¸æ“‡ `active_connections / weight` å€¼æœ€å°çš„æœå‹™å™¨
4. å°‡è«‹æ±‚è½‰ç™¼åˆ°è©²æœå‹™å™¨
  
**ç‚ºä½•é¸æ“‡æ­¤æ¼”ç®—æ³•ï¼š**
- é©åˆ**é•·é€£ç·šå ´æ™¯**ï¼šæ—¥èªŒç³»çµ±å¯èƒ½æœ‰æŒä¹…é€£ç·š
- **å‹•æ…‹è² è¼‰åˆ†é…**ï¼šæ ¹æ“šå¯¦éš›è² è¼‰è€Œéå›ºå®šé †åºåˆ†é…
- **é¿å…ä¸å‡å‹»åˆ†ä½ˆ**ï¼šé˜²æ­¢æŸå€‹å¯¦ä¾‹éè¼‰è€Œå…¶ä»–é–’ç½®
  
**èˆ‡å…¶ä»–æ¼”ç®—æ³•æ¯”è¼ƒï¼š**
| æ¼”ç®—æ³• | ç‰¹é» | é©ç”¨å ´æ™¯ |
|--------|------|----------|
| `round_robin` (é»˜èª) | è¼ªè©¢åˆ†é… | è«‹æ±‚è™•ç†æ™‚é–“ç›¸è¿‘ |
| `least_conn` | æœ€å°‘é€£ç·šå„ªå…ˆ | è«‹æ±‚è™•ç†æ™‚é–“å·®ç•°å¤§ã€é•·é€£ç·š |
| `ip_hash` | åŒä¸€ IP å›ºå®šå¾Œç«¯ | éœ€è¦æœƒè©±ä¿æŒ |
| `random` | éš¨æ©Ÿé¸æ“‡ | å‡è¡¡åˆ†ä½ˆ |
  
#### 2. æ¬Šé‡é…ç½® (Weight)
  
```nginx
server fastapi-1:8000 weight=1;
server fastapi-2:8000 weight=1;
```
  
- ç•¶å‰é…ç½®ï¼šå…©å€‹å¯¦ä¾‹æ¬Šé‡ç›¸åŒï¼ˆ1:1ï¼‰
- **è¨ˆç®—æ–¹å¼**ï¼šå¯¦éš›è² è¼‰ = é€£ç·šæ•¸ / æ¬Šé‡
- **æ“´å±•ç¤ºä¾‹**ï¼šè‹¥è¨­ç½® `weight=2`ï¼Œè©²æœå‹™å™¨å°‡æ‰¿æ“”é›™å€æµé‡
  
### è«‹æ±‚åˆ†ç™¼æµç¨‹
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å®¢æˆ¶ç«¯è«‹æ±‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Nginx è² è¼‰å‡è¡¡å™¨                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚        1. æ¥æ”¶è«‹æ±‚                    â”‚    â”‚
â”‚  â”‚           â†“                          â”‚    â”‚
â”‚  â”‚        2. è·¯ç”±åŒ¹é…                    â”‚    â”‚
â”‚  â”‚        (location /api/log)           â”‚    â”‚
â”‚  â”‚           â†“                          â”‚    â”‚
â”‚  â”‚        3. é™æµæª¢æŸ¥                    â”‚    â”‚
â”‚  â”‚        (rate=10000r/s)               â”‚    â”‚
â”‚  â”‚           â†“                          â”‚    â”‚
â”‚  â”‚        4. é€£ç·šæ•¸è¨ˆç®—                   â”‚    â”‚
â”‚  â”‚        fastapi-1: 45 é€£ç·š            â”‚    â”‚
â”‚  â”‚        fastapi-2: 38 é€£ç·š âœ“           â”‚    â”‚
â”‚  â”‚           â†“                          â”‚    â”‚
â”‚  â”‚        5. é¸æ“‡æœ€å°‘é€£ç·šæœå‹™å™¨            â”‚    â”‚
â”‚  â”‚        (fastapi-2)                   â”‚    â”‚
â”‚  â”‚           â†“                          â”‚    â”‚
â”‚  â”‚        6. è½‰ç™¼è«‹æ±‚                    â”‚    â”‚
â”‚  â”‚        proxy_pass â†’ fastapi-2:8000   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI-2   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
### å®¹éŒ¯èˆ‡æ•…éšœè½‰ç§»æ©Ÿåˆ¶
  
```nginx
server fastapi-1:8000 weight=1 max_fails=3 fail_timeout=30s;
```
  
**åƒæ•¸è§£æï¼š**
- **`max_fails=3`**: åœ¨ fail_timeout æ™‚é–“å…§å…è¨±çš„æœ€å¤§å¤±æ•—æ¬¡æ•¸
- **`fail_timeout=30s`**:
  - çµ±è¨ˆå¤±æ•—æ¬¡æ•¸çš„æ™‚é–“çª—å£ï¼ˆ30 ç§’ï¼‰
  - æœå‹™å™¨è¢«æ¨™è¨˜ç‚ºä¸å¯ç”¨å¾Œçš„æ¢å¾©ç­‰å¾…æ™‚é–“
  
**æ•…éšœæª¢æ¸¬æµç¨‹ï¼š**
```
æ™‚é–“ç·šï¼šT=0s
â”œâ”€ è«‹æ±‚ 1 â†’ fastapi-1 â†’ å¤±æ•— (fail_count=1)
â”œâ”€ è«‹æ±‚ 2 â†’ fastapi-1 â†’ å¤±æ•— (fail_count=2)
â”œâ”€ è«‹æ±‚ 3 â†’ fastapi-1 â†’ å¤±æ•— (fail_count=3)
â”‚
â”œâ”€ T=0.5s: fastapi-1 æ¨™è¨˜ç‚º DOWN
â”‚  (30 ç§’å…§å¤±æ•— 3 æ¬¡)
â”‚
â”œâ”€ T=0.5s ~ T=30.5s:
â”‚  æ‰€æœ‰è«‹æ±‚è½‰ç™¼è‡³ fastapi-2
â”‚
â”œâ”€ T=30.5s: fastapi-1 æ¢å¾©ç‚º UP
â”‚  é‡æ–°åŠ å…¥è² è¼‰å‡è¡¡æ± 
â”‚
â””â”€ ç¹¼çºŒç›£æ§...
```
  
**è‡ªå‹•æ¢å¾©ï¼š**
- 30 ç§’å¾Œ Nginx è‡ªå‹•å˜—è©¦å°‡æ•…éšœæœå‹™å™¨é‡æ–°åŠ å…¥æ± ä¸­
- å¦‚æœæœå‹™å™¨å·²æ¢å¾©ï¼Œå‰‡æ­£å¸¸æ¥æ”¶æµé‡
- å¦‚æœä»ç„¶æ•…éšœï¼Œé‡æ–°é–‹å§‹æ•…éšœè¨ˆæ•¸
  
### é€£ç·šæ± èˆ‡æŒä¹…é€£ç·š
  
```nginx
upstream fastapi_backend {
    ...
    keepalive 128;
}
  
location /api/log {
    proxy_http_version 1.1;
    proxy_set_header Connection "";
    ...
}
```
  
**é€£ç·šæ± å·¥ä½œåŸç†ï¼š**
  
```
ä¸ä½¿ç”¨é€£ç·šæ± ï¼š
Client â†’ Nginx â†’ [å»ºç«‹ TCP é€£ç·š] â†’ FastAPI â†’ [é—œé–‰é€£ç·š]
Client â†’ Nginx â†’ [å»ºç«‹ TCP é€£ç·š] â†’ FastAPI â†’ [é—œé–‰é€£ç·š]
(æ¯æ¬¡è«‹æ±‚éƒ½å»ºç«‹æ–°é€£ç·šï¼Œé–‹éŠ·å¤§)
  
ä½¿ç”¨é€£ç·šæ±  (keepalive 128)ï¼š
Client â†’ Nginx â†’ [å¾©ç”¨ç¾æœ‰é€£ç·š] â†’ FastAPI
                 â†‘ é€£ç·šæ±  (æœ€å¤š 128 å€‹ç©ºé–’é€£ç·š)
Client â†’ Nginx â†’ [å¾©ç”¨ç¾æœ‰é€£ç·š] â†’ FastAPI
(é€£ç·šé‡ç”¨ï¼Œæ¸›å°‘ TCP æ¡æ‰‹é–‹éŠ·)
```
  
**é—œéµé…ç½®èªªæ˜ï¼š**
  
1. **`keepalive 128`**:
   - æ¯å€‹ worker é€²ç¨‹ç¶­è­·æœ€å¤š 128 å€‹ç©ºé–’é€£ç·š
   - è¶…éæ­¤æ•¸é‡çš„ç©ºé–’é€£ç·šå°‡è¢«é—œé–‰
   - æœ‰æ•ˆæ¸›å°‘é€£ç·šå»ºç«‹é–‹éŠ·
  
2. **`proxy_http_version 1.1`**:
   - ä½¿ç”¨ HTTP/1.1 å”è­°ï¼ˆæ”¯æ´æŒä¹…é€£ç·šï¼‰
   - é»˜èªçš„ HTTP/1.0 ä¸æ”¯æ´ keepalive
  
3. **`proxy_set_header Connection ""`**:
   - æ¸…é™¤å®¢æˆ¶ç«¯çš„ Connection é ­
   - é˜²æ­¢å®¢æˆ¶ç«¯çš„ `Connection: close` é—œé–‰å¾Œç«¯é€£ç·š
   - ç¢ºä¿ Nginx åˆ°å¾Œç«¯çš„é€£ç·šä¿æŒæ´»èº
  
**æ€§èƒ½å½±éŸ¿ï¼š**
- æ¸›å°‘ TCP ä¸‰æ¬¡æ¡æ‰‹å»¶é²ï¼ˆç´„ 1-2ms/è«‹æ±‚ï¼‰
- é™ä½ç³»çµ±è³‡æºæ¶ˆè€—ï¼ˆæ¸›å°‘ TIME_WAIT ç‹€æ…‹é€£ç·šï¼‰
- æå‡é«˜ä½µç™¼å ´æ™¯ä¸‹çš„ååé‡
  
### è² è¼‰å‡è¡¡ç‹€æ…‹ç›£æ§
  
é€šéæ—¥èªŒæ ¼å¼è¿½è¹¤è² è¼‰åˆ†ä½ˆï¼š
  
```nginx
log_format main '... upstream: $upstream_addr response_time: $upstream_response_time';
```
  
**ç›£æ§æŒ‡æ¨™ï¼š**
- `<img src="https://latex.codecogs.com/gif.latex?upstream_addr`:%20è¨˜éŒ„å¯¦éš›è™•ç†è«‹æ±‚çš„å¾Œç«¯æœå‹™å™¨-%20`"/>upstream_response_time`: å¾Œç«¯éŸ¿æ‡‰æ™‚é–“
  
**åˆ†æç¤ºä¾‹ï¼š**
```bash
# æŸ¥çœ‹è² è¼‰åˆ†ä½ˆ
grep "upstream:" /var/log/nginx/access.log | awk '{print $NF}' | sort | uniq -c
  
# è¼¸å‡ºç¤ºä¾‹ï¼š
#   5023 fastapi-1:8000
#   4977 fastapi-2:8000
# (æ¥è¿‘ 1:1 åˆ†ä½ˆï¼Œè¡¨ç¤ºè² è¼‰å‡è¡¡æ­£å¸¸)
```
  
### å‹•æ…‹æ“´å±•å¾Œç«¯æœå‹™å™¨
  
**æ·»åŠ æ–°å¯¦ä¾‹æ­¥é©Ÿï¼š**
  
1. **æ›´æ–° docker-compose.yml**:
```yaml
fastapi-3:
  build:
    context: ./app
    dockerfile: Dockerfile
  container_name: log-fastapi-3
  command: uvicorn main:app --host 0.0.0.0 --port 8000 --workers 6
  environment:
    - INSTANCE_NAME=fastapi-3
    ...
```
  
2. **æ›´æ–° nginx.conf**:
```nginx
upstream fastapi_backend {
    least_conn;
    server fastapi-1:8000 weight=1 max_fails=3 fail_timeout=30s;
    server fastapi-2:8000 weight=1 max_fails=3 fail_timeout=30s;
    server fastapi-3:8000 weight=1 max_fails=3 fail_timeout=30s;  # æ–°å¢
    keepalive 128;
}
```
  
3. **é‡æ–°è¼‰å…¥é…ç½®**:
```bash
docker-compose up -d fastapi-3
docker-compose exec nginx nginx -s reload
```
  
**å„ªé»ï¼š**
- ç„¡éœ€é‡å•Ÿæ•´å€‹æœå‹™
- é›¶åœæ©Ÿæ™‚é–“æ“´å±•
- æ–°å¯¦ä¾‹è‡ªå‹•åŠ å…¥è² è¼‰å‡è¡¡æ± 
  
## API ç«¯é»æ˜ å°„
  
### æ—¥èªŒå¯«å…¥ç«¯é»
  
```nginx
location /api/log {
    limit_req zone=api_limit burst=20000 nodelay;
    limit_conn conn_limit 1000;
  
    proxy_pass http://fastapi_backend;
    proxy_http_version 1.1;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
}
```
  
- **åŠŸèƒ½**: æ¥æ”¶å–®å€‹æ—¥èªŒæ¢ç›®
- **é™æµ**: æ¯ç§’ 10,000 è«‹æ±‚ï¼Œçªç™¼ 20,000
- **è¶…æ™‚**: é€£ç·š 5sï¼Œç™¼é€/è®€å–å„ 10s
  
### æ‰¹é‡æ—¥èªŒç«¯é»
  
```nginx
location /api/logs/batch {
    limit_req zone=api_limit burst=20000 nodelay;
    limit_conn conn_limit 1000;
  
    proxy_pass http://fastapi_backend;
    client_max_body_size 50M;  # æ”¯æ´è¼ƒå¤§è«‹æ±‚é«”
}
```
  
- **åŠŸèƒ½**: æ‰¹é‡æ¥æ”¶æ—¥èªŒæ¢ç›®
- **è¼‰é«”é™åˆ¶**: 50MBï¼ˆæ¯”æ™®é€šç«¯é»æ›´å¤§ï¼‰
- **è¶…æ™‚**: æ›´é•·çš„è®€å¯«è¶…æ™‚ï¼ˆ30sï¼‰
  
### æŸ¥è©¢ç«¯é»
  
```nginx
location /api/logs {
    limit_req zone=api_limit burst=200 nodelay;
    proxy_pass http://fastapi_backend;
    proxy_read_timeout 30s;  # è¼ƒé•·çš„è®€å–è¶…æ™‚
}
```
  
- **åŠŸèƒ½**: æŸ¥è©¢æ—¥èªŒè³‡æ–™
- **é™æµ**: è¼ƒä¿å®ˆï¼ˆburst=200ï¼‰ï¼Œé¿å…æŸ¥è©¢å°ç³»çµ±é€ æˆéå¤§å£“åŠ›
- **è¶…æ™‚**: 30sï¼Œå…è¨±è¤‡é›œæŸ¥è©¢
  
### çµ±è¨ˆç«¯é»
  
```nginx
location /api/stats {
    limit_req zone=api_limit burst=200 nodelay;
    proxy_pass http://fastapi_backend;
    proxy_read_timeout 30s;
}
```
  
- **åŠŸèƒ½**: ç²å–ç³»çµ±çµ±è¨ˆè³‡è¨Š
- **å¿«å–**: FastAPI å…§éƒ¨ä½¿ç”¨ Redis å¿«å–çµæœ
  
### æ–‡ä»¶ç«¯é»
  
```nginx
location /docs {
    proxy_pass http://fastapi_backend/docs;
}
location /openapi.json {
    proxy_pass http://fastapi_backend/openapi.json;
}
location /redoc {
    proxy_pass http://fastapi_backend/redoc;
}
```
  
- **åŠŸèƒ½**: æä¾› FastAPI è‡ªå‹•ç”Ÿæˆçš„ API æ–‡ä»¶
  
## FastAPI æ‡‰ç”¨è¨­ç½®
  
### Docker Compose é…ç½®
  
```yaml
fastapi-1:
  build:
    context: ./app
    dockerfile: Dockerfile
  command: uvicorn main:app --host 0.0.0.0 --port 8000 --workers 6  # æå‡ workers æ•¸é‡
  environment:
    - POSTGRES_HOST=postgres
    - POSTGRES_PORT=5432
    - POSTGRES_USER=loguser
    - POSTGRES_PASSWORD=logpass
    - POSTGRES_DB=logsdb
    - REDIS_HOST=redis
    - REDIS_PORT=6379
    - INSTANCE_NAME=fastapi-1
```
  
- `--workers 6`: å•Ÿå‹• 6 å€‹å·¥ä½œé€²ç¨‹ä»¥è™•ç†æ›´å¤šä½µç™¼è«‹æ±‚
- `INSTANCE_NAME`: ç”¨æ–¼å€åˆ†ä¸åŒå¯¦ä¾‹çš„ç’°å¢ƒè®Šé‡
- æœå‹™å®¹å™¨åç¨±ç‚º `fastapi-1` å’Œ `fastapi-2`ï¼Œå°æ‡‰ nginx upstream ä¸­çš„æœå‹™å™¨å®šç¾©
  
### FastAPI æ‡‰ç”¨ç‰¹æ€§
  
- æ”¯æ´ async/await éåŒæ­¥è™•ç†
- ä½¿ç”¨ Redis ä½œç‚ºæ—¥èªŒéšŠåˆ—å’Œå¿«å–å±¤
- ä½¿ç”¨ PostgreSQL ä½œç‚ºæŒä¹…åŒ–å­˜å„²
- å¯¦ç¾äº†å¥åº·æª¢æŸ¥ç«¯é» `/health`
- æä¾› Redis é€£ç·šæ± é…ç½®ä»¥æ”¯æ´é«˜ä½µç™¼
  
## FastAPI è©³ç´°é…ç½®èˆ‡å¯¦ä½œ
  
### 1. æ‡‰ç”¨ç¨‹å¼åˆå§‹åŒ–
  
**æª”æ¡ˆä½ç½®**: `app/main.py` (ç¬¬ 24-28 è¡Œ)
  
```python
app = FastAPI(
    title="é«˜æ•ˆèƒ½æ—¥èªŒæ”¶é›†ç³»çµ±",
    description="åŸºæ–¼ FastAPI + Redis + PostgreSQL çš„æ—¥èªŒæ”¶é›†ç³»çµ±",
    version="1.0.0"
)
```
  
**æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•äº‹ä»¶** (ç¬¬ 43-88 è¡Œ):
  
```python
@app.on_event("startup")
async def startup_event():
    global redis_client
  
    print(f"ğŸš€ Starting FastAPI instance: {INSTANCE_NAME}")
  
    # å»ºç«‹ Redis é€£ç·šæ± 
    pool = redis.ConnectionPool(
        host=REDIS_HOST,
        port=REDIS_PORT,
        decode_responses=True,
        max_connections=200  # æå‡è‡³ 200 ä»¥æ”¯æ´é«˜ä½µç™¼
    )
    redis_client = redis.Redis(connection_pool=pool)
  
    # æ¸¬è©¦ Redis é€£ç·š
    await redis_client.ping()
    print("âœ… Redis connection successful")
  
    # æ¸¬è©¦ PostgreSQL é€£ç·š
    if await test_db_connection():
        print("âœ… PostgreSQL connection successful")
  
    # ç¢ºä¿ Redis Stream æ¶ˆè²»è€…ç¾¤çµ„å­˜åœ¨
    try:
        await redis_client.xgroup_create(
            name='logs:stream',
            groupname='log_workers',
            id='0',
            mkstream=True
        )
        print("âœ… Redis Stream group created")
    except redis.ResponseError as e:
        if "BUSYGROUP" in str(e):
            print("â„¹ï¸ Redis Stream group already exists")
```
  
**é—œéµåˆå§‹åŒ–æ­¥é©Ÿ**:
1. å»ºç«‹ Redis é€£ç·šæ± ï¼ˆæœ€å¤§ 200 é€£ç·šï¼‰
2. é©—è­‰ Redis å’Œ PostgreSQL é€£ç·šç‹€æ…‹
3. å‰µå»º Redis Stream æ¶ˆè²»è€…ç¾¤çµ„ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
  
### 2. Redis é€£ç·šé…ç½®
  
**æª”æ¡ˆä½ç½®**: `app/main.py` (ç¬¬ 33-60 è¡Œ)
  
```python
# ç’°å¢ƒè®Šæ•¸é…ç½®
REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
INSTANCE_NAME = os.getenv('INSTANCE_NAME', 'fastapi-default')
  
# é€£ç·šæ± è¨­ç½®
pool = redis.ConnectionPool(
    host=REDIS_HOST,
    port=REDIS_PORT,
    decode_responses=True,    # è‡ªå‹•è§£ç¢¼éŸ¿æ‡‰ç‚ºå­—ä¸²
    max_connections=200       # æœ€å¤§é€£ç·šæ•¸ï¼ˆå¾ 50 æå‡è‡³ 200ï¼‰
)
redis_client = redis.Redis(connection_pool=pool)
```
  
**é€£ç·šæ± åƒæ•¸èªªæ˜**:
| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| `max_connections` | 200 | æ”¯æ´é«˜ä½µç™¼è«‹æ±‚ |
| `decode_responses` | True | è‡ªå‹•å°‡ bytes è½‰ç‚º str |
| `socket_timeout` | é è¨­ | é€£ç·šè¶…æ™‚æ§åˆ¶ |
  
### 3. è³‡æ–™åº«é€£ç·šé…ç½®
  
**æª”æ¡ˆä½ç½®**: `app/database.py` (ç¬¬ 13-66 è¡Œ)
  
#### PostgreSQL é€£ç·šåƒæ•¸
  
```python
POSTGRES_HOST = os.getenv('POSTGRES_HOST', 'localhost')
POSTGRES_PORT = os.getenv('POSTGRES_PORT', '5432')
POSTGRES_USER = os.getenv('POSTGRES_USER', 'loguser')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'logpass')
POSTGRES_DB = os.getenv('POSTGRES_DB', 'logsdb')
  
DATABASE_URL = f"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
ASYNC_DATABASE_URL = f"postgresql+asyncpg://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"
```
  
#### éåŒæ­¥å¼•æ“ï¼ˆFastAPI ä½¿ç”¨ï¼‰
  
```python
async_engine = create_async_engine(
    ASYNC_DATABASE_URL,
    pool_size=10,           # å¸¸é§é€£ç·šæ•¸
    max_overflow=5,         # é¡å¤–é€£ç·šæ•¸ï¼ˆç¸½è¨ˆå¯é” 15ï¼‰
    pool_timeout=30,        # é€£ç·šç­‰å¾…è¶…æ™‚ï¼ˆç§’ï¼‰
    pool_recycle=3600,      # é€£ç·šå›æ”¶æ™‚é–“ï¼ˆ1 å°æ™‚ï¼‰
    pool_pre_ping=True,     # ä½¿ç”¨å‰æ¸¬è©¦é€£ç·š
    echo=False              # ç”Ÿç”¢æ¨¡å¼ï¼ˆä¸è¼¸å‡º SQLï¼‰
)
```
  
#### åŒæ­¥å¼•æ“ï¼ˆWorker ä½¿ç”¨ï¼‰
  
```python
sync_engine = create_engine(
    DATABASE_URL,
    poolclass=QueuePool,
    pool_size=10,
    max_overflow=5,
    pool_timeout=30,
    pool_recycle=3600,
    pool_pre_ping=True,
    echo=False
)
```
  
**é€£ç·šæ± é…ç½®è¦é»**:
- **Pool Size**: 10 å€‹å¸¸é§é€£ç·š
- **Max Overflow**: 5 å€‹é¡å¤–é€£ç·šï¼ˆç¸½è¨ˆ 15ï¼‰
- **Pool Recycle**: 3600 ç§’å¾Œå›æ”¶é€£ç·šï¼ˆé¿å…éæœŸé€£ç·šï¼‰
- **Pre-ping**: æ¯æ¬¡ä½¿ç”¨å‰æ¸¬è©¦é€£ç·šæœ‰æ•ˆæ€§
  
### 4. API ç«¯é»è©³ç´°å¯¦ä½œ
  
#### 4.1 å–®ä¸€æ—¥èªŒå¯«å…¥ç«¯é»
  
**æª”æ¡ˆä½ç½®**: `app/main.py` (ç¬¬ 147-185 è¡Œ)
  
```python
@app.post("/api/log", response_model=LogEntryResponse)
async def create_log(log: LogEntryRequest):
    """
    æ¥æ”¶å–®ä¸€æ—¥èªŒæ¢ç›®ä¸¦å¯«å…¥ Redis Stream
  
    è™•ç†æµç¨‹:
    1. é©—è­‰æ—¥èªŒæ ¼å¼ï¼ˆPydantic è‡ªå‹•å®Œæˆï¼‰
    2. æ§‹å»ºæ—¥èªŒå­—å…¸
    3. å¯«å…¥ Redis Streamï¼ˆä½¿ç”¨ XADDï¼‰
    4. ç«‹å³è¿”å›éŸ¿æ‡‰ï¼ˆéåŒæ­¥è™•ç†ï¼‰
  
    é æœŸéŸ¿æ‡‰æ™‚é–“: < 5ms
    """
    try:
        log_dict = {
            "device_id": log.device_id,
            "log_level": log.log_level,
            "message": log.message,
            "log_data": json.dumps(log.log_data) if log.log_data else "{}",
            "timestamp": datetime.now().isoformat()
        }
  
        # å¯«å…¥ Redis Stream
        message_id = await redis_client.xadd(
            name="logs:stream",        # Stream åç¨±
            fields=log_dict,           # è¨Šæ¯å…§å®¹
            maxlen=100000,             # ä¿ç•™æœ€è¿‘ 10 è¬ç­†
            approximate=True           # ä½¿ç”¨è¿‘ä¼¼è£å‰ªæå‡æ•ˆèƒ½
        )
  
        return LogEntryResponse(
            status="received",
            message_id=str(message_id),
            received_at=datetime.now()
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```
  
**Redis XADD å‘½ä»¤èªªæ˜**:
- **name**: Stream çš„éµå (`logs:stream`)
- **fields**: è¨Šæ¯çš„éµå€¼å°è³‡æ–™
- **maxlen**: é™åˆ¶ Stream é•·åº¦ç‚º 100,000 ç­†
- **approximate**: ä½¿ç”¨è¿‘ä¼¼è£å‰ªï¼ˆæ•ˆèƒ½å„ªåŒ–ï¼Œå¯¦éš›é•·åº¦å¯èƒ½ç•¥è¶…é maxlenï¼‰
  
#### 4.2 æ‰¹é‡æ—¥èªŒå¯«å…¥ç«¯é»
  
**æª”æ¡ˆä½ç½®**: `app/main.py` (ç¬¬ 190-237 è¡Œ)
  
```python
@app.post("/api/logs/batch", response_model=BatchLogEntryResponse)
async def create_batch_logs(batch: BatchLogEntryRequest):
    """
    æ‰¹é‡æ¥æ”¶æ—¥èªŒæ¢ç›®ä¸¦ä½¿ç”¨ Redis Pipeline æ‰¹æ¬¡å¯«å…¥
  
    è™•ç†æµç¨‹:
    1. é©—è­‰æ‰¹é‡æ ¼å¼ï¼ˆæœ€å¤š 1000 ç­†ï¼‰
    2. å»ºç«‹ Redis Pipeline
    3. æ‰¹æ¬¡åŸ·è¡Œ XADD å‘½ä»¤
    4. è¿”å›æ‰€æœ‰ message_id
  
    é æœŸååé‡: 10,000+ logs/ç§’
    """
    try:
        current_time = datetime.now().isoformat()
  
        # ä½¿ç”¨ Pipeline æ¸›å°‘ç¶²è·¯å¾€è¿”
        pipe = redis_client.pipeline()
  
        for log in batch.logs:
            log_dict = {
                "device_id": log.device_id,
                "log_level": log.log_level,
                "message": log.message,
                "log_data": json.dumps(log.log_data) if log.log_data else "{}",
                "timestamp": current_time
            }
            pipe.xadd(
                name="logs:stream",
                fields=log_dict,
                maxlen=100000,
                approximate=True
            )
  
        # æ‰¹æ¬¡åŸ·è¡Œæ‰€æœ‰å‘½ä»¤
        results = await pipe.execute()
        message_ids = [str(r) for r in results]
  
        return BatchLogEntryResponse(
            status="received",
            count=len(message_ids),
            message_ids=message_ids,
            received_at=datetime.now()
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```
  
**Pipeline æ‰¹æ¬¡è™•ç†å„ªå‹¢**:
- **æ¸›å°‘ç¶²è·¯å»¶é²**: ä¸€æ¬¡å¾€è¿”åŸ·è¡Œå¤šå€‹å‘½ä»¤
- **åŸå­æ€§**: æ‰€æœ‰å‘½ä»¤åœ¨ä¼ºæœå™¨ç«¯é€£çºŒåŸ·è¡Œ
- **é«˜ååé‡**: æ”¯æ´æ¯æ‰¹æ¬¡æœ€å¤š 1000 ç­†æ—¥èªŒ
  
#### 4.3 æ—¥èªŒæŸ¥è©¢ç«¯é»ï¼ˆå¸¶å¿«å–ï¼‰
  
**æª”æ¡ˆä½ç½®**: `app/main.py` (ç¬¬ 242-318 è¡Œ)
  
```python
@app.get("/api/logs/{device_id}", response_model=BatchLogQueryResponse)
async def get_logs(
    device_id: str,
    limit: int = Query(default=100, ge=1, le=1000),
    db: AsyncSession = Depends(get_async_db)
):
    """
    æŸ¥è©¢ç‰¹å®šè¨­å‚™çš„æ—¥èªŒè³‡æ–™ï¼ˆæ”¯æ´å¿«å–ï¼‰
  
    è™•ç†æµç¨‹:
    1. æª¢æŸ¥ Redis å¿«å–
    2. å¿«å–å‘½ä¸­ â†’ ç›´æ¥è¿”å›
    3. å¿«å–æœªå‘½ä¸­ â†’ æŸ¥è©¢ PostgreSQL
    4. å°‡çµæœå¯«å…¥å¿«å–ï¼ˆTTL 5 åˆ†é˜ï¼‰
    """
    cache_key = f"cache:logs:{device_id}:{limit}"
  
    # å˜—è©¦å¾å¿«å–ç²å–
    try:
        cached_data = await redis_client.get(cache_key)
        if cached_data:
            logs_data = json.loads(cached_data)
            return BatchLogQueryResponse(
                total=len(logs_data),
                source="cache",
                data=logs_data
            )
    except Exception as e:
        print(f"Cache read error: {e}")
  
    # å¿«å–æœªå‘½ä¸­ï¼ŒæŸ¥è©¢è³‡æ–™åº«
    query = select(Log).where(
        Log.device_id == device_id
    ).order_by(
        Log.created_at.desc()
    ).limit(limit)
  
    result = await db.execute(query)
    logs = result.scalars().all()
  
    logs_data = [
        {
            "id": log.id,
            "device_id": log.device_id,
            "log_level": log.log_level,
            "message": log.message,
            "log_data": log.log_data,
            "created_at": log.created_at.isoformat(),
        }
        for log in logs
    ]
  
    # å¯«å…¥å¿«å–ï¼ˆTTL 5 åˆ†é˜ï¼‰
    try:
        await redis_client.setex(
            name=cache_key,
            time=300,  # 5 åˆ†é˜
            value=json.dumps(logs_data, default=str)
        )
    except Exception as e:
        print(f"Cache write error: {e}")
  
    return BatchLogQueryResponse(
        total=len(logs_data),
        source="database",
        data=logs_data
    )
```
  
**å¿«å–ç­–ç•¥**:
- **å¿«å–éµæ ¼å¼**: `cache:logs:{device_id}:{limit}`
- **TTL**: 300 ç§’ï¼ˆ5 åˆ†é˜ï¼‰
- **å¿«å–ç©¿é€ä¿è­·**: å³ä½¿è³‡æ–™åº«ç‚ºç©ºä¹Ÿå¿«å–çµæœ
  
#### 4.4 çµ±è¨ˆè³‡è¨Šç«¯é»
  
**æª”æ¡ˆä½ç½®**: `app/main.py` (ç¬¬ 323-394 è¡Œ)
  
```python
@app.get("/api/stats", response_model=StatsResponse)
async def get_stats(db: AsyncSession = Depends(get_async_db)):
    """
    ç²å–ç³»çµ±çµ±è¨ˆè³‡è¨Šï¼ˆå¿«å– 60 ç§’ï¼‰
  
    è¿”å›:
    - ç¸½æ—¥èªŒæ•¸é‡
    - æŒ‰ç´šåˆ¥åˆ†é¡çš„æ—¥èªŒæ•¸
    - æœ€è¿‘æ´»èºçš„è¨­å‚™ï¼ˆå‰ 10 åï¼‰
    """
    cache_key = "cache:stats"
  
    # æª¢æŸ¥å¿«å–
    cached_stats = await redis_client.get(cache_key)
    if cached_stats:
        return json.loads(cached_stats)
  
    # ç¸½æ—¥èªŒæ•¸
    total_result = await db.execute(select(func.count(Log.id)))
    total_logs = total_result.scalar()
  
    # æŒ‰ç´šåˆ¥åˆ†é¡
    level_query = select(
        Log.log_level,
        func.count(Log.id)
    ).group_by(Log.log_level)
    level_result = await db.execute(level_query)
    logs_by_level = {row[0]: row[1] for row in level_result.all()}
  
    # æœ€è¿‘æ´»èºè¨­å‚™
    device_query = select(
        Log.device_id,
        func.count(Log.id)
    ).group_by(Log.device_id).order_by(
        func.count(Log.id).desc()
    ).limit(10)
    device_result = await db.execute(device_query)
    recent_devices = [row[0] for row in device_result.all()]
  
    stats = StatsResponse(
        total_logs=total_logs,
        logs_by_level=logs_by_level,
        recent_devices=recent_devices
    )
  
    # å¿«å– 60 ç§’
    await redis_client.setex(
        cache_key,
        60,
        json.dumps(stats.dict(), default=str)
    )
  
    return stats
```
  
### 5. è³‡æ–™æ¨¡å‹å®šç¾©
  
**æª”æ¡ˆä½ç½®**: `app/models.py`
  
#### ORM æ¨¡å‹ï¼ˆè³‡æ–™åº«æ˜ å°„ï¼‰
  
```python
class Log(Base):
    __tablename__ = 'logs'
  
    id = Column(Integer, primary_key=True, autoincrement=True)
    device_id = Column(String(50), nullable=False, index=True)
    log_level = Column(String(20), nullable=False, index=True)
    message = Column(Text, nullable=True)
    log_data = Column(JSON, nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    indexed_at = Column(DateTime(timezone=True), server_default=func.now())
  
    __table_args__ = (
        Index('idx_device_created', 'device_id', 'created_at'),
        Index('idx_created_desc', 'created_at'),
    )
```
  
#### Pydantic è«‹æ±‚æ¨¡å‹
  
```python
class LogEntryRequest(BaseModel):
    device_id: str = Field(..., min_length=1, max_length=50, description="è¨­å‚™ ID")
    log_level: str = Field(..., description="æ—¥èªŒç´šåˆ¥: DEBUG/INFO/WARNING/ERROR/CRITICAL")
    message: str = Field(..., min_length=1, max_length=5000, description="æ—¥èªŒè¨Šæ¯")
    log_data: Optional[Dict[str, Any]] = Field(default={}, description="é¡å¤–è³‡æ–™")
  
class BatchLogEntryRequest(BaseModel):
    logs: list[LogEntryRequest] = Field(..., min_length=1, max_length=1000)
```
  
#### Pydantic éŸ¿æ‡‰æ¨¡å‹
  
```python
class LogEntryResponse(BaseModel):
    status: str
    message_id: str
    received_at: datetime
  
class BatchLogEntryResponse(BaseModel):
    status: str
    count: int
    message_ids: list[str]
    received_at: datetime
  
class BatchLogQueryResponse(BaseModel):
    total: int
    source: str  # "cache" æˆ– "database"
    data: list[Dict[str, Any]]
```
  
### 6. è³‡æ–™åº«çµæ§‹è¨­è¨ˆ
  
**æª”æ¡ˆä½ç½®**: `postgres/init.sql`
  
#### ä¸»è¡¨çµæ§‹
  
```sql
CREATE TABLE IF NOT EXISTS logs (
    id BIGSERIAL PRIMARY KEY,
    device_id VARCHAR(50) NOT NULL,
    log_level VARCHAR(20) NOT NULL,
    message TEXT,
    log_data JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    indexed_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```
  
#### ç´¢å¼•å„ªåŒ–ç­–ç•¥
  
```sql
-- è¤‡åˆç´¢å¼•ï¼šæœ€å¸¸ç”¨çš„æŸ¥è©¢æ¨¡å¼ï¼ˆè¨­å‚™ ID + æ™‚é–“é™åºï¼‰
CREATE INDEX IF NOT EXISTS idx_device_created
ON logs(device_id, created_at DESC);
  
-- æ—¥èªŒç´šåˆ¥ç´¢å¼•ï¼šæ”¯æ´æŒ‰ç´šåˆ¥éæ¿¾
CREATE INDEX IF NOT EXISTS idx_log_level
ON logs(log_level);
  
-- æ™‚é–“ç´¢å¼•ï¼šæ”¯æ´æ™‚é–“ç¯„åœæŸ¥è©¢
CREATE INDEX IF NOT EXISTS idx_created_at
ON logs(created_at DESC);
  
-- GIN ç´¢å¼•ï¼šæ”¯æ´ JSONB æ¬„ä½æŸ¥è©¢
CREATE INDEX IF NOT EXISTS idx_log_data_gin
ON logs USING GIN(log_data);
```
  
**ç´¢å¼•è¨­è¨ˆè€ƒé‡**:
- **idx_device_created**: è¦†è“‹ 90% çš„æŸ¥è©¢æ¨¡å¼
- **idx_log_level**: æ”¯æ´æŒ‰ç´šåˆ¥çµ±è¨ˆ
- **idx_created_at**: æ”¯æ´æ™‚åºæŸ¥è©¢
- **idx_log_data_gin**: æ”¯æ´ JSON å…§å®¹æœå°‹
  
## Redis å®Œæ•´é…ç½®èˆ‡æ¶æ§‹
  
Redis åœ¨æœ¬ç³»çµ±ä¸­æ‰®æ¼”å¤šé‡è§’è‰²ï¼š**è¨Šæ¯ä½‡åˆ—ï¼ˆStreamï¼‰**ã€**è³‡æ–™å¿«å–ï¼ˆCacheï¼‰** å’Œ **ç·©è¡å±¤ï¼ˆBufferï¼‰**ã€‚ä»¥ä¸‹æ˜¯ Redis çš„å®Œæ•´é…ç½®èˆ‡é‹ä½œæ©Ÿåˆ¶ã€‚
  
### 1. Redis ä¼ºæœå™¨é…ç½®
  
**æª”æ¡ˆä½ç½®**: `docker-compose.yml` (ç¬¬ 98-119 è¡Œ)
  
```yaml
redis:
  image: redis:7-alpine
  container_name: log-redis
  ports:
    - "16891:6379"              # å†·é–€ç«¯å£é¿å…è¡çª
  volumes:
    - redis-data:/data          # è³‡æ–™æŒä¹…åŒ–
  command: >
    redis-server
    --appendonly yes            # å•Ÿç”¨ AOF æŒä¹…åŒ–
    --maxmemory 512mb           # è¨˜æ†¶é«”ä¸Šé™
    --maxmemory-policy allkeys-lru  # LRU æ·˜æ±°ç­–ç•¥
  healthcheck:
    test: ["CMD", "redis-cli", "ping"]
    interval: 10s
    timeout: 3s
    retries: 3
```
  
#### é—œéµé…ç½®åƒæ•¸è§£æ
  
| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| **image** | `redis:7-alpine` | è¼•é‡åŒ– Alpine ç‰ˆæœ¬ï¼Œé«”ç©å°ã€æ•ˆèƒ½ä½³ |
| **--appendonly yes** | AOF æŒä¹…åŒ– | æ¯æ¬¡å¯«å…¥æ“ä½œéƒ½è¿½åŠ åˆ°æª”æ¡ˆï¼Œè³‡æ–™å®‰å…¨æ€§é«˜ |
| **--maxmemory 512mb** | 512MB | Redis æœ€å¤§ä½¿ç”¨è¨˜æ†¶é«”ä¸Šé™ |
| **--maxmemory-policy** | `allkeys-lru` | ç•¶è¨˜æ†¶é«”æ»¿æ™‚ï¼Œæ·˜æ±°æœ€è¿‘æœ€å°‘ä½¿ç”¨çš„ key |
  
### 2. Redis æŒä¹…åŒ–ç­–ç•¥ (AOF)
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AOF æŒä¹…åŒ–å·¥ä½œæµç¨‹                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
Redis å¯«å…¥å‘½ä»¤
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¯«å…¥ AOF ç·©è¡å€  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼ (æ ¹æ“š fsync ç­–ç•¥)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŒæ­¥åˆ°ç£ç¢Ÿ      â”‚  appendonly.aof
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼ (ä¼ºæœå™¨é‡å•Ÿæ™‚)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é‡å»ºè¨˜æ†¶é«”è³‡æ–™   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
**AOF å„ªå‹¢**ï¼š
- **è³‡æ–™å®‰å…¨æ€§é«˜**ï¼šæ¯å€‹å¯«å…¥å‘½ä»¤éƒ½æœƒè¢«è¨˜éŒ„
- **å¯è®€æ€§ä½³**ï¼šAOF æª”æ¡ˆæ˜¯ç´”æ–‡å­—æ ¼å¼ï¼Œä¾¿æ–¼é™¤éŒ¯
- **é©åˆ Stream**ï¼šRedis Stream çš„è³‡æ–™ä¸æœƒå› é‡å•Ÿè€Œéºå¤±
  
**AOF é…ç½®é¸é …**ï¼š
- `appendfsync always`: æ¯æ¬¡å¯«å…¥éƒ½åŒæ­¥ï¼ˆæœ€å®‰å…¨ï¼Œæ•ˆèƒ½æœ€ä½ï¼‰
- `appendfsync everysec`: æ¯ç§’åŒæ­¥ä¸€æ¬¡ï¼ˆé è¨­ï¼Œå¹³è¡¡å®‰å…¨èˆ‡æ•ˆèƒ½ï¼‰
- `appendfsync no`: ç”±ä½œæ¥­ç³»çµ±æ±ºå®šï¼ˆæ•ˆèƒ½æœ€ä½³ï¼Œé¢¨éšªè¼ƒé«˜ï¼‰
  
### 3. Redis è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥
  
**è¨˜æ†¶é«”ä½¿ç”¨åˆ†ä½ˆ**ï¼š
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Redis 512MB è¨˜æ†¶é«”é…ç½®                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
â”œâ”€â”€ Redis Stream (logs:stream)
â”‚   â”œâ”€ maxlen: 100,000 ç­†æ—¥èªŒ
â”‚   â””â”€ é ä¼°å¤§å°: ~200-400MB (æ¯ç­†ç´„ 2-4KB)
â”‚
â”œâ”€â”€ å¿«å–è³‡æ–™
â”‚   â”œâ”€ cache:logs:{device_id}:{limit}  (TTL: 300s)
â”‚   â”œâ”€ cache:stats                      (TTL: 60s)
â”‚   â””â”€ é ä¼°å¤§å°: ~50-100MB
â”‚
â””â”€â”€ ç³»çµ±é–‹éŠ·èˆ‡ç·©è¡
    â””â”€ ~50MB
```
  
**LRU (Least Recently Used) æ·˜æ±°ç­–ç•¥**ï¼š
  
```python
# ç•¶è¨˜æ†¶é«”é”åˆ° 512MB ä¸Šé™æ™‚
if memory_used >= 512MB:
    # æ‰¾å‡ºæ‰€æœ‰ key ä¸­æœ€ä¹…æœªä½¿ç”¨çš„
    least_recently_used_key = find_lru_key(all_keys)
    # æ·˜æ±°è©² key
    delete(least_recently_used_key)
```
  
**æ·˜æ±°ç­–ç•¥æ¯”è¼ƒ**ï¼š
  
| ç­–ç•¥ | èªªæ˜ | é©ç”¨å ´æ™¯ |
|------|------|----------|
| `allkeys-lru` âœ“ | æ‰€æœ‰ key ä¸­æ·˜æ±° LRU | æ··åˆç”¨é€”ï¼ˆå¿«å–+ä½‡åˆ—ï¼‰|
| `volatile-lru` | åƒ…æœ‰ TTL çš„ key ä¸­æ·˜æ±° | ç´”å¿«å–å ´æ™¯ |
| `noeviction` | ä¸æ·˜æ±°ï¼Œæ»¿æ™‚å ±éŒ¯ | è³‡æ–™ä¸å¯éºå¤±å ´æ™¯ |
| `allkeys-random` | éš¨æ©Ÿæ·˜æ±° | è¨ªå•æ¨¡å¼å‡å‹» |
  
### 4. Redis é€£ç·šæ± é…ç½®
  
#### 4.1 FastAPI éåŒæ­¥é€£ç·šæ± 
  
**æª”æ¡ˆä½ç½®**: `app/main.py` (ç¬¬ 53-60 è¡Œ)
  
```python
# FastAPI å•Ÿå‹•æ™‚åˆå§‹åŒ–
pool = redis.ConnectionPool(
    host=REDIS_HOST,              # å¾ç’°å¢ƒè®Šæ•¸è®€å– (é è¨­: redis)
    port=REDIS_PORT,              # å¾ç’°å¢ƒè®Šæ•¸è®€å– (é è¨­: 6379)
    decode_responses=True,        # è‡ªå‹•è§£ç¢¼ç‚ºå­—ä¸²
    max_connections=200           # æœ€å¤§é€£ç·šæ•¸ (æå‡è‡³ 200)
)
redis_client = redis.Redis(connection_pool=pool)
```
  
**é€£ç·šæ± åƒæ•¸èªªæ˜**ï¼š
  
| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| `max_connections` | 200 | æ”¯æ´é«˜ä½µç™¼ï¼ˆ6 workers Ã— ~33 é€£ç·š/workerï¼‰|
| `decode_responses` | True | è‡ªå‹•å°‡ bytes è½‰ç‚º str |
| `socket_timeout` | é è¨­ | è®€å¯«è¶…æ™‚æ§åˆ¶ |
| `retry_on_timeout` | é è¨­ False | è¶…æ™‚æ˜¯å¦é‡è©¦ |
  
**é€£ç·šæ± å·¥ä½œåŸç†**ï¼š
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   é€£ç·šæ± ç®¡ç†æµç¨‹                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
FastAPI Worker 1 â”€â”
FastAPI Worker 2 â”€â”¤    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
FastAPI Worker 3 â”€â”¼â”€â”€â”€â–ºâ”‚   é€£ç·šæ±             â”‚
FastAPI Worker 4 â”€â”¤    â”‚  (max_conn=200)    â”‚
FastAPI Worker 5 â”€â”¤    â”‚                    â”‚
FastAPI Worker 6 â”€â”˜    â”‚  â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚  â”‚C1â”‚C2â”‚C3â”‚..â”‚    â”‚â”€â”€â”€â”€â–ºâ”‚  Redis  â”‚
                       â”‚  â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜    â”‚     â”‚  Server â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
æµç¨‹ï¼š
1. è«‹æ±‚åˆ°é” â†’ å¾é€£ç·šæ± å€Ÿç”¨é€£ç·š
2. åŸ·è¡Œ Redis å‘½ä»¤
3. å‘½ä»¤å®Œæˆ â†’ é€£ç·šæ­¸é‚„é€£ç·šæ± 
4. é€£ç·šè¤‡ç”¨ï¼Œæ¸›å°‘å»ºç«‹é–‹éŠ·
```
  
#### 4.2 Worker åŒæ­¥é€£ç·šæ± 
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 54-61 è¡Œ)
  
```python
# Worker å•Ÿå‹•æ™‚åˆå§‹åŒ–
redis_client = redis.Redis(
    host=REDIS_HOST,
    port=REDIS_PORT,
    decode_responses=True,
    socket_connect_timeout=5,      # é€£ç·šè¶…æ™‚ 5 ç§’
    socket_keepalive=True,         # ä¿æŒé€£ç·šå­˜æ´»
    max_connections=10             # Worker ä½¿ç”¨è¼ƒå°‘é€£ç·š
)
```
  
**Worker é€£ç·šæ± åƒæ•¸èªªæ˜**ï¼š
  
| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| `max_connections` | 10 | Worker å–®ä¸€ç¨‹åºï¼Œé€£ç·šéœ€æ±‚è¼ƒä½ |
| `socket_connect_timeout` | 5s | é€£ç·šå»ºç«‹è¶…æ™‚æ™‚é–“ |
| `socket_keepalive` | True | TCP Keep-Alive ä¿æŒé€£ç·š |
  
### 5. Redis Stream è©³ç´°æ©Ÿåˆ¶
  
Redis Stream æ˜¯æœ¬ç³»çµ±çš„æ ¸å¿ƒè¨Šæ¯ä½‡åˆ—ï¼Œæä¾›**æŒä¹…åŒ–ã€æ¶ˆè²»è€…ç¾¤çµ„ã€è¨Šæ¯ç¢ºèª**ç­‰ä¼æ¥­ç´šåŠŸèƒ½ã€‚
  
#### 5.1 Stream è³‡æ–™çµæ§‹
  
```
logs:stream (Redis Stream)
â”‚
â”œâ”€ Entry ID: 1704067200000-0
â”‚  â”œâ”€ device_id: "device-001"
â”‚  â”œâ”€ log_level: "INFO"
â”‚  â”œâ”€ message: "Application started"
â”‚  â”œâ”€ log_data: '{"version": "1.0.0"}'
â”‚  â””â”€ timestamp: "2024-01-01T12:00:00"
â”‚
â”œâ”€ Entry ID: 1704067200000-1
â”‚  â”œâ”€ device_id: "device-002"
â”‚  â””â”€ ...
â”‚
â””â”€ Entry ID: 1704067200001-0
   â””â”€ ...
```
  
**Entry ID æ ¼å¼**ï¼š`<æ¯«ç§’æ™‚é–“æˆ³>-<åºåˆ—è™Ÿ>`
- æ™‚é–“æˆ³ï¼šUnix æ¯«ç§’æ™‚é–“
- åºåˆ—è™Ÿï¼šåŒä¸€æ¯«ç§’å…§çš„éå¢ç·¨è™Ÿ
- è‡ªå‹•ç”Ÿæˆï¼šRedis è‡ªå‹•åˆ†é…å”¯ä¸€ ID
  
#### 5.2 Stream å¯«å…¥æ“ä½œ (FastAPI ç«¯)
  
**å–®ä¸€æ—¥èªŒå¯«å…¥** (`app/main.py:170-175`):
  
```python
message_id = await redis_client.xadd(
    name="logs:stream",        # Stream åç¨±
    fields={                   # è¨Šæ¯æ¬„ä½
        "device_id": "device-001",
        "log_level": "INFO",
        "message": "Log content",
        "log_data": '{"key": "value"}',
        "timestamp": "2024-01-01T12:00:00"
    },
    maxlen=100000,             # Stream æœ€å¤§é•·åº¦
    approximate=True           # ä½¿ç”¨è¿‘ä¼¼è£å‰ª
)
# è¿”å›: b"1704067200000-0"
```
  
**Redis XADD å‘½ä»¤å°æ‡‰**ï¼š
```bash
XADD logs:stream MAXLEN ~ 100000 * device_id "device-001" log_level "INFO" ...
```
  
**maxlen èˆ‡ approximate åƒæ•¸**ï¼š
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Stream é•·åº¦æ§åˆ¶æ©Ÿåˆ¶                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
maxlen=100000, approximate=False (ç²¾ç¢ºæ¨¡å¼)
â”œâ”€ æ¯æ¬¡ XADD å¾Œæª¢æŸ¥é•·åº¦
â”œâ”€ è¶…é 100000 å‰‡åˆªé™¤æœ€èˆŠçš„
â””â”€ æ•ˆèƒ½è¼ƒä½ï¼ˆæ¯æ¬¡éƒ½è¦è£å‰ªï¼‰
  
maxlen=100000, approximate=True (è¿‘ä¼¼æ¨¡å¼) âœ“
â”œâ”€ å…è¨±é•·åº¦æš«æ™‚è¶…é 100000
â”œâ”€ ç•¶è¶…éè¼ƒå¤šæ™‚æ‰æ‰¹æ¬¡è£å‰ª
â”œâ”€ å¯¦éš›é•·åº¦å¯èƒ½ç‚º 100000~101000
â””â”€ æ•ˆèƒ½è¼ƒé«˜ï¼ˆæ¸›å°‘è£å‰ªé »ç‡ï¼‰
```
  
**æ‰¹é‡å¯«å…¥ (Pipeline)** (`app/main.py:206-226`):
  
```python
# å»ºç«‹ Pipeline
pipe = redis_client.pipeline()
  
# æ·»åŠ å¤šå€‹ XADD å‘½ä»¤ï¼ˆä¸ç«‹å³åŸ·è¡Œï¼‰
for log in batch.logs:
    pipe.xadd(
        name="logs:stream",
        fields=log_dict,
        maxlen=100000,
        approximate=True
    )
  
# ä¸€æ¬¡æ€§åŸ·è¡Œæ‰€æœ‰å‘½ä»¤
results = await pipe.execute()
# è¿”å›: [b"1704067200000-0", b"1704067200000-1", ...]
```
  
**Pipeline æ•ˆèƒ½æå‡åŸç†**ï¼š
  
```
ä¸ä½¿ç”¨ Pipelineï¼š
Client â”€â–º XADD â”€â–º Server â”€â–º Response â”€â–º Client
Client â”€â–º XADD â”€â–º Server â”€â–º Response â”€â–º Client
Client â”€â–º XADD â”€â–º Server â”€â–º Response â”€â–º Client
(3 æ¬¡ç¶²è·¯å¾€è¿”)
  
ä½¿ç”¨ Pipelineï¼š
Client â”€â–º [XADD, XADD, XADD] â”€â–º Server â”€â–º [Resp1, Resp2, Resp3] â”€â–º Client
(1 æ¬¡ç¶²è·¯å¾€è¿”)
```
  
**æ•ˆèƒ½æ•¸æ“š**ï¼š
- å–®ä¸€ XADDï¼š~0.5ms/ç­†
- Pipeline (100ç­†)ï¼š~5ms/æ‰¹æ¬¡ = ~0.05ms/ç­†
- **æ•ˆèƒ½æå‡ï¼š10å€**
  
#### 5.3 Stream æ¶ˆè²»æ“ä½œ (Worker ç«¯)
  
**æ¶ˆè²»è€…ç¾¤çµ„æ¨¡å¼** (`app/worker.py:195-201`):
  
```python
messages = redis_client.xreadgroup(
    groupname='log_workers',       # æ¶ˆè²»è€…ç¾¤çµ„åç¨±
    consumername='worker-1',       # Worker è­˜åˆ¥ç¢¼
    streams={'logs:stream': '>'},  # '>' è¡¨ç¤ºåªè®€å–æ–°è¨Šæ¯
    count=100,                     # æ¯æ¬¡æœ€å¤šè®€å– 100 ç­†
    block=5000                     # é˜»å¡ç­‰å¾… 5000 æ¯«ç§’
)
```
  
**XREADGROUP è¿”å›æ ¼å¼**ï¼š
  
```python
[
    (
        'logs:stream',  # Stream åç¨±
        [
            (
                '1704067200000-0',  # Entry ID
                {
                    'device_id': 'device-001',
                    'log_level': 'INFO',
                    'message': 'Log content',
                    'log_data': '{"key": "value"}',
                    'timestamp': '2024-01-01T12:00:00'
                }
            ),
            ('1704067200000-1', {...}),
            # ... æ›´å¤šè¨Šæ¯
        ]
    )
]
```
  
**æ¶ˆè²»è€…ç¾¤çµ„ç‹€æ…‹è¿½è¹¤**ï¼š
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 æ¶ˆè²»è€…ç¾¤çµ„å…§éƒ¨ç‹€æ…‹                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
Consumer Group: log_workers
â”‚
â”œâ”€ last-delivered-id: 1704067200100-0
â”‚  (æœ€å¾Œä¸€å€‹è¢«åˆ†é…å‡ºå»çš„è¨Šæ¯ ID)
â”‚
â”œâ”€ consumers:
â”‚  â”œâ”€ worker-1
â”‚  â”‚  â”œâ”€ pending-count: 100      (è™•ç†ä¸­çš„è¨Šæ¯æ•¸)
â”‚  â”‚  â”œâ”€ idle-time: 1000ms       (é–’ç½®æ™‚é–“)
â”‚  â”‚  â””â”€ pending-entries:
â”‚  â”‚     â”œâ”€ 1704067200000-0 (åˆ†é…æ™‚é–“: T1)
â”‚  â”‚     â””â”€ 1704067200000-1 (åˆ†é…æ™‚é–“: T2)
â”‚  â”‚
â”‚  â””â”€ worker-2
â”‚     â””â”€ pending-count: 0
â”‚
â””â”€ pending-entries-list (PEL):
   â”œâ”€ ç¸½å¾…è™•ç†è¨Šæ¯: 100
   â”œâ”€ æœ€å° ID: 1704067200000-0
   â””â”€ æœ€å¤§ ID: 1704067200099-0
```
  
#### 5.4 è¨Šæ¯ç¢ºèªæ©Ÿåˆ¶ (ACK)
  
**ç¢ºèªè™•ç†å®Œæˆ** (`app/worker.py:219-223`):
  
```python
# æ‰¹æ¬¡å¯«å…¥è³‡æ–™åº«æˆåŠŸå¾Œ
for message_id in message_ids:
    redis_client.xack('logs:stream', 'log_workers', message_id)
```
  
**XACK å·¥ä½œåŸç†**ï¼š
  
```
è™•ç†å‰ï¼š
logs:stream â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
              â†‘
              last-delivered-id = 1704067200100-0
  
Pending Entries List (PEL):
â”œâ”€ 1704067200000-0 â†’ worker-1
â”œâ”€ 1704067200000-1 â†’ worker-1
â””â”€ ... (100 ç­†)
  
è™•ç†å¾Œ (XACK):
logs:stream â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
  
Pending Entries List (PEL):
â””â”€ (ç©º)  âœ“ æ‰€æœ‰è¨Šæ¯å·²ç¢ºèª
```
  
**æœªç¢ºèªè¨Šæ¯çš„å½±éŸ¿**ï¼š
- è¨Šæ¯ä¿ç•™åœ¨ PEL ä¸­
- Worker å´©æ½°å¾Œå¯é‡æ–°åˆ†é…
- ä½¿ç”¨ XCLAIM é‡æ–°èªé ˜è¶…æ™‚è¨Šæ¯
  
### 6. Redis å¿«å–å±¤è©³è§£
  
é™¤äº† Stream è¨Šæ¯ä½‡åˆ—ï¼ŒRedis ä¹Ÿä½œç‚º**æŸ¥è©¢çµæœå¿«å–**ï¼Œæ¸›å°‘è³‡æ–™åº«å£“åŠ›ã€‚
  
#### 6.1 å¿«å–ç­–ç•¥è¨­è¨ˆ
  
**å¿«å– Key å‘½åè¦ç¯„**ï¼š
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¿«å– Key è¨­è¨ˆ                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
æ—¥èªŒæŸ¥è©¢å¿«å–ï¼š
â”œâ”€ æ ¼å¼: cache:logs:{device_id}:{limit}
â”œâ”€ ç¯„ä¾‹: cache:logs:device-001:100
â”œâ”€ TTL: 300 ç§’ (5 åˆ†é˜)
â””â”€ ç”¨é€”: å„²å­˜ç‰¹å®šè¨­å‚™çš„æ—¥èªŒåˆ—è¡¨
  
çµ±è¨ˆè³‡æ–™å¿«å–ï¼š
â”œâ”€ æ ¼å¼: cache:stats
â”œâ”€ TTL: 60 ç§’ (1 åˆ†é˜)
â””â”€ ç”¨é€”: ç³»çµ±æ•´é«”çµ±è¨ˆè³‡è¨Š
```
  
#### 6.2 å¿«å–è®€å¯«æµç¨‹
  
**æ—¥èªŒæŸ¥è©¢å¿«å–** (`app/main.py:260-314`):
  
```python
# 1. å»ºç«‹å¿«å– Key
cache_key = f"cache:logs:{device_id}:{limit}"
  
# 2. å˜—è©¦è®€å–å¿«å–
cached_data = await redis_client.get(cache_key)
  
if cached_data:
    # å¿«å–å‘½ä¸­ (Cache Hit)
    logs_data = json.loads(cached_data)
    return BatchLogQueryResponse(
        total=len(logs_data),
        source="cache",  # æ¨™ç¤ºè³‡æ–™ä¾†æº
        data=logs_data
    )
  
# 3. å¿«å–æœªå‘½ä¸­ (Cache Miss) - æŸ¥è©¢è³‡æ–™åº«
logs = await db.execute(query)
  
# 4. å¯«å…¥å¿«å–
await redis_client.setex(
    name=cache_key,
    time=300,  # TTL 5 åˆ†é˜
    value=json.dumps(logs_data)
)
  
return BatchLogQueryResponse(
    total=len(logs_data),
    source="database",  # æ¨™ç¤ºè³‡æ–™ä¾†æº
    data=logs_data
)
```
  
**å¿«å–æµç¨‹åœ–**ï¼š
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å¿«å–æŸ¥è©¢æ±ºç­–æµç¨‹                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
GET /api/logs/device-001?limit=100
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å»ºç«‹å¿«å– Key     â”‚
â”‚ cache:logs:...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     Yes    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis GET key   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ è¿”å›å¿«å–è³‡æ–™     â”‚
â”‚ å¿«å–å‘½ä¸­ï¼Ÿ       â”‚            â”‚ source="cache"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ No
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL      â”‚
â”‚ SELECT ... FROM â”‚
â”‚ logs WHERE ...  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis SETEX     â”‚
â”‚ TTL=300s        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è¿”å›è³‡æ–™åº«çµæœ   â”‚
â”‚ source="db"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
#### 6.3 å¿«å–å¤±æ•ˆç­–ç•¥
  
**Time-To-Live (TTL) è¨­å®š**ï¼š
  
| å¿«å–é¡å‹ | TTL | ç†ç”± |
|----------|-----|------|
| æ—¥èªŒæŸ¥è©¢ | 300s (5åˆ†é˜) | æ—¥èªŒè³‡æ–™è®Šå‹•é »ç¹ï¼Œä½†çŸ­æœŸå…§ç›¸åŒæŸ¥è©¢å¯è¤‡ç”¨ |
| çµ±è¨ˆè³‡æ–™ | 60s (1åˆ†é˜) | çµ±è¨ˆæŸ¥è©¢æ˜‚è²´ï¼Œä½†éœ€è¦ç›¸å°å³æ™‚ |
  
**è‡ªå‹•å¤±æ•ˆ vs ä¸»å‹•å¤±æ•ˆ**ï¼š
  
```
è‡ªå‹•å¤±æ•ˆ (æœ¬ç³»çµ±æ¡ç”¨):
â”œâ”€ å„ªé»ï¼šå¯¦ä½œç°¡å–®ï¼Œç„¡éœ€è¿½è¹¤è³‡æ–™è®Šæ›´
â”œâ”€ ç¼ºé»ï¼šå¿«å–å¯èƒ½åŒ…å«éæœŸè³‡æ–™
â””â”€ é©ç”¨ï¼šæ—¥èªŒç³»çµ±ï¼ˆå¯æ¥å—çŸ­æœŸä¸ä¸€è‡´ï¼‰
  
ä¸»å‹•å¤±æ•ˆ (é€²éšæ–¹æ¡ˆ):
â”œâ”€ å„ªé»ï¼šè³‡æ–™å§‹çµ‚ä¸€è‡´
â”œâ”€ ç¼ºé»ï¼šå¯¦ä½œè¤‡é›œï¼Œéœ€è¦ç›£è½è³‡æ–™è®Šæ›´
â””â”€ é©ç”¨ï¼šå³æ™‚æ€§è¦æ±‚é«˜çš„ç³»çµ±
```
  
### 7. Redis é›™é‡è§’è‰²ï¼šStream vs Cache
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Redis åœ¨ç³»çµ±ä¸­çš„é›™é‡è§’è‰²                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
                          Redis 512MB
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                                â”‚
    Stream    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚    Cache
   (å¯«å…¥è·¯å¾‘)  â”‚  â”‚   logs:stream        â”‚     â”‚   (è®€å–è·¯å¾‘)
              â”‚  â”‚   (è¨Šæ¯ä½‡åˆ—)          â”‚     â”‚
FastAPI â”€â”€â”€â”€â”€â–ºâ”‚  â”‚   100,000 entries    â”‚     â”‚â—„â”€â”€â”€â”€ FastAPI
  XADD        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚        GET
              â”‚                                â”‚
              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
Worker â—„â”€â”€â”€â”€â”€â”€â”‚  â”‚   cache:logs:*       â”‚     â”‚
  XREADGROUP  â”‚  â”‚   cache:stats        â”‚     â”‚
              â”‚  â”‚   (æŸ¥è©¢å¿«å–)          â”‚     â”‚
              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
              â”‚                                â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
**è§’è‰²æ¯”è¼ƒ**ï¼š
  
| ç‰¹æ€§ | Stream (è¨Šæ¯ä½‡åˆ—) | Cache (è³‡æ–™å¿«å–) |
|------|-------------------|------------------|
| **è³‡æ–™æµå‘** | å¯«å…¥ â†’ æ¶ˆè²» | è®€å– â† å¿«å– |
| **æŒä¹…æ€§** | æ°¸ä¹…ä¿å­˜ï¼ˆAOFï¼‰| æœ‰é™æœŸï¼ˆTTLï¼‰|
| **Key æ•¸é‡** | 1 å€‹ Stream | å¤šå€‹å¿«å– Key |
| **è³‡æ–™çµæ§‹** | æ™‚åºåºåˆ— | Key-Value |
| **ä½¿ç”¨å‘½ä»¤** | XADD/XREADGROUP/XACK | GET/SETEX |
| **è¨˜æ†¶é«”ä½”ç”¨** | ~400MB | ~100MB |
  
### 8. Redis èˆ‡ PostgreSQL å”ä½œæ¨¡å¼
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Redis + PostgreSQL è³‡æ–™æµå®Œæ•´è·¯å¾‘               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
[å¯«å…¥è·¯å¾‘ - é«˜ååé‡]
Client â†’ FastAPI â†’ Redis Stream â†’ Worker â†’ PostgreSQL
                   (< 5ms)                  (~50ms/batch)
  
ç‰¹é»ï¼š
â”œâ”€ FastAPI åªè² è²¬å¯«å…¥ Redisï¼ˆæ¥µå¿«ï¼‰
â”œâ”€ Worker æ‰¹æ¬¡è™•ç†ï¼ˆæ•ˆç‡é«˜ï¼‰
â””â”€ PostgreSQL è² è²¬æŒä¹…åŒ–ï¼ˆå¯é ï¼‰
  
[è®€å–è·¯å¾‘ - ä½å»¶é²]
Client â† FastAPI â† Redis Cache    OR    PostgreSQL
                   (< 1ms)               (< 50ms)
  
ç‰¹é»ï¼š
â”œâ”€ å…ˆæŸ¥è©¢ Redis å¿«å–
â”œâ”€ å‘½ä¸­å‰‡ç›´æ¥è¿”å›ï¼ˆæ¥µå¿«ï¼‰
â””â”€ æœªå‘½ä¸­æ‰æŸ¥è©¢è³‡æ–™åº«
```
  
**ç‚ºä½•é€™æ¨£è¨­è¨ˆï¼Ÿ**
  
1. **å¯«å…¥è§£è€¦**ï¼š
   - FastAPI ä¸ç›´æ¥å¯«è³‡æ–™åº«ï¼Œé¿å…é˜»å¡
   - Redis Stream ä½œç‚ºç·©è¡å€ï¼Œå¸æ”¶æµé‡å³°å€¼
   - Worker æ‰¹æ¬¡å¯«å…¥ï¼Œæå‡è³‡æ–™åº«æ•ˆç‡
  
2. **è®€å–åŠ é€Ÿ**ï¼š
   - ç†±é–€æŸ¥è©¢çµæœå¿«å–åœ¨ Redis
   - æ¸›å°‘è³‡æ–™åº«æŸ¥è©¢æ¬¡æ•¸
   - é™ä½ PostgreSQL è² è¼‰
  
3. **æ•…éšœéš”é›¢**ï¼š
   - PostgreSQL æš«æ™‚æ•…éšœ â†’ æ—¥èªŒä»å­˜åœ¨ Redis Stream
   - Redis æ•…éšœ â†’ FastAPI å¯ä»¥é™ç´šè™•ç†
   - Worker æ•…éšœ â†’ è¨Šæ¯ä¿ç•™åœ¨ Pending List
  
## Worker å·¥ä½œæµç¨‹
  
### Docker Compose é…ç½®
  
```yaml
worker:
  build:
    context: ./app
    dockerfile: Dockerfile
  container_name: log-worker
  command: python worker.py
  environment:
    - POSTGRES_HOST=postgres
    - POSTGRES_PORT=5432
    - POSTGRES_USER=loguser
    - POSTGRES_PASSWORD=logpass
    - POSTGRES_DB=logsdb
    - REDIS_HOST=redis
    - REDIS_PORT=6379
    - WORKER_NAME=worker-1
```
  
- `command: python worker.py`: åŸ·è¡Œç¨ç«‹çš„ Worker è…³æœ¬
- Worker ä½¿ç”¨ Redis Stream æ¶ˆè²»è€…ç¾¤çµ„æ¨¡å¼å¾ 'logs:stream' è®€å–æ—¥èªŒ
- å°‡æ—¥èªŒæ‰¹æ¬¡å¯«å…¥ PostgreSQL ä»¥æé«˜æ•ˆèƒ½
- ä½¿ç”¨ `xreadgroup` æ¶ˆè²» Redis Stream è¨Šæ¯
  
### Worker æ ¸å¿ƒé‚è¼¯
  
1. **è¨Šæ¯æ¶ˆè²»**: å¾ Redis Stream çš„ 'log_workers' ç¾¤çµ„ä¸­è®€å–è¨Šæ¯
2. **æ‰¹æ¬¡è™•ç†**: å°‡å¤šå€‹æ—¥èªŒè¨Šæ¯çµ„æˆæ‰¹æ¬¡ï¼Œæé«˜å¯«å…¥æ•ˆç‡
3. **æ•¸æ“šæŒä¹…åŒ–**: ä½¿ç”¨åŸç”Ÿ SQL æ‰¹æ¬¡æ’å…¥åˆ° PostgreSQL
4. **ç¢ºèªæ©Ÿåˆ¶**: è™•ç†å®Œæˆå¾Œå‘ Redis ç™¼é€ ACK ä»¥é¿å…é‡è¤‡è™•ç†
5. **éŒ¯èª¤è™•ç†**: å¯¦ç¾äº†éŒ¯èª¤é‡è©¦å’Œå®¹éŒ¯æ©Ÿåˆ¶
  
## Worker è©³ç´°å¯¦ä½œ
  
### 1. Worker é…ç½®åƒæ•¸
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 14-24 è¡Œ)
  
```python
# ç’°å¢ƒè®Šæ•¸é…ç½®
REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
WORKER_NAME = os.getenv('WORKER_NAME', f'worker-{int(time.time())}')
  
# Redis Stream é…ç½®
STREAM_NAME = 'logs:stream'        # Stream åç¨±ï¼ˆèˆ‡ FastAPI ä¸€è‡´ï¼‰
GROUP_NAME = 'log_workers'         # æ¶ˆè²»è€…ç¾¤çµ„åç¨±
BATCH_SIZE = 100                   # æ¯æ‰¹æ¬¡è™•ç† 100 ç­†æ—¥èªŒ
BLOCK_MS = 5000                    # é˜»å¡ç­‰å¾… 5 ç§’
```
  
**é—œéµé…ç½®åƒæ•¸**:
| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| `BATCH_SIZE` | 100 | æ¯æ¬¡è®€å–æœ€å¤š 100 ç­†è¨Šæ¯ |
| `BLOCK_MS` | 5000 | ç„¡è¨Šæ¯æ™‚é˜»å¡ 5 ç§’å¾Œè¿”å› |
| `STREAM_NAME` | logs:stream | Redis Stream éµå |
| `GROUP_NAME` | log_workers | æ¶ˆè²»è€…ç¾¤çµ„åç¨± |
  
### 2. Redis é€£ç·šåˆå§‹åŒ–
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 44-87 è¡Œ)
  
```python
def init_redis():
    """
    åˆå§‹åŒ– Redis é€£ç·š
    """
    global redis_client
  
    try:
        redis_client = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            decode_responses=True,        # è‡ªå‹•è§£ç¢¼
            socket_connect_timeout=5,     # é€£ç·šè¶…æ™‚ 5 ç§’
            socket_keepalive=True,        # ä¿æŒé€£ç·š
            max_connections=10            # Worker ä½¿ç”¨è¼ƒå°‘é€£ç·š
        )
  
        # æ¸¬è©¦é€£ç·š
        redis_client.ping()
        print(f"âœ… Redis é€£ç·šæˆåŠŸ ({REDIS_HOST}:{REDIS_PORT})")
  
        # ç¢ºä¿æ¶ˆè²»è€…ç¾¤çµ„å­˜åœ¨
        try:
            redis_client.xgroup_create(
                name=STREAM_NAME,         # Stream åç¨±
                groupname=GROUP_NAME,     # ç¾¤çµ„åç¨±
                id='0',                   # å¾ Stream é–‹é ­é–‹å§‹
                mkstream=True             # è‹¥ Stream ä¸å­˜åœ¨å‰‡å‰µå»º
            )
            print(f"âœ… å»ºç«‹æ¶ˆè²»è€…ç¾¤çµ„: {GROUP_NAME}")
        except redis.exceptions.ResponseError as e:
            if "BUSYGROUP" in str(e):
                print(f"â„¹ï¸ æ¶ˆè²»è€…ç¾¤çµ„å·²å­˜åœ¨: {GROUP_NAME}")
            else:
                raise
  
        return True
  
    except Exception as e:
        print(f"âŒ Redis é€£ç·šå¤±æ•—: {e}")
        return False
```
  
**æ¶ˆè²»è€…ç¾¤çµ„å‰µå»ºèªªæ˜**:
- **id='0'**: å¾ Stream é–‹é ­é–‹å§‹è®€å–ï¼ˆä¸è·³éæ­·å²è¨Šæ¯ï¼‰
- **mkstream=True**: è‹¥ Stream ä¸å­˜åœ¨å‰‡è‡ªå‹•å‰µå»º
- **BUSYGROUP**: ç¾¤çµ„å·²å­˜åœ¨æ™‚çš„æ¨™æº–éŒ¯èª¤ï¼Œå¯å®‰å…¨å¿½ç•¥
  
### 3. Redis Stream æ¶ˆè²»æ“ä½œ
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 195-201 è¡Œ)
  
```python
# å¾ Redis Stream è®€å–è¨Šæ¯
messages = redis_client.xreadgroup(
    groupname=GROUP_NAME,              # æ¶ˆè²»è€…ç¾¤çµ„: 'log_workers'
    consumername=WORKER_NAME,          # å”¯ä¸€çš„ Worker è­˜åˆ¥ç¢¼
    streams={STREAM_NAME: '>'},        # åªè®€å–æ–°çš„æœªè™•ç†è¨Šæ¯
    count=BATCH_SIZE,                  # æœ€å¤šè®€å– 100 ç­†
    block=BLOCK_MS                     # é˜»å¡ 5000 æ¯«ç§’
)
```
  
**XREADGROUP å‘½ä»¤åƒæ•¸**:
- **groupname**: æ¶ˆè²»è€…ç¾¤çµ„åç¨±
- **consumername**: Worker çš„å”¯ä¸€è­˜åˆ¥ç¢¼ï¼ˆæ”¯æ´å¤š Worker éƒ¨ç½²ï¼‰
- **streams**: æŒ‡å®šè¦è®€å–çš„ Streamï¼Œ`>` è¡¨ç¤ºåªè®€å–æ–°è¨Šæ¯
- **count**: å–®æ¬¡è®€å–çš„æœ€å¤§è¨Šæ¯æ•¸é‡
- **block**: ç„¡è¨Šæ¯æ™‚çš„é˜»å¡ç­‰å¾…æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰
  
**æ¶ˆè²»è€…ç¾¤çµ„çš„å„ªå‹¢**:
1. **è‡ªå‹•è² è¼‰å‡è¡¡**: å¤šå€‹ Worker è‡ªå‹•åˆ†é…è¨Šæ¯
2. **è¨Šæ¯è¿½è¹¤**: æœªç¢ºèªçš„è¨Šæ¯æœƒä¿ç•™åœ¨ Pending Entries List
3. **æ•…éšœæ¢å¾©**: Worker å´©æ½°å¾Œå¯é‡æ–°è™•ç†æœªç¢ºèªè¨Šæ¯
4. **é˜²æ­¢é‡è¤‡**: åŒä¸€è¨Šæ¯åªæœƒè¢«ä¸€å€‹ Worker è™•ç†
  
### 4. è¨Šæ¯è§£æèˆ‡è½‰æ›
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 132-175 è¡Œ)
  
```python
def process_messages(messages):
    """
    è™•ç†å¾ Redis Stream è®€å–çš„è¨Šæ¯
  
    åƒæ•¸ï¼š
        messages: Redis Stream è¨Šæ¯åˆ—è¡¨
  
    è¿”å›ï¼š
        tuple: (logs_to_insert, message_ids)
  
    è¨Šæ¯æ ¼å¼:
    [
        (stream_name, [
            (message_id, {
                'device_id': 'device-001',
                'log_level': 'INFO',
                'message': 'Log content',
                'log_data': '{"key": "value"}',
                'timestamp': '2024-01-01T12:00:00'
            }),
            ...
        ]),
        ...
    ]
    """
    logs_to_insert = []
    message_ids = []
  
    for stream_name, stream_messages in messages:
        for message_id, message_data in stream_messages:
            try:
                # è§£ææ—¥èªŒè³‡æ–™
                log_data_str = message_data.get('log_data', '{}')
  
                # ç¢ºä¿ log_data æ˜¯å­—ä¸²æ ¼å¼
                if isinstance(log_data_str, dict):
                    log_data_str = json.dumps(log_data_str)
  
                # æ§‹å»ºè³‡æ–™åº«æ’å…¥è¨˜éŒ„
                log_entry = {
                    'device_id': message_data['device_id'],
                    'log_level': message_data['log_level'],
                    'message': message_data['message'],
                    'log_data': log_data_str,
                    'created_at': message_data.get('timestamp', datetime.now().isoformat())
                }
  
                logs_to_insert.append(log_entry)
                message_ids.append(message_id)
  
            except Exception as e:
                print(f"âŒ è§£æè¨Šæ¯å¤±æ•— ({message_id}): {e}")
                # ä»ç„¶è¨˜éŒ„ message_idï¼Œä»¥ä¾¿ ACKï¼ˆé¿å…é‡è¤‡è™•ç†ï¼‰
                message_ids.append(message_id)
  
    return logs_to_insert, message_ids
```
  
**è³‡æ–™è½‰æ›æµç¨‹**:
1. éæ­·æ‰€æœ‰ Stream è¨Šæ¯
2. è§£ææ¯å€‹è¨Šæ¯çš„æ¬„ä½
3. ç¢ºä¿ `log_data` ç‚º JSON å­—ä¸²æ ¼å¼
4. æ§‹å»ºç¬¦åˆ PostgreSQL è¡¨çµæ§‹çš„å­—å…¸
5. æ”¶é›†æ‰€æœ‰ message_id ç”¨æ–¼å¾ŒçºŒ ACK
  
### 5. PostgreSQL æ‰¹æ¬¡å¯«å…¥
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 88-131 è¡Œ)
  
```python
def batch_insert_logs(logs_data):
    """
    æ‰¹æ¬¡æ’å…¥æ—¥èªŒåˆ° PostgreSQL
  
    åƒæ•¸ï¼š
        logs_data: list of dictï¼ŒåŒ…å«æ—¥èªŒè³‡æ–™
  
    è¿”å›ï¼š
        bool: æ˜¯å¦æˆåŠŸ
  
    ä½¿ç”¨åŸç”Ÿ SQL åƒæ•¸åŒ–æŸ¥è©¢ä»¥ç²å¾—æœ€ä½³æ•ˆèƒ½
    ï¼ˆæ¯” ORM å¿«ç´„ 3-5 å€ï¼‰
    """
    if not logs_data:
        return True
  
    session = SyncSessionLocal()
  
    try:
        # ä½¿ç”¨åŸç”Ÿ SQL æ‰¹æ¬¡æ’å…¥ï¼ˆæ•ˆèƒ½æœ€ä½³ï¼‰
        stmt = text("""
            INSERT INTO logs (device_id, log_level, message, log_data, created_at)
            VALUES (:device_id, :log_level, :message, CAST(:log_data AS jsonb), :created_at)
        """)
  
        # åŸ·è¡Œæ‰¹æ¬¡æ’å…¥ï¼ˆæ‰€æœ‰è³‡æ–™åœ¨å–®ä¸€äº¤æ˜“ä¸­ï¼‰
        session.execute(stmt, logs_data)
        session.commit()
  
        print(f"âœ… æˆåŠŸå¯«å…¥ {len(logs_data)} ç­†æ—¥èªŒåˆ°è³‡æ–™åº«")
        return True
  
    except Exception as e:
        session.rollback()
        print(f"âŒ æ‰¹æ¬¡å¯«å…¥å¤±æ•—: {e}")
        return False
  
    finally:
        session.close()
```
  
**æ•ˆèƒ½å„ªåŒ–è¦é»**:
- **åŸç”Ÿ SQL**: é¿å… ORM é–‹éŠ·ï¼Œç›´æ¥ä½¿ç”¨åƒæ•¸åŒ– SQL
- **CAST AS JSONB**: ç¢ºä¿æ­£ç¢ºçš„ PostgreSQL é¡å‹è½‰æ›
- **å–®ä¸€äº¤æ˜“**: æ‰€æœ‰æ’å…¥åœ¨åŒä¸€äº¤æ˜“ä¸­ï¼Œä¿è­‰åŸå­æ€§
- **æ‰¹æ¬¡å¤§å°**: 100 ç­†/æ‰¹æ¬¡ï¼Œå¹³è¡¡æ•ˆèƒ½èˆ‡è¨˜æ†¶é«”ä½¿ç”¨
  
### 6. è¨Šæ¯ç¢ºèªæ©Ÿåˆ¶ (ACK)
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 217-224 è¡Œ)
  
```python
if success:
    # ACK å·²è™•ç†çš„è¨Šæ¯
    for message_id in message_ids:
        try:
            redis_client.xack(STREAM_NAME, GROUP_NAME, message_id)
        except Exception as e:
            print(f"âŒ ACK å¤±æ•— ({message_id}): {e}")
  
    print(f"ğŸ“ è™•ç†å®Œæˆ: {len(logs_to_insert)} ç­†æ—¥èªŒ")
    error_count = 0  # é‡ç½®éŒ¯èª¤è¨ˆæ•¸
```
  
**XACK å‘½ä»¤èªªæ˜**:
- å¾æ¶ˆè²»è€…ç¾¤çµ„çš„ Pending Entries List ä¸­ç§»é™¤è¨Šæ¯
- æ¨™è¨˜è¨Šæ¯å·²è¢«æˆåŠŸè™•ç†
- é˜²æ­¢ Worker å´©æ½°å¾Œé‡è¤‡è™•ç†
  
**æœªç¢ºèªè¨Šæ¯çš„è™•ç†**:
```python
# æŸ¥çœ‹æœªç¢ºèªè¨Šæ¯
pending_info = redis_client.xpending(STREAM_NAME, GROUP_NAME)
# {'pending': 5, 'min': '1234-0', 'max': '1238-0', 'consumers': {...}}
  
# é‡æ–°åˆ†é…è¶…æ™‚è¨Šæ¯çµ¦å…¶ä»– Worker
redis_client.xclaim(
    STREAM_NAME,
    GROUP_NAME,
    'worker-2',        # æ–°çš„ Worker
    min_idle_time=60000,  # é–’ç½®è¶…é 60 ç§’
    message_ids=['1234-0', '1235-0']
)
```
  
### 7. ä¸»å¾ªç’°èˆ‡éŒ¯èª¤è™•ç†
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 179-263 è¡Œ)
  
```python
def worker_loop():
    """
    Worker ä¸»è¦å·¥ä½œå¾ªç’°
  
    è™•ç†æµç¨‹:
    1. å¾ Redis Stream è®€å–è¨Šæ¯
    2. è§£æä¸¦è½‰æ›è¨Šæ¯æ ¼å¼
    3. æ‰¹æ¬¡å¯«å…¥ PostgreSQL
    4. ç¢ºèªè¨Šæ¯å·²è™•ç†
    5. éŒ¯èª¤é‡è©¦èˆ‡å®¹éŒ¯
    """
    global running
  
    print(f"ğŸš€ å•Ÿå‹• Worker: {WORKER_NAME}")
    print(f"ğŸ“Š è¨­å®š: æ‰¹æ¬¡å¤§å°={BATCH_SIZE}, é˜»å¡æ™‚é–“={BLOCK_MS}ms")
    print("-" * 60)
  
    error_count = 0
    max_errors = 10  # æœ€å¤§é€£çºŒéŒ¯èª¤æ¬¡æ•¸
  
    while running:
        try:
            # 1. å¾ Redis Stream æ‰¹æ¬¡è®€å–è¨Šæ¯
            messages = redis_client.xreadgroup(
                groupname=GROUP_NAME,
                consumername=WORKER_NAME,
                streams={STREAM_NAME: '>'},
                count=BATCH_SIZE,
                block=BLOCK_MS
            )
  
            # æ²’æœ‰æ–°è¨Šæ¯
            if not messages:
                continue
  
            # 2. è™•ç†è¨Šæ¯
            logs_to_insert, message_ids = process_messages(messages)
  
            if not logs_to_insert:
                print("âš ï¸ æ²’æœ‰æœ‰æ•ˆçš„æ—¥èªŒè³‡æ–™")
                continue
  
            # 3. æ‰¹æ¬¡å¯«å…¥ PostgreSQL
            success = batch_insert_logs(logs_to_insert)
  
            if success:
                # 4. ACK å·²è™•ç†çš„è¨Šæ¯
                for message_id in message_ids:
                    try:
                        redis_client.xack(STREAM_NAME, GROUP_NAME, message_id)
                    except Exception as e:
                        print(f"âŒ ACK å¤±æ•— ({message_id}): {e}")
  
                print(f"ğŸ“ è™•ç†å®Œæˆ: {len(logs_to_insert)} ç­†æ—¥èªŒ")
                error_count = 0  # é‡ç½®éŒ¯èª¤è¨ˆæ•¸
            else:
                error_count += 1
                print(f"âš ï¸ è™•ç†å¤±æ•—ï¼ŒéŒ¯èª¤æ¬¡æ•¸: {error_count}/{max_errors}")
  
                if error_count >= max_errors:
                    print(f"âŒ éŒ¯èª¤æ¬¡æ•¸éå¤šï¼Œåœæ­¢ Worker")
                    break
  
                # ç­‰å¾…å¾Œé‡è©¦
                time.sleep(5)
  
        except redis.exceptions.ConnectionError as e:
            print(f"âŒ Redis é€£ç·šéŒ¯èª¤: {e}")
            error_count += 1
  
            if error_count >= max_errors:
                print(f"âŒ é€£ç·šéŒ¯èª¤æ¬¡æ•¸éå¤šï¼Œåœæ­¢ Worker")
                break
  
            print(f"â³ 5ç§’å¾Œé‡æ–°é€£ç·š...")
            time.sleep(5)
  
            # å˜—è©¦é‡æ–°é€£ç·š
            if not init_redis():
                print("âŒ Redis é‡æ–°é€£ç·šå¤±æ•—")
                break
  
        except Exception as e:
            print(f"âŒ Worker ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
            error_count += 1
  
            if error_count >= max_errors:
                print(f"âŒ éŒ¯èª¤æ¬¡æ•¸éå¤šï¼Œåœæ­¢ Worker")
                break
  
            time.sleep(1)
```
  
**éŒ¯èª¤è™•ç†ç­–ç•¥**:
| éŒ¯èª¤é¡å‹ | è™•ç†æ–¹å¼ | ç­‰å¾…æ™‚é–“ |
|----------|----------|----------|
| è³‡æ–™åº«å¯«å…¥å¤±æ•— | é‡è©¦ | 5 ç§’ |
| Redis é€£ç·šéŒ¯èª¤ | é‡æ–°é€£ç·š | 5 ç§’ |
| ä¸€èˆ¬éŒ¯èª¤ | é‡è©¦ | 1 ç§’ |
| é€£çºŒ 10 æ¬¡å¤±æ•— | åœæ­¢ Worker | - |
  
### 8. å„ªé›…åœæ©Ÿè™•ç†
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 26-42 è¡Œ)
  
```python
# å…¨åŸŸè®Šæ•¸
running = True
redis_client = None
  
# ==========================================
# è¨Šè™Ÿè™•ç†
# ==========================================
def signal_handler(sig, frame):
    """
    è™•ç† SIGINT å’Œ SIGTERM è¨Šè™Ÿï¼ˆå„ªé›…é—œé–‰ï¼‰
    """
    global running
    print(f"\nğŸ›‘ æ¥æ”¶åˆ°è¨Šè™Ÿ {sig}ï¼Œæº–å‚™é—œé–‰ Worker...")
    running = False
  
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)
```
  
**è³‡æºæ¸…ç†** (`app/worker.py` ç¬¬ 267-288 è¡Œ):
  
```python
def cleanup():
    """
    æ¸…ç†è³‡æº
    """
    global redis_client
  
    print("\nğŸ§¹ æ¸…ç†è³‡æº...")
  
    if redis_client:
        try:
            redis_client.close()
            print("âœ… Redis é€£ç·šå·²é—œé–‰")
        except Exception as e:
            print(f"âš ï¸ é—œé–‰ Redis é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
  
    if sync_engine:
        try:
            sync_engine.dispose()
            print("âœ… è³‡æ–™åº«é€£ç·šæ± å·²é—œé–‰")
        except Exception as e:
            print(f"âš ï¸ é—œé–‰è³‡æ–™åº«é€£ç·šæ± æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
```
  
**ä¸»ç¨‹å¼å…¥å£** (`app/worker.py` ç¬¬ 292-326 è¡Œ):
  
```python
def main():
    """
    ä¸»ç¨‹å¼å…¥å£
    """
    print("=" * 60)
    print("  ğŸ“¦ æ—¥èªŒæ”¶é›†ç³»çµ± - èƒŒæ™¯ Worker")
    print("=" * 60)
  
    # åˆå§‹åŒ– Redis
    if not init_redis():
        print("âŒ ç„¡æ³•å•Ÿå‹• Workerï¼ŒRedis é€£ç·šå¤±æ•—")
        sys.exit(1)
  
    # æ¸¬è©¦è³‡æ–™åº«é€£ç·š
    try:
        with sync_engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print(f"âœ… PostgreSQL é€£ç·šæˆåŠŸ")
    except Exception as e:
        print(f"âŒ PostgreSQL é€£ç·šå¤±æ•—: {e}")
        sys.exit(1)
  
    print("-" * 60)
  
    try:
        # é–‹å§‹å·¥ä½œå¾ªç’°
        worker_loop()
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¥æ”¶åˆ°éµç›¤ä¸­æ–·")
    finally:
        cleanup()
        print("ğŸ‘‹ Worker å·²åœæ­¢")
  
if __name__ == "__main__":
    main()
```
  
**å„ªé›…åœæ©Ÿæµç¨‹**:
1. æ¥æ”¶åˆ°åœæ­¢ä¿¡è™Ÿï¼ˆSIGINT æˆ– SIGTERMï¼‰
2. è¨­ç½® `running = False`
3. å®Œæˆç•¶å‰æ‰¹æ¬¡è™•ç†ï¼ˆworker_loop æœƒåœ¨ä¸‹ä¸€æ¬¡è¿­ä»£æª¢æŸ¥ running ç‹€æ…‹ï¼‰
4. ç¢ºèªæ‰€æœ‰å·²è™•ç†è¨Šæ¯ï¼ˆACKï¼‰
5. åŸ·è¡Œ cleanup() æ¸…ç†è³‡æº
6. é—œé–‰ Redis é€£ç·šå’Œè³‡æ–™åº«é€£ç·šæ± 
7. é€€å‡ºç¨‹å¼
  
**Docker åœæ­¢æ™‚çš„è¡Œç‚º**:
```bash
docker-compose stop worker
# ç™¼é€ SIGTERM â†’ å„ªé›…åœæ©Ÿ
# è‹¥ 10 ç§’å…§æœªåœæ­¢ â†’ ç™¼é€ SIGKILL
```
  
## Worker å¾ Redis è®€å–åˆ° PostgreSQL å¯«å…¥çš„å®Œæ•´æµç¨‹
  
æœ¬ç¯€æ·±å…¥å‰–æ Worker å¦‚ä½•å¾ Redis Stream è®€å–æ—¥èªŒè³‡æ–™ï¼Œä¸¦æ‰¹æ¬¡å¯«å…¥ PostgreSQL è³‡æ–™åº«çš„å®Œæ•´æ©Ÿåˆ¶ã€‚
  
### 1. è³‡æ–™è®€å¯«æ¶æ§‹æ¦‚è¦½
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Worker è³‡æ–™è™•ç†å®Œæ•´æµç¨‹                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
[Redis Stream: logs:stream]
       â”‚
       â”‚ Step 1: XREADGROUP æ‰¹æ¬¡è®€å–
       â”‚ â”œâ”€ æ¶ˆè²»è€…ç¾¤çµ„: log_workers
       â”‚ â”œâ”€ æ¶ˆè²»è€…åç¨±: worker-1
       â”‚ â”œâ”€ æ‰¹æ¬¡å¤§å°: 100 ç­†
       â”‚ â””â”€ é˜»å¡æ™‚é–“: 5000ms
       â–¼
[Worker Memory: Raw Messages]
       â”‚
       â”‚ Step 2: process_messages() è§£æè½‰æ›
       â”‚ â”œâ”€ éæ­·æ¯å€‹è¨Šæ¯
       â”‚ â”œâ”€ è§£æ JSON æ ¼å¼
       â”‚ â”œâ”€ é©—è­‰è³‡æ–™å®Œæ•´æ€§
       â”‚ â””â”€ è½‰æ›ç‚º PostgreSQL æ ¼å¼
       â–¼
[Worker Memory: logs_to_insert]
       â”‚
       â”‚ Step 3: batch_insert_logs() æ‰¹æ¬¡å¯«å…¥
       â”‚ â”œâ”€ å»ºç«‹è³‡æ–™åº« Session
       â”‚ â”œâ”€ ä½¿ç”¨åŸç”Ÿ SQL (text())
       â”‚ â”œâ”€ æ‰¹æ¬¡åŸ·è¡Œ INSERT
       â”‚ â””â”€ æäº¤äº¤æ˜“ (commit)
       â–¼
[PostgreSQL: logs table]
       â”‚
       â”‚ Step 4: XACK ç¢ºèªè™•ç†å®Œæˆ
       â”‚ â”œâ”€ é€ä¸€ç¢ºèªæ¯å€‹ message_id
       â”‚ â””â”€ å¾ Pending List ç§»é™¤
       â–¼
[Redis Stream: Message Acknowledged]
```
  
### 2. Redis è®€å–éšæ®µï¼šXREADGROUP è©³è§£
  
#### 2.1 XREADGROUP å‘½ä»¤åŸ·è¡Œ
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 198-204 è¡Œ)
  
```python
# å¾ Redis Stream æ‰¹æ¬¡è®€å–è¨Šæ¯
messages = redis_client.xreadgroup(
    groupname=GROUP_NAME,              # 'log_workers'
    consumername=WORKER_NAME,          # 'worker-1' (å”¯ä¸€è­˜åˆ¥ç¢¼)
    streams={STREAM_NAME: '>'},        # {'logs:stream': '>'}
    count=BATCH_SIZE,                  # 100 (æ¯æ‰¹æ¬¡æœ€å¤šè®€å– 100 ç­†)
    block=BLOCK_MS                     # 5000 (é˜»å¡ç­‰å¾… 5 ç§’)
)
```
  
**åƒæ•¸è©³è§£**:
  
| åƒæ•¸ | å€¼ | èªªæ˜ |
|------|-----|------|
| `groupname` | `'log_workers'` | æ¶ˆè²»è€…ç¾¤çµ„åç¨±ï¼Œå¤šå€‹ Worker å…±äº«æ­¤ç¾¤çµ„ |
| `consumername` | `WORKER_NAME` | ç•¶å‰ Worker çš„å”¯ä¸€è­˜åˆ¥ç¢¼ï¼ˆå¦‚ worker-1ï¼‰ |
| `streams` | `{'logs:stream': '>'}` | Stream åç¨±èˆ‡è®€å–ä½ç½®ï¼ˆ`>` è¡¨ç¤ºåªè®€å–æ–°è¨Šæ¯ï¼‰ |
| `count` | `100` | å–®æ¬¡è®€å–çš„æœ€å¤§è¨Šæ¯æ•¸é‡ |
| `block` | `5000` | ç„¡è¨Šæ¯æ™‚çš„é˜»å¡ç­‰å¾…æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰ |
  
#### 2.2 XREADGROUP è¿”å›è³‡æ–™çµæ§‹
  
```python
# messages çš„å¯¦éš›æ ¼å¼ï¼š
[
    (
        'logs:stream',  # Stream åç¨±
        [
            (
                '1704067200000-0',  # Message IDï¼ˆæ™‚é–“æˆ³-åºåˆ—è™Ÿï¼‰
                {
                    'device_id': 'device-001',
                    'log_level': 'INFO',
                    'message': 'Application started',
                    'log_data': '{"version": "1.0.0"}',
                    'timestamp': '2024-01-01T12:00:00+08:00'
                }
            ),
            (
                '1704067200001-0',  # ç¬¬äºŒç­†è¨Šæ¯
                {
                    'device_id': 'device-002',
                    'log_level': 'ERROR',
                    'message': 'Connection failed',
                    'log_data': '{"error_code": 500}',
                    'timestamp': '2024-01-01T12:00:01+08:00'
                }
            ),
            # ... æœ€å¤š 100 ç­†è¨Šæ¯
        ]
    )
]
```
  
#### 2.3 é˜»å¡ç­‰å¾…æ©Ÿåˆ¶
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   XREADGROUP é˜»å¡ç­‰å¾…æµç¨‹                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
Worker åŸ·è¡Œ XREADGROUP (block=5000)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis æª¢æŸ¥      â”‚ â† æ˜¯å¦æœ‰æ–°è¨Šæ¯ï¼Ÿ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€ æœ‰æ–°è¨Šæ¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                 â”‚
       â”‚                                 â–¼
       â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                        â”‚ ç«‹å³è¿”å›è¨Šæ¯     â”‚
       â”‚                        â”‚ (< 1ms)         â”‚
       â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€ ç„¡æ–°è¨Šæ¯ â”€â”€â”€â”€â”
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ é˜»å¡ç­‰å¾…         â”‚
              â”‚ (æœ€å¤š 5000ms)   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â”œâ”€ æœ‰æ–°è¨Šæ¯åˆ°é” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚                        â”‚
                      â”‚                        â–¼
                      â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚               â”‚ è¿”å›æ–°è¨Šæ¯       â”‚
                      â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â””â”€ è¶…æ™‚ (5000ms) â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚
                                               â–¼
                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚ è¿”å›ç©ºåˆ—è¡¨ []    â”‚
                                      â”‚ Worker ç¹¼çºŒå¾ªç’°  â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
**é˜»å¡ç­‰å¾…çš„å„ªå‹¢**:
- **æ¸›å°‘ CPU æ¶ˆè€—**: ä¸ç”¨é »ç¹è¼ªè©¢ Redis
- **å³æ™‚éŸ¿æ‡‰**: æ–°è¨Šæ¯åˆ°é”æ™‚ç«‹å³è¿”å›
- **é™ä½å»¶é²**: å¹³å‡è™•ç†å»¶é²é™ä½è‡³æ¯«ç§’ç´š
  
### 3. è¨Šæ¯è™•ç†éšæ®µï¼šprocess_messages() è©³è§£
  
#### 3.1 è¨Šæ¯è§£æèˆ‡è½‰æ›
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 136-177 è¡Œ)
  
```python
def process_messages(messages):
    """
    è™•ç†å¾ Redis Stream è®€å–çš„è¨Šæ¯
  
    è½‰æ›æµç¨‹:
    1. Redis Stream æ ¼å¼ â†’ Python Dict
    2. è³‡æ–™é©—è­‰èˆ‡æ¸…ç†
    3. PostgreSQL ç›¸å®¹æ ¼å¼
    """
    logs_to_insert = []  # PostgreSQL æ’å…¥è³‡æ–™
    message_ids = []     # Redis ACK åˆ—è¡¨
  
    # éæ­·æ‰€æœ‰ Streamï¼ˆé€šå¸¸åªæœ‰ä¸€å€‹ 'logs:stream'ï¼‰
    for stream_name, stream_messages in messages:
        # éæ­·è©² Stream ä¸­çš„æ‰€æœ‰è¨Šæ¯
        for message_id, message_data in stream_messages:
            try:
                # === è³‡æ–™æ¸…ç†èˆ‡é©—è­‰ ===
  
                # 1. è™•ç† log_data æ¬„ä½ï¼ˆå¯èƒ½æ˜¯ dict æˆ– stringï¼‰
                log_data_str = message_data.get('log_data', '{}')
  
                # ç¢ºä¿ log_data æ˜¯å­—ä¸²æ ¼å¼ï¼ˆPostgreSQL JSONB éœ€è¦ï¼‰
                if isinstance(log_data_str, dict):
                    log_data_str = json.dumps(log_data_str)
  
                # 2. è™•ç†æ™‚é–“æˆ³ï¼ˆä½¿ç”¨ Asia/Taipei æ™‚å€ï¼‰
                timestamp_str = message_data.get(
                    'timestamp',
                    datetime.now(ZoneInfo("Asia/Taipei")).isoformat()
                )
  
                # === æ§‹å»º PostgreSQL æ’å…¥è¨˜éŒ„ ===
                log_entry = {
                    'device_id': message_data['device_id'],      # è¨­å‚™ ID
                    'log_level': message_data['log_level'],      # æ—¥èªŒç´šåˆ¥
                    'message': message_data['message'],          # æ—¥èªŒè¨Šæ¯
                    'log_data': log_data_str,                    # JSON é¡å¤–è³‡æ–™
                    'created_at': timestamp_str                  # å»ºç«‹æ™‚é–“
                }
  
                logs_to_insert.append(log_entry)
                message_ids.append(message_id)
  
            except Exception as e:
                print(f"âŒ è§£æè¨Šæ¯å¤±æ•— ({message_id}): {e}")
                # å³ä½¿è§£æå¤±æ•—ï¼Œä»ç„¶è¨˜éŒ„ message_id
                # ç¨å¾Œ ACK æ­¤è¨Šæ¯ï¼Œé¿å…é‡è¤‡è™•ç†
                message_ids.append(message_id)
  
    return logs_to_insert, message_ids
```
  
#### 3.2 è³‡æ–™è½‰æ›ç¯„ä¾‹
  
**è¼¸å…¥ (Redis Stream æ ¼å¼)**:
```python
message_data = {
    'device_id': 'device-001',
    'log_level': 'ERROR',
    'message': 'Database connection timeout',
    'log_data': '{"retry_count": 3, "timeout": 30}',  # JSON å­—ä¸²
    'timestamp': '2024-01-01T12:00:00+08:00'
}
```
  
**è¼¸å‡º (PostgreSQL æ ¼å¼)**:
```python
log_entry = {
    'device_id': 'device-001',
    'log_level': 'ERROR',
    'message': 'Database connection timeout',
    'log_data': '{"retry_count": 3, "timeout": 30}',  # ä¿æŒ JSON å­—ä¸²æ ¼å¼
    'created_at': '2024-01-01T12:00:00+08:00'
}
```
  
#### 3.3 éŒ¯èª¤è™•ç†æ©Ÿåˆ¶
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               è¨Šæ¯è§£æéŒ¯èª¤è™•ç†ç­–ç•¥                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
for message_id, message_data in stream_messages:
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ try: è§£æè¨Šæ¯    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€ æˆåŠŸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                              â”‚
    â”‚                              â–¼
    â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     â”‚ æ·»åŠ åˆ°æ’å…¥åˆ—è¡¨   â”‚
    â”‚                     â”‚ æ·»åŠ åˆ° ACK åˆ—è¡¨  â”‚
    â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â””â”€ å¤±æ•— (Exception) â”€â”€â”€â”
                           â”‚
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ è¨˜éŒ„éŒ¯èª¤è¨Šæ¯     â”‚
                  â”‚ print(f"âŒ...") â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ ä»æ·»åŠ åˆ° ACK åˆ—è¡¨â”‚
                  â”‚ (é¿å…é‡è¤‡è™•ç†)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
**è¨­è¨ˆè€ƒé‡**:
- **é¿å…é˜»å¡**: å–®ç­†è¨Šæ¯è§£æå¤±æ•—ä¸å½±éŸ¿å…¶ä»–è¨Šæ¯
- **é˜²æ­¢é‡è¤‡**: å¤±æ•—çš„è¨Šæ¯ä»ç„¶ ACKï¼Œä¸æœƒç„¡é™é‡è©¦
- **å¯è§€æ¸¬æ€§**: è¨˜éŒ„è©³ç´°éŒ¯èª¤è¨Šæ¯ï¼Œä¾¿æ–¼é™¤éŒ¯
  
### 4. PostgreSQL å¯«å…¥éšæ®µï¼šbatch_insert_logs() è©³è§£
  
#### 4.1 æ‰¹æ¬¡æ’å…¥å¯¦ä½œ
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 92-131 è¡Œ)
  
```python
def batch_insert_logs(logs_data):
    """
    æ‰¹æ¬¡æ’å…¥æ—¥èªŒåˆ° PostgreSQL
  
    æ•ˆèƒ½å„ªåŒ–:
    - ä½¿ç”¨åŸç”Ÿ SQLï¼ˆæ¯” ORM å¿« 3-5 å€ï¼‰
    - æ‰¹æ¬¡æäº¤ï¼ˆæ¸›å°‘äº¤æ˜“é–‹éŠ·ï¼‰
    - åƒæ•¸åŒ–æŸ¥è©¢ï¼ˆé˜²æ­¢ SQL æ³¨å…¥ï¼‰
    """
    if not logs_data:
        return True
  
    # å»ºç«‹åŒæ­¥è³‡æ–™åº« Session
    session = SyncSessionLocal()
  
    try:
        # === åŸç”Ÿ SQL æ‰¹æ¬¡æ’å…¥ ===
        stmt = text("""
            INSERT INTO logs (device_id, log_level, message, log_data, created_at)
            VALUES (:device_id, :log_level, :message, CAST(:log_data AS jsonb), :created_at)
        """)
  
        # æ‰¹æ¬¡åŸ·è¡Œæ‰€æœ‰æ’å…¥ï¼ˆå–®ä¸€äº¤æ˜“ï¼‰
        session.execute(stmt, logs_data)
        session.commit()
  
        print(f"âœ… æˆåŠŸå¯«å…¥ {len(logs_data)} ç­†æ—¥èªŒåˆ°è³‡æ–™åº«")
        return True
  
    except Exception as e:
        # ç™¼ç”ŸéŒ¯èª¤æ™‚å›æ»¾äº¤æ˜“
        session.rollback()
        print(f"âŒ æ‰¹æ¬¡å¯«å…¥å¤±æ•—: {e}")
        return False
  
    finally:
        # ç¢ºä¿ Session é—œé–‰
        session.close()
```
  
#### 4.2 åŸç”Ÿ SQL vs ORM æ•ˆèƒ½æ¯”è¼ƒ
  
**ä½¿ç”¨åŸç”Ÿ SQL (ç•¶å‰å¯¦ä½œ)**:
```python
# å–®ä¸€ SQL èªå¥ + æ‰¹æ¬¡åƒæ•¸
stmt = text("""
    INSERT INTO logs (device_id, log_level, message, log_data, created_at)
    VALUES (:device_id, :log_level, :message, CAST(:log_data AS jsonb), :created_at)
""")
session.execute(stmt, logs_data)  # logs_data: list of dict
```
  
**æ•ˆèƒ½**: ~50ms / 100 ç­†
  
**ä½¿ç”¨ ORM (è¼ƒæ…¢çš„æ›¿ä»£æ–¹æ¡ˆ)**:
```python
# ç‚ºæ¯ç­†è³‡æ–™å»ºç«‹ ORM ç‰©ä»¶
for log in logs_data:
    log_obj = Log(
        device_id=log['device_id'],
        log_level=log['log_level'],
        message=log['message'],
        log_data=log['log_data'],
        created_at=log['created_at']
    )
    session.add(log_obj)
session.commit()
```
  
**æ•ˆèƒ½**: ~150-200ms / 100 ç­†
  
**æ•ˆèƒ½å·®ç•°åŸå› **:
1. ORM éœ€è¦å»ºç«‹ 100 å€‹ Python ç‰©ä»¶
2. ORM éœ€è¦è¿½è¹¤ç‰©ä»¶ç‹€æ…‹ï¼ˆDirty Checkingï¼‰
3. ORM ç”¢ç”Ÿçš„ SQL èªå¥è¼ƒå†—é•·
4. åŸç”Ÿ SQL ç›´æ¥åŸ·è¡Œï¼Œç„¡é¡å¤–é–‹éŠ·
  
#### 4.3 JSONB é¡å‹è½‰æ›
  
**é‡é»**: `CAST(:log_data AS jsonb)`
  
```sql
-- PostgreSQL éœ€è¦æ˜ç¢ºçš„é¡å‹è½‰æ›
INSERT INTO logs (..., log_data, ...)
VALUES (..., CAST(:log_data AS jsonb), ...)
```
  
**ç‚ºä½•éœ€è¦ CASTï¼Ÿ**
- SQLAlchemy `text()` å°‡åƒæ•¸è¦–ç‚ºç´”æ–‡å­—
- PostgreSQL `log_data` æ¬„ä½é¡å‹ç‚º `JSONB`
- å¿…é ˆæ˜ç¢ºå‘ŠçŸ¥ PostgreSQL é€²è¡Œé¡å‹è½‰æ›
- å¦å‰‡æœƒå‡ºç¾é¡å‹ä¸åŒ¹é…éŒ¯èª¤
  
**éŒ¯èª¤ç¤ºç¯„**ï¼ˆæœªä½¿ç”¨ CASTï¼‰:
```python
# âŒ éŒ¯èª¤ï¼šæœƒå°è‡´ PostgreSQL é¡å‹éŒ¯èª¤
stmt = text("""
    INSERT INTO logs (device_id, log_level, message, log_data, created_at)
    VALUES (:device_id, :log_level, :message, :log_data, :created_at)
""")
# éŒ¯èª¤è¨Šæ¯: column "log_data" is of type jsonb but expression is of type text
```
  
#### 4.4 æ‰¹æ¬¡æ’å…¥æµç¨‹åœ–
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              batch_insert_logs() åŸ·è¡Œæµç¨‹                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
logs_data = [
    {'device_id': 'device-001', ...},
    {'device_id': 'device-002', ...},
    ...  # 100 ç­†è³‡æ–™
]
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ å»ºç«‹ Session     â”‚ â† SyncSessionLocal()
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æº–å‚™ SQL èªå¥    â”‚ â† text("""INSERT INTO logs ...""")
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åŸ·è¡Œæ‰¹æ¬¡æ’å…¥     â”‚ â† session.execute(stmt, logs_data)
â”‚ (å–®ä¸€äº¤æ˜“)       â”‚   PostgreSQL æ¥æ”¶ 100 ç­†è³‡æ–™
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€ æˆåŠŸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                              â”‚
       â”‚                              â–¼
       â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                     â”‚ session.commit()â”‚
       â”‚                     â”‚ æäº¤äº¤æ˜“         â”‚
       â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                              â”‚
       â”‚                              â–¼
       â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                     â”‚ return True     â”‚
       â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€ å¤±æ•— (Exception) â”€â”€â”€â”
                              â”‚
                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ session.rollbackâ”‚
                     â”‚ å›æ»¾äº¤æ˜“         â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ print éŒ¯èª¤è¨Šæ¯   â”‚
                     â”‚ return False    â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ session.close() â”‚ â† finally å€å¡Šç¢ºä¿åŸ·è¡Œ
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
### 5. è¨Šæ¯ç¢ºèªéšæ®µï¼šXACK è©³è§£
  
#### 5.1 XACK åŸ·è¡Œ
  
**æª”æ¡ˆä½ç½®**: `app/worker.py` (ç¬¬ 221-226 è¡Œ)
  
```python
if success:  # batch_insert_logs() è¿”å› True
    # ACK å·²è™•ç†çš„è¨Šæ¯
    for message_id in message_ids:
        try:
            redis_client.xack(
                STREAM_NAME,    # 'logs:stream'
                GROUP_NAME,     # 'log_workers'
                message_id      # '1704067200000-0'
            )
        except Exception as e:
            print(f"âŒ ACK å¤±æ•— ({message_id}): {e}")
```
  
#### 5.2 XACK å·¥ä½œåŸç†
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Redis Stream Pending Entries List (PEL)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
[XREADGROUP å¾Œ - è¨Šæ¯åˆ†é…]
  
Consumer Group: log_workers
â”œâ”€ Consumer: worker-1
â”‚  â””â”€ Pending Entries:
â”‚      â”œâ”€ 1704067200000-0 (åˆ†é…æ™‚é–“: T1)
â”‚      â”œâ”€ 1704067200000-1 (åˆ†é…æ™‚é–“: T1)
â”‚      â”œâ”€ ...
â”‚      â””â”€ 1704067200099-0 (åˆ†é…æ™‚é–“: T1)
â”‚      Total: 100 ç­†
  
[XACK å¾Œ - è¨Šæ¯ç¢ºèª]
  
Consumer Group: log_workers
â”œâ”€ Consumer: worker-1
â”‚  â””â”€ Pending Entries:
â”‚      Total: 0 ç­† âœ“ (æ‰€æœ‰è¨Šæ¯å·²ç¢ºèª)
```
  
#### 5.3 æœªç¢ºèªè¨Šæ¯çš„å½±éŸ¿
  
**å ´æ™¯ 1: æ­£å¸¸æµç¨‹**
```
XREADGROUP â†’ process_messages â†’ batch_insert_logs (æˆåŠŸ) â†’ XACK
Result: è¨Šæ¯å¾ PEL ç§»é™¤ï¼Œè™•ç†å®Œæˆ
```
  
**å ´æ™¯ 2: è³‡æ–™åº«å¯«å…¥å¤±æ•—**
```
XREADGROUP â†’ process_messages â†’ batch_insert_logs (å¤±æ•—) â†’ ä¸åŸ·è¡Œ XACK
Result: è¨Šæ¯ä¿ç•™åœ¨ PELï¼Œå¯é‡æ–°è™•ç†
```
  
**å ´æ™¯ 3: Worker å´©æ½°**
```
XREADGROUP â†’ process_messages â†’ batch_insert_logs (æˆåŠŸ) â†’ [Worker å´©æ½°]
Result: è¨Šæ¯ä¿ç•™åœ¨ PELï¼ˆæœª ACKï¼‰ï¼Œæ–° Worker å¯ä½¿ç”¨ XCLAIM é‡æ–°è™•ç†
```
  
#### 5.4 é‡æ–°è™•ç†æœªç¢ºèªè¨Šæ¯
  
```bash
# æŸ¥çœ‹ Pending Entries
redis-cli XPENDING logs:stream log_workers
  
# è¼¸å‡ºç¯„ä¾‹:
# 1) (integer) 100                    # å¾…è™•ç†è¨Šæ¯æ•¸é‡
# 2) "1704067200000-0"                # æœ€å° ID
# 3) "1704067200099-0"                # æœ€å¤§ ID
# 4) 1) 1) "worker-1"                 # æ¶ˆè²»è€…
#       2) "100"                       # è©²æ¶ˆè²»è€…çš„å¾…è™•ç†æ•¸é‡
  
# é‡æ–°åˆ†é…è¶…æ™‚è¨Šæ¯ï¼ˆè¶…é 60 ç§’æœª ACKï¼‰
redis-cli XCLAIM logs:stream log_workers worker-2 60000 1704067200000-0
  
# å°‡ worker-1 çš„è¶…æ™‚è¨Šæ¯åˆ†é…çµ¦ worker-2
```
  
### 6. å®Œæ•´è™•ç†é€±æœŸæ™‚åºåœ–
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Worker å®Œæ•´è™•ç†é€±æœŸï¼ˆ100 ç­†æ—¥èªŒï¼‰                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
æ™‚é–“è»¸ (ms)    â”‚ æ“ä½œ                          â”‚ ç‹€æ…‹
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T=0            â”‚ XREADGROUP (block=5000)      â”‚ é˜»å¡ç­‰å¾…...
               â”‚                              â”‚
T=2000         â”‚ (æ–°è¨Šæ¯åˆ°é” Redis)            â”‚
               â”‚                              â”‚
T=2001         â”‚ â† è¿”å› 100 ç­†è¨Šæ¯             â”‚ è®€å–å®Œæˆ
               â”‚                              â”‚ PEL +100
               â”‚                              â”‚
T=2005         â”‚ process_messages()           â”‚ è§£æä¸­...
               â”‚ â”œâ”€ è§£æ message_id           â”‚
               â”‚ â”œâ”€ è½‰æ›è³‡æ–™æ ¼å¼               â”‚
               â”‚ â””â”€ å»ºç«‹ logs_to_insert       â”‚
               â”‚                              â”‚
T=2015         â”‚ batch_insert_logs()          â”‚ è³‡æ–™åº«å¯«å…¥ä¸­...
               â”‚ â”œâ”€ SyncSessionLocal()        â”‚
               â”‚ â”œâ”€ session.execute(stmt)     â”‚
               â”‚ â””â”€ session.commit()          â”‚
               â”‚                              â”‚
T=2065         â”‚ â† å¯«å…¥æˆåŠŸ                    â”‚ PostgreSQL +100
               â”‚                              â”‚
T=2070         â”‚ XACK (100 ç­†)                â”‚ ç¢ºèªä¸­...
               â”‚ â”œâ”€ xack(msg_id_1)            â”‚
               â”‚ â”œâ”€ xack(msg_id_2)            â”‚
               â”‚ â””â”€ ...                       â”‚
               â”‚                              â”‚
T=2100         â”‚ â† ç¢ºèªå®Œæˆ                    â”‚ PEL -100
               â”‚                              â”‚
T=2100         â”‚ print("è™•ç†å®Œæˆ: 100 ç­†")     â”‚ æœ¬æ‰¹æ¬¡çµæŸ
               â”‚                              â”‚
T=2101         â”‚ XREADGROUP (block=5000)      â”‚ ä¸‹ä¸€æ‰¹æ¬¡...
               â”‚                              â”‚ é˜»å¡ç­‰å¾…...
```
  
**é—œéµæ•ˆèƒ½æŒ‡æ¨™**:
- **XREADGROUP**: ~1msï¼ˆè¨Šæ¯å·²å­˜åœ¨ï¼‰æˆ– <5000msï¼ˆé˜»å¡ç­‰å¾…ï¼‰
- **process_messages()**: ~10ms / 100 ç­†
- **batch_insert_logs()**: ~50ms / 100 ç­†
- **XACK**: ~30ms / 100 ç­†
- **ç¸½è™•ç†æ™‚é–“**: ~90-100ms / 100 ç­†
- **ç†è«–ååé‡**: ~1,000 ç­†/ç§’/Worker
  
### 7. éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶
  
#### 7.1 è³‡æ–™åº«å¯«å…¥å¤±æ•—è™•ç†
  
```python
# worker_loop() ä¸­çš„éŒ¯èª¤è™•ç†é‚è¼¯
success = batch_insert_logs(logs_to_insert)
  
if success:
    # æˆåŠŸï¼šACK è¨Šæ¯
    for message_id in message_ids:
        redis_client.xack(STREAM_NAME, GROUP_NAME, message_id)
    error_count = 0  # é‡ç½®éŒ¯èª¤è¨ˆæ•¸
else:
    # å¤±æ•—ï¼šä¸ ACKï¼Œå¢åŠ éŒ¯èª¤è¨ˆæ•¸
    error_count += 1
    print(f"âš ï¸ è™•ç†å¤±æ•—ï¼ŒéŒ¯èª¤æ¬¡æ•¸: {error_count}/{max_errors}")
  
    if error_count >= max_errors:
        print(f"âŒ éŒ¯èª¤æ¬¡æ•¸éå¤šï¼Œåœæ­¢ Worker")
        break
  
    # ç­‰å¾… 5 ç§’å¾Œé‡è©¦
    time.sleep(5)
```
  
#### 7.2 éŒ¯èª¤è™•ç†æ±ºç­–æ¨¹
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Worker éŒ¯èª¤è™•ç†æ±ºç­–æ¨¹                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
batch_insert_logs(logs_to_insert)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ˜¯å¦æˆåŠŸï¼Ÿ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€ æˆåŠŸ (True) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                                  â”‚
       â”‚                                  â–¼
       â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                         â”‚ XACK æ‰€æœ‰è¨Šæ¯    â”‚
       â”‚                         â”‚ error_count = 0 â”‚
       â”‚                         â”‚ ç¹¼çºŒä¸‹ä¸€æ‰¹æ¬¡     â”‚
       â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€ å¤±æ•— (False) â”€â”€â”€â”
                          â”‚
                          â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ error_count += 1â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ error_count     â”‚
                 â”‚ >= 10 ?         â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”œâ”€ æ˜¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                       â”‚
                          â”‚                       â–¼
                          â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚              â”‚ åœæ­¢ Worker      â”‚
                          â”‚              â”‚ (é¿å…ç„¡é™é‡è©¦)   â”‚
                          â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â””â”€ å¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                  â”‚
                                                  â–¼
                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â”‚ time.sleep(5)   â”‚
                                         â”‚ ç­‰å¾…å¾Œé‡è©¦       â”‚
                                         â”‚ (è¨Šæ¯ä¿ç•™åœ¨ PEL)â”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
### 8. å¤š Worker å”ä½œæ©Ÿåˆ¶
  
#### 8.1 æ¶ˆè²»è€…ç¾¤çµ„è‡ªå‹•åˆ†é…
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å¤š Worker è¨Šæ¯åˆ†é…æ©Ÿåˆ¶                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
Redis Stream: logs:stream
â”œâ”€ Entry ID: 1704067200000-0
â”œâ”€ Entry ID: 1704067200000-1
â”œâ”€ Entry ID: 1704067200000-2
â”œâ”€ Entry ID: 1704067200000-3
â”œâ”€ Entry ID: 1704067200000-4
â”œâ”€ Entry ID: 1704067200000-5
â””â”€ ... (æ›´å¤šè¨Šæ¯)
  
Consumer Group: log_workers
â”œâ”€ worker-1 (XREADGROUP count=100, block=5000)
â”‚  â”œâ”€ ç²å¾—: 1704067200000-0, 1704067200000-3, 1704067200000-6, ...
â”‚  â””â”€ è™•ç†: ~33 ç­†
â”‚
â”œâ”€ worker-2 (XREADGROUP count=100, block=5000)
â”‚  â”œâ”€ ç²å¾—: 1704067200000-1, 1704067200000-4, 1704067200000-7, ...
â”‚  â””â”€ è™•ç†: ~33 ç­†
â”‚
â””â”€ worker-3 (XREADGROUP count=100, block=5000)
   â”œâ”€ ç²å¾—: 1704067200000-2, 1704067200000-5, 1704067200000-8, ...
   â””â”€ è™•ç†: ~34 ç­†
```
  
**è‡ªå‹•åˆ†é…è¦å‰‡**:
1. Redis è‡ªå‹•å°‡æ–°è¨Šæ¯åˆ†é…çµ¦ç©ºé–’çš„ Consumer
2. åˆ†é…ç­–ç•¥ï¼šRound-Robinï¼ˆè¼ªè©¢ï¼‰
3. æ¯å€‹è¨Šæ¯åªæœƒè¢«åˆ†é…çµ¦ä¸€å€‹ Consumer
4. Consumer æ•…éšœå¾Œï¼Œè¨Šæ¯ä¿ç•™åœ¨ PELï¼Œå¯é‡æ–°åˆ†é…
  
#### 8.2 æ“´å±• Worker æ•¸é‡
  
**é…ç½® 3 å€‹ Worker (`docker-compose.yml`)**:
```yaml
worker-1:
  command: python worker.py
  environment:
    - WORKER_NAME=worker-1
  
worker-2:
  command: python worker.py
  environment:
    - WORKER_NAME=worker-2
  
worker-3:
  command: python worker.py
  environment:
    - WORKER_NAME=worker-3
```
  
**æ•ˆèƒ½æå‡**:
- å–® Worker: ~1,000 logs/s
- 3 Workers: ~3,000 logs/s
- N Workers: ~N Ã— 1,000 logs/sï¼ˆç·šæ€§æ“´å±•ï¼‰
  
### 9. è³‡æ–™ä¸€è‡´æ€§ä¿è­‰
  
#### 9.1 è‡³å°‘ä¸€æ¬¡äº¤ä»˜ (At-Least-Once Delivery)
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 è‡³å°‘ä¸€æ¬¡äº¤ä»˜ä¿è­‰æ©Ÿåˆ¶                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
FastAPI å¯«å…¥ Redis
       â”‚
       â”‚ XADD logs:stream
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis Stream    â”‚ âœ“ è¨Šæ¯å·²æŒä¹…åŒ–
â”‚ (AOF enabled)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ XREADGROUP
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker è®€å–     â”‚ âœ“ è¨Šæ¯åœ¨ PEL ä¸­
â”‚ (æœª ACK)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ batch_insert_logs()
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL      â”‚ âœ“ è³‡æ–™å·²å¯«å…¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ XACK
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Redis PEL       â”‚ âœ“ è¨Šæ¯ç§»é™¤
â”‚ (å·²ç¢ºèª)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
**ä¿è­‰æ©Ÿåˆ¶**:
1. Redis AOF ç¢ºä¿è¨Šæ¯ä¸æœƒéºå¤±
2. Worker å´©æ½°æ™‚ï¼Œè¨Šæ¯ä¿ç•™åœ¨ PEL
3. æ–° Worker å¯é‡æ–°è™•ç†æœªç¢ºèªè¨Šæ¯
4. PostgreSQL äº¤æ˜“ç¢ºä¿è³‡æ–™åŸå­æ€§
  
#### 9.2 æ½›åœ¨é‡è¤‡è™•ç†
  
**é‡è¤‡è™•ç†å ´æ™¯**:
```
Worker åŸ·è¡Œé †åº:
1. XREADGROUP        â† è¨Šæ¯åˆ†é…åˆ° Worker
2. batch_insert_logs â† PostgreSQL INSERT æˆåŠŸ
3. [Worker å´©æ½°]     â† åœ¨ XACK å‰å´©æ½°
4. è¨Šæ¯ä»åœ¨ PEL      â† æœªè¢«ç¢ºèª
5. æ–° Worker é‡æ–°è™•ç† â† é‡è¤‡ INSERT
```
  
**è§£æ±ºæ–¹æ¡ˆé¸é …**:
  
**é¸é … 1: ä½¿ç”¨å”¯ä¸€ç´„æŸ**
```sql
-- ç‚º logs è¡¨æ·»åŠ å”¯ä¸€ç´„æŸï¼ˆéœ€è¦é¡å¤–æ¬„ä½ï¼‰
ALTER TABLE logs ADD COLUMN message_id VARCHAR(50) UNIQUE;
```
  
**é¸é … 2: ä½¿ç”¨ UPSERT**
```sql
-- PostgreSQL UPSERTï¼ˆéœ€è¦å”¯ä¸€éµï¼‰
INSERT INTO logs (...) VALUES (...)
ON CONFLICT (message_id) DO NOTHING;
```
  
**é¸é … 3: æ¥å—æ½›åœ¨é‡è¤‡**
- æ—¥èªŒç³»çµ±é€šå¸¸å¯å®¹å¿å°‘é‡é‡è¤‡
- Worker å´©æ½°æ©Ÿç‡ä½
- é‡è¤‡å½±éŸ¿å¯å¿½ç•¥ä¸è¨ˆ
  
### 10. æ•ˆèƒ½ç›£æ§èˆ‡å„ªåŒ–
  
#### 10.1 é—œéµæ•ˆèƒ½æŒ‡æ¨™
  
```python
# åœ¨ worker_loop() ä¸­æ·»åŠ æ•ˆèƒ½ç›£æ§
import time
  
start_time = time.time()
  
# è®€å–è¨Šæ¯
messages = redis_client.xreadgroup(...)
read_time = time.time() - start_time
  
# è§£æè¨Šæ¯
logs_to_insert, message_ids = process_messages(messages)
parse_time = time.time() - start_time - read_time
  
# æ‰¹æ¬¡å¯«å…¥
success = batch_insert_logs(logs_to_insert)
write_time = time.time() - start_time - read_time - parse_time
  
# ACK ç¢ºèª
for message_id in message_ids:
    redis_client.xack(...)
ack_time = time.time() - start_time - read_time - parse_time - write_time
  
total_time = time.time() - start_time
  
print(f"ğŸ“Š æ•ˆèƒ½çµ±è¨ˆ:")
print(f"  - è®€å–æ™‚é–“: {read_time*1000:.2f}ms")
print(f"  - è§£ææ™‚é–“: {parse_time*1000:.2f}ms")
print(f"  - å¯«å…¥æ™‚é–“: {write_time*1000:.2f}ms")
print(f"  - ç¢ºèªæ™‚é–“: {ack_time*1000:.2f}ms")
print(f"  - ç¸½è™•ç†æ™‚é–“: {total_time*1000:.2f}ms")
print(f"  - ååé‡: {len(logs_to_insert)/total_time:.2f} logs/s")
```
  
#### 10.2 æ•ˆèƒ½å„ªåŒ–å»ºè­°
  
**å„ªåŒ– PostgreSQL é€£ç·šæ± **:
```python
# database.py
sync_engine = create_engine(
    DATABASE_URL,
    pool_size=20,        # å¢åŠ é€£ç·šæ± å¤§å° (é è¨­ 10)
    max_overflow=10,     # å¢åŠ é¡å¤–é€£ç·šæ•¸ (é è¨­ 5)
    pool_pre_ping=True,  # ä¿æŒé–‹å•Ÿ
)
```
  
**å„ªåŒ–æ‰¹æ¬¡å¤§å°**:
```python
# worker.py
BATCH_SIZE = 200  # å¢åŠ æ‰¹æ¬¡å¤§å°ï¼ˆé è¨­ 100ï¼‰
```
  
**å„ªåŒ–é˜»å¡æ™‚é–“**:
```python
# worker.py
BLOCK_MS = 2000  # æ¸›å°‘é˜»å¡æ™‚é–“ï¼ˆé è¨­ 5000ï¼‰ï¼Œæå‡éŸ¿æ‡‰é€Ÿåº¦
```
  
**æ•ˆèƒ½æ¬Šè¡¡**:
| åƒæ•¸ | å¢åŠ  | æ¸›å°‘ |
|------|------|------|
| `BATCH_SIZE` | ååé‡â†‘ å»¶é²â†‘ | ååé‡â†“ å»¶é²â†“ |
| `BLOCK_MS` | å»¶é²â†‘ CPUâ†“ | å»¶é²â†“ CPUâ†‘ |
| `pool_size` | ä¸¦ç™¼â†‘ è¨˜æ†¶é«”â†‘ | ä¸¦ç™¼â†“ è¨˜æ†¶é«”â†“ |
  
## Worker å®Œæ•´ç”Ÿå‘½é€±æœŸ
  
### 1. Worker å•Ÿå‹•æµç¨‹
  
```
main()
  â”‚
  â”œâ”€ 1. é¡¯ç¤ºå•Ÿå‹•æ¨™é¡Œ
  â”‚     "ğŸ“¦ æ—¥èªŒæ”¶é›†ç³»çµ± - èƒŒæ™¯ Worker"
  â”‚
  â”œâ”€ 2. init_redis()
  â”‚     â”œâ”€ å»ºç«‹ Redis é€£ç·š (max_connections=10)
  â”‚     â”œâ”€ æ¸¬è©¦é€£ç·š (ping)
  â”‚     â””â”€ å‰µå»º/ç¢ºèªæ¶ˆè²»è€…ç¾¤çµ„ (xgroup_create)
  â”‚
  â”œâ”€ 3. æ¸¬è©¦ PostgreSQL é€£ç·š
  â”‚     sync_engine.connect() â†’ SELECT 1
  â”‚
  â””â”€ 4. worker_loop()
        â””â”€ é€²å…¥ä¸»å·¥ä½œå¾ªç’°
```
  
### 2. Worker ä¸»å·¥ä½œå¾ªç’°
  
```
worker_loop() [while running]
  â”‚
  â”œâ”€ 1. xreadgroup() - å¾ Redis Stream è®€å–
  â”‚     â”œâ”€ groupname: 'log_workers'
  â”‚     â”œâ”€ consumername: WORKER_NAME
  â”‚     â”œâ”€ streams: {'logs:stream': '>'}
  â”‚     â”œâ”€ count: 100 (æ‰¹æ¬¡å¤§å°)
  â”‚     â””â”€ block: 5000ms (é˜»å¡ç­‰å¾…)
  â”‚
  â”œâ”€ 2. process_messages() - è§£æè¨Šæ¯
  â”‚     â”œâ”€ éæ­·æ¯å€‹ (message_id, message_data)
  â”‚     â”œâ”€ è½‰æ›ç‚º PostgreSQL æ ¼å¼
  â”‚     â””â”€ è¿”å› (logs_to_insert, message_ids)
  â”‚
  â”œâ”€ 3. batch_insert_logs() - æ‰¹æ¬¡å¯«å…¥è³‡æ–™åº«
  â”‚     â”œâ”€ ä½¿ç”¨åŸç”Ÿ SQL (text())
  â”‚     â”œâ”€ CAST(:log_data AS jsonb)
  â”‚     â””â”€ å–®ä¸€äº¤æ˜“æäº¤
  â”‚
  â””â”€ 4. xack() - ç¢ºèªè¨Šæ¯å·²è™•ç†
        â””â”€ é€ä¸€ç¢ºèªæ¯å€‹ message_id
```
  
### 3. Redis Stream èˆ‡ Worker å”ä½œæ©Ÿåˆ¶è©³è§£
  
#### è³‡æ–™æµå‘å®Œæ•´è·¯å¾‘
  
```
[Client Request]
       â”‚
       â–¼
[Nginx - è² è¼‰å‡è¡¡]
       â”‚
       â–¼
[FastAPI Instance]
       â”‚
       â”‚ redis_client.xadd('logs:stream', fields)
       â–¼
[Redis Stream: logs:stream]
       â”‚
       â”‚ redis_client.xreadgroup('log_workers', WORKER_NAME)
       â–¼
[Worker - process_messages()]
       â”‚
       â”‚ batch_insert_logs() - åŸç”Ÿ SQL INSERT
       â–¼
[PostgreSQL - logs table]
       â”‚
       â”‚ redis_client.xack() - ç¢ºèªè™•ç†å®Œæˆ
       â–¼
[Redis Stream - ç§»é™¤ Pending Entry]
```
  
#### é—œéµ Redis å‘½ä»¤å°ç…§
  
| çµ„ä»¶ | Redis å‘½ä»¤ | ç”¨é€” | æª”æ¡ˆä½ç½® |
|------|-----------|------|----------|
| **FastAPI** | `XGROUP CREATE` | å‰µå»ºæ¶ˆè²»è€…ç¾¤çµ„ | `app/main.py:77-82` |
| **FastAPI** | `XADD logs:stream` | å¯«å…¥æ—¥èªŒè¨Šæ¯ | `app/main.py:170-175` |
| **FastAPI** | `Pipeline.XADD` | æ‰¹é‡å¯«å…¥è¨Šæ¯ | `app/main.py:217-222` |
| **Worker** | `XGROUP CREATE` | ç¢ºä¿ç¾¤çµ„å­˜åœ¨ | `app/worker.py:69-74` |
| **Worker** | `XREADGROUP` | æ¶ˆè²»è¨Šæ¯ | `app/worker.py:195-201` |
| **Worker** | `XACK` | ç¢ºèªè¨Šæ¯è™•ç† | `app/worker.py:221` |
  
#### Redis Stream é…ç½®ä¸€è‡´æ€§
  
```python
# FastAPI (app/main.py)
STREAM_NAME = 'logs:stream'
GROUP_NAME = 'log_workers'
MAX_LEN = 100000
  
# Worker (app/worker.py)
STREAM_NAME = 'logs:stream'
GROUP_NAME = 'log_workers'
BATCH_SIZE = 100
BLOCK_MS = 5000
```
  
### 4. Worker éŒ¯èª¤è™•ç†èˆ‡å®¹éŒ¯æ©Ÿåˆ¶
  
#### éŒ¯èª¤è™•ç†åˆ†å±¤
  
```
worker_loop()
  â”‚
  â”œâ”€ Level 1: è¨Šæ¯è§£æéŒ¯èª¤
  â”‚   â””â”€ process_messages() å…§éƒ¨è™•ç†
  â”‚       â”œâ”€ è¨˜éŒ„éŒ¯èª¤è¨Šæ¯
  â”‚       â””â”€ ä»æ·»åŠ åˆ° message_ids (é¿å…é‡è¤‡è™•ç†)
  â”‚
  â”œâ”€ Level 2: è³‡æ–™åº«å¯«å…¥å¤±æ•—
  â”‚   â””â”€ batch_insert_logs() è¿”å› False
  â”‚       â”œâ”€ error_count += 1
  â”‚       â”œâ”€ ç­‰å¾… 5 ç§’å¾Œé‡è©¦
  â”‚       â””â”€ ä¸ ACK è¨Šæ¯ (ä¿ç•™åœ¨ Pending)
  â”‚
  â”œâ”€ Level 3: Redis é€£ç·šéŒ¯èª¤
  â”‚   â””â”€ redis.exceptions.ConnectionError
  â”‚       â”œâ”€ error_count += 1
  â”‚       â”œâ”€ ç­‰å¾… 5 ç§’
  â”‚       â””â”€ é‡æ–°åŸ·è¡Œ init_redis()
  â”‚
  â””â”€ Level 4: æœªé æœŸéŒ¯èª¤
      â””â”€ Exception
          â”œâ”€ error_count += 1
          â””â”€ ç­‰å¾… 1 ç§’å¾Œé‡è©¦
```
  
#### å®¹éŒ¯é–¾å€¼
  
```python
max_errors = 10  # æœ€å¤§é€£çºŒéŒ¯èª¤æ¬¡æ•¸
  
if error_count >= max_errors:
    # åœæ­¢ Workerï¼Œé¿å…ç„¡é™é‡è©¦
    break
```
  
### 5. Worker èˆ‡ Docker Compose æ•´åˆ
  
#### æœå‹™ä¾è³´é—œä¿‚
  
```yaml
worker:
  depends_on:
    - postgres   # ç¢ºä¿è³‡æ–™åº«å…ˆå•Ÿå‹•
    - redis      # ç¢ºä¿å¿«å–å±¤å…ˆå•Ÿå‹•
  restart: unless-stopped  # è‡ªå‹•é‡å•Ÿç­–ç•¥
```
  
#### ç’°å¢ƒè®Šæ•¸é…ç½®
  
| è®Šæ•¸ | ç”¨é€” | å°æ‡‰ç¨‹å¼ç¢¼ |
|------|------|------------|
| `POSTGRES_HOST` | è³‡æ–™åº«ä¸»æ©Ÿ | `database.py` |
| `POSTGRES_PORT` | è³‡æ–™åº«ç«¯å£ | `database.py` |
| `POSTGRES_USER` | è³‡æ–™åº«ç”¨æˆ¶ | `database.py` |
| `POSTGRES_PASSWORD` | è³‡æ–™åº«å¯†ç¢¼ | `database.py` |
| `POSTGRES_DB` | è³‡æ–™åº«åç¨± | `database.py` |
| `REDIS_HOST` | Redis ä¸»æ©Ÿ | `worker.py:17` |
| `REDIS_PORT` | Redis ç«¯å£ | `worker.py:18` |
| `WORKER_NAME` | Worker è­˜åˆ¥ç¢¼ | `worker.py:19` |
  
### 6. æ“´å±•å¤š Worker å¯¦ä¾‹
  
#### é…ç½®æ–¹å¼
  
```yaml
# docker-compose.yml
worker-1:
  command: python worker.py
  environment:
    - WORKER_NAME=worker-1
  
worker-2:
  command: python worker.py
  environment:
    - WORKER_NAME=worker-2
  
worker-3:
  command: python worker.py
  environment:
    - WORKER_NAME=worker-3
```
  
#### æ¶ˆè²»è€…ç¾¤çµ„è‡ªå‹•åˆ†é…
  
```
Redis Stream: logs:stream
Consumer Group: log_workers
  â”‚
  â”œâ”€ worker-1: æ¶ˆè²» message-0, message-3, message-6...
  â”œâ”€ worker-2: æ¶ˆè²» message-1, message-4, message-7...
  â””â”€ worker-3: æ¶ˆè²» message-2, message-5, message-8...
```
  
**å„ªå‹¢**:
- è‡ªå‹•è² è¼‰å‡è¡¡ï¼Œç„¡éœ€æ‰‹å‹•é…ç½®
- ç·šæ€§æ“´å±•ååé‡ (N Workers = N Ã— 2000 logs/s)
- ç¨ç«‹å®¹éŒ¯ï¼Œä¸€å€‹ Worker æ•…éšœä¸å½±éŸ¿å…¶ä»–
  
## FastAPI èˆ‡ Worker å”ä½œæ©Ÿåˆ¶
  
### 1. æ•´é«”è³‡æ–™æµå‘
  
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å®Œæ•´è³‡æ–™æµç¨‹åœ–                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  
Client HTTP Request
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Nginx       â”‚ â† è² è¼‰å‡è¡¡ & é™æµ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    FastAPI      â”‚ â† è«‹æ±‚é©—è­‰ & è™•ç†
â”‚   (Instance)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼ XADD (å¯«å…¥)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis Stream   â”‚ â† logs:stream (ç·©è¡å€)
â”‚  (logs:stream)  â”‚   maxlen=100,000
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ XREADGROUP (æ¶ˆè²»)
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Worker      â”‚ â† æ‰¹æ¬¡è™•ç† & æŒä¹…åŒ–
â”‚  (Consumer)     â”‚   BATCH_SIZE=100
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼ INSERT (æ‰¹æ¬¡å¯«å…¥)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚ â† æ°¸ä¹…å­˜å„²
â”‚   (Database)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
  
### 2. FastAPI ç«¯çš„æ“ä½œ
  
**å¯«å…¥ Redis Stream (`app/main.py`)**:
  
```python
# å–®ä¸€æ—¥èªŒå¯«å…¥
message_id = await redis_client.xadd(
    name="logs:stream",
    fields={
        "device_id": "device-001",
        "log_level": "INFO",
        "message": "Application started",
        "log_data": '{"version": "1.0.0"}',
        "timestamp": "2024-01-01T12:00:00"
    },
    maxlen=100000,
    approximate=True
)
# è¿”å›: "1704067200000-0" (æ™‚é–“æˆ³-åºåˆ—è™Ÿ)
  
# æ‰¹é‡å¯«å…¥ï¼ˆä½¿ç”¨ Pipelineï¼‰
pipe = redis_client.pipeline()
for log in logs:
    pipe.xadd("logs:stream", fields=log_dict, maxlen=100000)
results = await pipe.execute()
# è¿”å›: ["1704067200000-0", "1704067200000-1", ...]
```
  
**FastAPI å‰µå»ºæ¶ˆè²»è€…ç¾¤çµ„ï¼ˆå•Ÿå‹•æ™‚ï¼‰**:
  
```python
@app.on_event("startup")
async def startup_event():
    # ç¢ºä¿æ¶ˆè²»è€…ç¾¤çµ„å­˜åœ¨
    try:
        await redis_client.xgroup_create(
            name='logs:stream',
            groupname='log_workers',
            id='0',
            mkstream=True
        )
    except redis.ResponseError as e:
        if "BUSYGROUP" in str(e):
            pass  # ç¾¤çµ„å·²å­˜åœ¨ï¼Œç„¡éœ€æ“ä½œ
```
  
### 3. Worker ç«¯çš„æ“ä½œ
  
**å¾ Redis Stream æ¶ˆè²» (`app/worker.py`)**:
  
```python
# æ¶ˆè²»è¨Šæ¯
messages = redis_client.xreadgroup(
    groupname='log_workers',
    consumername='worker-1',
    streams={'logs:stream': '>'},
    count=100,
    block=5000
)
  
# è¨Šæ¯æ ¼å¼:
# [
#     ('logs:stream', [
#         ('1704067200000-0', {
#             'device_id': 'device-001',
#             'log_level': 'INFO',
#             'message': 'Application started',
#             'log_data': '{"version": "1.0.0"}',
#             'timestamp': '2024-01-01T12:00:00'
#         }),
#         ('1704067200000-1', {...}),
#         ...
#     ])
# ]
  
# è™•ç†å¾Œç¢ºèª
for message_id in message_ids:
    redis_client.xack('logs:stream', 'log_workers', message_id)
```
  
### 4. Redis Stream ç‹€æ…‹ç›£æ§
  
```bash
# æŸ¥çœ‹ Stream è³‡è¨Š
redis-cli XINFO STREAM logs:stream
# length: 50000
# radix-tree-keys: 100
# radix-tree-nodes: 200
# last-generated-id: 1704067200000-0
# groups: 1
  
# æŸ¥çœ‹æ¶ˆè²»è€…ç¾¤çµ„
redis-cli XINFO GROUPS logs:stream
# name: log_workers
# consumers: 1
# pending: 0
# last-delivered-id: 1704067200000-0
  
# æŸ¥çœ‹å¾…è™•ç†è¨Šæ¯
redis-cli XPENDING logs:stream log_workers
# pending: 0 (æ‰€æœ‰è¨Šæ¯å·²ç¢ºèª)
```
  
### 5. æ•ˆèƒ½æŒ‡æ¨™èˆ‡ååé‡
  
**FastAPI ç«¯**:
- **éŸ¿æ‡‰æ™‚é–“**: < 5msï¼ˆå¯«å…¥ Redis Streamï¼‰
- **ååé‡**: 10,000+ logs/ç§’ï¼ˆæ‰¹é‡ APIï¼‰
- **ä½µç™¼èƒ½åŠ›**: 200 Redis é€£ç·š Ã— 6 Workers = 1,200 ä½µç™¼è«‹æ±‚
  
**Worker ç«¯**:
- **æ‰¹æ¬¡è™•ç†**: 100 logs/æ‰¹æ¬¡
- **è™•ç†é€±æœŸ**: 5 ç§’é˜»å¡ç­‰å¾…
- **å¯«å…¥å»¶é²**: ~50-100ms/æ‰¹æ¬¡ï¼ˆåŒ…å« PostgreSQL å¯«å…¥ï¼‰
- **ç†è«–ååé‡**: 2,000 logs/ç§’/Worker
  
**ç³»çµ±æ•´é«”**:
| æŒ‡æ¨™ | å€¼ | èªªæ˜ |
|------|-----|------|
| å¯«å…¥ååé‡ | 10,000+ logs/s | FastAPI åˆ° Redis |
| æŒä¹…åŒ–ååé‡ | 2,000 logs/s | Worker åˆ° PostgreSQL |
| ç·©è¡å®¹é‡ | 100,000 logs | Redis Stream maxlen |
| å¿«å– TTL | 300s / 60s | æŸ¥è©¢/çµ±è¨ˆå¿«å– |
  
### 6. æ•…éšœæ¢å¾©æ©Ÿåˆ¶
  
**FastAPI æ•…éšœ**:
- Nginx è‡ªå‹•å°‡æµé‡è½‰ç§»åˆ°å…¶ä»–å¯¦ä¾‹
- `max_fails=3 fail_timeout=30s` é…ç½®
- 30 ç§’å¾Œè‡ªå‹•é‡è©¦æ•…éšœå¯¦ä¾‹
  
**Worker æ•…éšœ**:
- æœªç¢ºèªè¨Šæ¯ä¿ç•™åœ¨ Pending Entries List
- æ–° Worker å•Ÿå‹•å¾Œå¯é‡æ–°è™•ç†
- ä½¿ç”¨ XCLAIM é‡æ–°åˆ†é…è¶…æ™‚è¨Šæ¯
  
**Redis æ•…éšœ**:
- Worker è‡ªå‹•é‡è©¦é€£ç·šï¼ˆæœ€å¤š 10 æ¬¡ï¼‰
- 5 ç§’é‡è©¦é–“éš”
- è¶…éé‡è©¦æ¬¡æ•¸å‰‡åœæ­¢ Worker
  
**PostgreSQL æ•…éšœ**:
- Worker è‡ªå‹•é‡è©¦å¯«å…¥
- è¨Šæ¯ä¿ç•™åœ¨ Redis Stream ä¸­
- è³‡æ–™åº«æ¢å¾©å¾Œè‡ªå‹•ç¹¼çºŒè™•ç†
  
### 7. å¤š Worker æ“´å±•
  
**æ°´å¹³æ“´å±•é…ç½®** (`docker-compose.yml`):
  
```yaml
worker-1:
  command: python worker.py
  environment:
    - WORKER_NAME=worker-1
  
worker-2:
  command: python worker.py
  environment:
    - WORKER_NAME=worker-2
  
worker-3:
  command: python worker.py
  environment:
    - WORKER_NAME=worker-3
```
  
**æ¶ˆè²»è€…ç¾¤çµ„è‡ªå‹•è² è¼‰å‡è¡¡**:
```
logs:stream (100 ç­†è¨Šæ¯)
    â”‚
    â”œâ”€ worker-1: è®€å–è¨Šæ¯ 0-33
    â”œâ”€ worker-2: è®€å–è¨Šæ¯ 34-66
    â””â”€ worker-3: è®€å–è¨Šæ¯ 67-99
```
  
**å„ªå‹¢**:
- è‡ªå‹•åˆ†é…ï¼Œç„¡éœ€é¡å¤–é…ç½®
- ç·šæ€§æ“´å±•è™•ç†èƒ½åŠ›
- å®¹éŒ¯ï¼šä¸€å€‹ Worker æ•…éšœä¸å½±éŸ¿å…¶ä»–
  
### 8. è³‡æ–™ä¸€è‡´æ€§ä¿è­‰
  
**å¯«å…¥é †åºä¿è­‰**:
- Redis Stream ä¿è­‰è¨Šæ¯é †åº
- æ¯å€‹è¨Šæ¯æœ‰å”¯ä¸€ IDï¼ˆæ™‚é–“æˆ³-åºåˆ—è™Ÿï¼‰
- Worker æŒ‰é †åºè™•ç†åŒä¸€ Stream çš„è¨Šæ¯
  
**è‡³å°‘ä¸€æ¬¡äº¤ä»˜**:
- è¨Šæ¯å¯«å…¥ Redis å¾Œå³ç¢ºèªæ¥æ”¶
- Worker è™•ç†å®Œæˆå¾Œæ‰ ACK
- æœª ACK çš„è¨Šæ¯å¯é‡æ–°è™•ç†
  
**æ½›åœ¨é‡è¤‡è™•ç†**:
- Worker å´©æ½°åœ¨ INSERT å¾Œã€ACK å‰
- è¨Šæ¯å¯èƒ½è¢«é‡è¤‡æ’å…¥
- è§£æ±ºæ–¹æ¡ˆï¼šä½¿ç”¨å”¯ä¸€ç´„æŸæˆ– UPSERT
  
## è«‹æ±‚è™•ç†æµç¨‹
  
ä»¥ä¸‹æ˜¯ç•¶å‰ç«¯é»è«‹æ±‚å¦‚ä½•è¢« Nginx åˆ†ç™¼åˆ° FastAPI çš„è©³ç´°æµç¨‹ï¼š
  
1. **è«‹æ±‚æ¥æ”¶**: Nginx æ¥æ”¶ä¾†è‡ªå®¢æˆ¶ç«¯çš„ HTTP è«‹æ±‚
2. **è·¯ç”±åŒ¹é…**: æ ¹æ“š location æŒ‡ä»¤åŒ¹é…è«‹æ±‚è·¯å¾‘
3. **é™æµæª¢æŸ¥**: æ‡‰ç”¨é™æµè¦å‰‡æª¢æŸ¥æ˜¯å¦è¶…éé€Ÿç‡é™åˆ¶
4. **è² è¼‰å‡è¡¡**: æ ¹æ“š `least_conn` ç®—æ³•é¸æ“‡å¾Œç«¯æœå‹™å™¨
5. **è«‹æ±‚è½‰ç™¼**: é€šé `proxy_pass` æŒ‡ä»¤å°‡è«‹æ±‚è½‰ç™¼åˆ°é¸å®šçš„ FastAPI å¯¦ä¾‹
6. **éŸ¿æ‡‰è¿”å›**: FastAPI è™•ç†è«‹æ±‚å¾Œè¿”å›éŸ¿æ‡‰ï¼Œé€šé Nginx è¿”å›çµ¦å®¢æˆ¶ç«¯
  
### æ—¥èªŒå¯«å…¥æµç¨‹
  
å°æ–¼æ—¥èªŒå¯«å…¥è«‹æ±‚ (/api/log)ï¼Œè™•ç†æµç¨‹å¦‚ä¸‹ï¼š
  
```
Client Request -> Nginx -> FastAPI -> Redis (Queue) -> Worker -> PostgreSQL
     â”‚             â”‚        â”‚
     â”‚             â”‚        â””â”€â”€ éåŒæ­¥è™•ç†ï¼Œç«‹å³è¿”å›éŸ¿æ‡‰
     â”‚             â””â”€â”€ è² è¼‰å‡è¡¡å’Œé™æµ
     â””â”€â”€ HTTP Request
```
  
### Redis åˆ° PostgreSQL çš„è™•ç†æµç¨‹
  
Worker æœå‹™æŒçºŒå¾ Redis Stream æ¶ˆè²»æ—¥èªŒè³‡æ–™ï¼š
  
1. **è¨Šæ¯æ¶ˆè²»**: ä½¿ç”¨ `xreadgroup` å¾ Redis Stream è®€å–æ‰¹æ¬¡è¨Šæ¯
2. **è³‡æ–™è½‰æ›**: å°‡ Redis ä¸­çš„è¨Šæ¯æ ¼å¼è½‰æ›ç‚º PostgreSQL ç›¸å®¹çš„æ ¼å¼
3. **æ‰¹æ¬¡å¯«å…¥**: ä½¿ç”¨æ‰¹æ¬¡ SQL å‘½ä»¤å°‡å¤šç­†è³‡æ–™åŒæ™‚å¯«å…¥ PostgreSQL
4. **ç¢ºèªè™•ç†**: å‘ Redis ç™¼é€ ACK ç¢ºèªè¨Šæ¯å·²è™•ç†
5. **éŒ¯èª¤é‡è©¦**: å¦‚æœå¯«å…¥å¤±æ•—ï¼Œé€²è¡ŒéŒ¯èª¤è™•ç†å’Œé‡è©¦
  
## æ‰¹é‡è™•ç†èˆ‡æ•ˆèƒ½å„ªåŒ–
  
### æ‰¹é‡æ—¥èªŒç«¯é»
  
ç³»çµ±æä¾›å°ˆé–€çš„æ‰¹é‡è™•ç†ç«¯é» `/api/logs/batch`ï¼Œæ”¯æ´ä¸€æ¬¡æ¥æ”¶å¤šå€‹æ—¥èªŒæ¢ç›®ï¼š
  
- ä½¿ç”¨ Redis Pipeline æ¸›å°‘ç¶²è·¯å¾€è¿”æ™‚é–“
- æ‰¹é‡å¯«å…¥ Redis Stream æå‡æ•ˆèƒ½
- åœ¨å£“åŠ›æ¸¬è©¦ä¸­è¡¨ç¾å„ªç•°ï¼Œæ”¯æ´é«˜ååé‡
  
### æ•ˆèƒ½å„ªåŒ–æªæ–½
  
1. **Redis é…ç½®å„ªåŒ–**
   - `maxmemory 512mb` å’Œ `maxmemory-policy allkeys-lru`: è¨˜æ†¶é«”é™åˆ¶å’Œæ·˜æ±°ç­–ç•¥
   - `client_max_body_size 50M`: æ”¯æ´è¼ƒå¤§çš„æ‰¹é‡è«‹æ±‚
  
2. **è³‡æ–™åº«é€£ç·šå„ªåŒ–**
   - FastAPI ä½¿ç”¨éåŒæ­¥é€£ç·šæ± é…ç½®
   - Worker ä½¿ç”¨åŒæ­¥é€£ç·šæ± é…ç½®
   - åˆé©çš„ `pool_size` å’Œ `max_overflow` è¨­ç½®
  
3. **Redis Stream é…ç½®**
   - `maxlen=100000` åœ¨ Redis ä¸­ä¿ç•™æœ€è¿‘ 10 è¬ç­†æ—¥èªŒ
   - `approximate=True` æå‡æ•ˆèƒ½
  
4. **å¿«å–ç­–ç•¥**
   - æ—¥èªŒæŸ¥è©¢çµæœåœ¨ Redis ä¸­å¿«å– 5 åˆ†é˜
   - çµ±è¨ˆè³‡æ–™å¿«å– 60 ç§’
   - æ¸›å°‘è³‡æ–™åº«æŸ¥è©¢å£“åŠ›
  
### å£“åŠ›æ¸¬è©¦é…ç½®
  
æ ¹æ“š `tests/stress_test.py` çš„é…ç½®ï¼š
- 100 å°è¨­å‚™ï¼Œæ¯å°ç™¼é€ 100 æ¢æ—¥èªŒ
- 200 ä¸¦ç™¼é™åˆ¶ï¼Œ5 æ‰¹æ¬¡å¤§å°
- ç›®æ¨™ï¼š10,000 logs/ç§’ï¼ŒP95 éŸ¿æ‡‰æ™‚é–“ <100ms
  
## æ€§èƒ½å„ªåŒ–é…ç½®
  
### Nginx å„ªåŒ–
  
1. **é€£ç·šå„ªåŒ–**
   - `worker_connections 4096`: å¢åŠ å·¥ä½œé€²ç¨‹é€£ç·šæ•¸
   - `keepalive 128`: å¢åŠ å¾Œç«¯é€£ç·šæ± å¤§å°
  
2. **è¶…æ™‚è¨­ç½®**
   - é‡å°ä¸åŒç«¯é»è¨­ç½®åˆç†çš„è¶…æ™‚æ™‚é–“
   - é¿å…ä¸å¿…è¦çš„é€£ç·šä½”ç”¨
  
3. **é™æµèª¿å„ª**
   - å¯«å…¥ç«¯é»ï¼šé«˜é™æµå€¼ä»¥æ”¯æ´å¤§é‡æ—¥èªŒè¼¸å…¥
   - æŸ¥è©¢ç«¯é»ï¼šè¼ƒä¿å®ˆçš„é™æµå€¼ä»¥ä¿è­·å¾Œç«¯
  
### FastAPI å„ªåŒ–
  
1. **éåŒæ­¥è™•ç†**
   - ä½¿ç”¨ async/await è™•ç† I/O å¯†é›†æ“ä½œ
   - é€šé uvicorn å¤š worker æå‡ä½µç™¼èƒ½åŠ›
   - Redis é€£ç·šæ± é…ç½®ä»¥æ”¯æ´é«˜ä½µç™¼
  
2. **å¿«å–æ©Ÿåˆ¶**
   - Redis ç”¨ä½œæ—¥èªŒéšŠåˆ—ï¼Œå¯¦ç¾éåŒæ­¥è™•ç†
   - æŸ¥è©¢çµæœå¿«å–ï¼Œæ¸›å°‘è³‡æ–™åº«å£“åŠ›
   - ä½¿ç”¨ Redis Stream æ¶ˆè²»è€…ç¾¤çµ„æ¨¡å¼
  
3. **è³‡æ–™åº«å„ªåŒ–**
   - ä½¿ç”¨ SQLAlchemy éåŒæ­¥æœƒè©±
   - åˆç†çš„ç´¢å¼•å’ŒæŸ¥è©¢å„ªåŒ–
   - é€£ç·šæ± é…ç½®
  
## å¥åº·æª¢æŸ¥èˆ‡ç›£æ§
  
### å¥åº·æª¢æŸ¥ç«¯é»
  
```nginx
location /health {
    proxy_pass http://fastapi_backend/health;
    access_log off;  # å¥åº·æª¢æŸ¥ä¸è¨˜éŒ„æ—¥èªŒ
}
```
  
- æª¢æŸ¥ Redis å’Œ PostgreSQL é€£ç·šç‹€æ…‹
- ä¸è¨˜éŒ„å¥åº·æª¢æŸ¥è«‹æ±‚æ—¥èªŒï¼Œé¿å…æ—¥èªŒæ±¡æŸ“
  
### ç›£æ§ç«¯é»
  
```nginx
location /nginx_status {
    stub_status on;
    access_log off;
    allow 127.0.0.1;  # åƒ…æœ¬åœ°è¨ªå•
    deny all;         # æ‹’çµ•å…¶ä»–è¨ªå•
}
```
  
- æä¾› Nginx ç‹€æ…‹è³‡è¨Šï¼Œä¾¿æ–¼ç›£æ§
- é™åˆ¶è¨ªå•æ¬Šé™ç¢ºä¿å®‰å…¨æ€§
  
### éŒ¯èª¤è™•ç†
  
```nginx
error_page 502 503 504 /50x.html;
location = /50x.html {
    return 503 '{"error": "Service temporarily unavailable"}';
    add_header Content-Type application/json;
}
```
  
- çµ±ä¸€éŒ¯èª¤éŸ¿æ‡‰æ ¼å¼
- æä¾› JSON æ ¼å¼çš„éŒ¯èª¤è³‡è¨Š
  
## æ“´å±•èˆ‡ç¶­è­·
  
### æœå‹™æ“´å±•
  
è¦æ“´å±•æ›´å¤š FastAPI å¯¦ä¾‹ï¼Œéœ€åœ¨ nginx é…ç½®ä¸­æ·»åŠ ï¼š
  
```nginx
server fastapi-n:8000 weight=1 max_fails=3 fail_timeout=30s;
```
  
åŒæ™‚åœ¨ docker-compose.yml ä¸­å®šç¾©ç›¸æ‡‰æœå‹™å®¹å™¨ã€‚
  
### é…ç½®ç”Ÿæ•ˆ
  
ä¿®æ”¹ nginx é…ç½®å¾Œï¼Œéœ€é‡å•Ÿ nginx æœå‹™ä»¥ä½¿é…ç½®ç”Ÿæ•ˆï¼š
  
```bash
docker-compose restart nginx
```
  
### ç›£æ§å’Œèª¿å„ª
  
- å®šæœŸæª¢æŸ¥ Nginx ç‹€æ…‹ç«¯é»ä»¥äº†è§£ç³»çµ±æ€§èƒ½
- ç›£æ§ FastAPI å¯¦ä¾‹çš„è³‡æºä½¿ç”¨æƒ…æ³
- æ ¹æ“šå¯¦éš›æµé‡æ¨¡å¼èª¿æ•´é™æµå’Œè¶…æ™‚è¨­å®š
- è§€å¯Ÿ Redis å’Œ PostgreSQL çš„æ€§èƒ½æŒ‡æ¨™
- æª¢æŸ¥ Worker çš„è™•ç†å»¶é²å’ŒéŒ¯èª¤ç‡
  
## ç¸½çµ
  
æœ¬ç³»çµ±é€šé Nginx èˆ‡ FastAPI çš„ç·Šå¯†é…åˆï¼Œå¯¦ç¾äº†é«˜æ•ˆèƒ½çš„æ—¥èªŒæ”¶é›†èƒ½åŠ›ã€‚Nginx ä½œç‚ºè² è¼‰å‡è¡¡å™¨å’Œåå‘ä»£ç†ï¼Œæä¾›äº†è«‹æ±‚è·¯ç”±ã€é™æµã€å¥åº·æª¢æŸ¥ç­‰åŠŸèƒ½ï¼›FastAPI ä½œç‚ºå‰ç«¯æœå‹™ï¼Œæä¾›äº†éåŒæ­¥è™•ç†ã€æ•¸æ“šå¿«å–ç­‰åŠŸèƒ½ï¼›Worker ä½œç‚ºå¾Œç«¯è™•ç†æœå‹™ï¼Œå¯¦ç¾äº†æ—¥èªŒçš„éåŒæ­¥æŒä¹…åŒ–ã€‚
  
**Worker æ ¸å¿ƒç‰¹æ€§**ï¼š
- **éåŒæ­¥è§£è€¦**: FastAPI å¿«é€Ÿæ¥æ”¶è«‹æ±‚ï¼ˆ<5msï¼‰ï¼ŒWorker ç¨ç«‹è™•ç†æŒä¹…åŒ–
- **æ‰¹æ¬¡è™•ç†**: æ¯æ¬¡è™•ç† 100 ç­†æ—¥èªŒï¼Œä½¿ç”¨åŸç”Ÿ SQL æ‰¹æ¬¡æ’å…¥
- **æ¶ˆè²»è€…ç¾¤çµ„**: Redis Stream è‡ªå‹•åˆ†é…è¨Šæ¯ï¼Œæ”¯æ´å¤š Worker æ“´å±•
- **å®¹éŒ¯æ©Ÿåˆ¶**: å®Œæ•´çš„éŒ¯èª¤è™•ç†ã€é‡è©¦é‚è¼¯å’Œå„ªé›…åœæ©Ÿ
- **è³‡æºç®¡ç†**: é€£ç·šæ± é…ç½®ã€è‡ªå‹•æ¸…ç†ã€ä¿¡è™Ÿè™•ç†
  
**Redis Stream é—œéµè§’è‰²**ï¼š
- **ç·©è¡å±¤**: è§£è€¦ API å±¤èˆ‡è³‡æ–™åº«å±¤ï¼Œå¸æ”¶æµé‡å³°å€¼
- **è¨Šæ¯éšŠåˆ—**: ä¿è­‰è¨Šæ¯é †åºå’Œè‡³å°‘ä¸€æ¬¡äº¤ä»˜
- **è² è¼‰åˆ†é…**: é€éæ¶ˆè²»è€…ç¾¤çµ„å¯¦ç¾è‡ªå‹•è² è¼‰å‡è¡¡
- **æ•…éšœæ¢å¾©**: Pending Entries List è¿½è¹¤æœªç¢ºèªè¨Šæ¯
  
ä¸‰è€…çµåˆå½¢æˆäº†ç©©å®šã€é«˜æ•ˆã€å¯æ“´å±•çš„æ—¥èªŒæ”¶é›†ç³»çµ±æ¶æ§‹ï¼Œèƒ½å¤ æ”¯æ´é«˜ä½µç™¼çš„æ—¥èªŒå¯«å…¥éœ€æ±‚ä¸¦æä¾›å¿«é€ŸéŸ¿æ‡‰ã€‚
  