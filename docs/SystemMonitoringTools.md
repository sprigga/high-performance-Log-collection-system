# ç³»çµ±ç›£æ§å·¥å…·å®Œæ•´æŒ‡å—

## ğŸ“‹ ç›®éŒ„
1. [Prometheus + Grafana ç›£æ§æ–¹æ¡ˆ](#prometheus--grafana-ç›£æ§æ–¹æ¡ˆ)
2. [å³æ™‚ç³»çµ±ç›£æ§å·¥å…·](#å³æ™‚ç³»çµ±ç›£æ§å·¥å…·)
3. [æ—¥èªŒåˆ†æå·¥å…·](#æ—¥èªŒåˆ†æå·¥å…·)
4. [å®¹å™¨ç›£æ§](#å®¹å™¨ç›£æ§)
5. [å®Œæ•´ç›£æ§æ¶æ§‹](#å®Œæ•´ç›£æ§æ¶æ§‹)
6. [å‘Šè­¦é…ç½®](#å‘Šè­¦é…ç½®)

---

## Prometheus + Grafana ç›£æ§æ–¹æ¡ˆ

### 1. Prometheus é…ç½®

#### åŸºç¤é…ç½®æ–‡ä»¶

```yaml
# prometheus.yml
global:
  scrape_interval: 15s      # æ¯ 15 ç§’æŠ“å–ä¸€æ¬¡æŒ‡æ¨™
  evaluation_interval: 15s   # æ¯ 15 ç§’è©•ä¼°ä¸€æ¬¡å‘Šè­¦è¦å‰‡
  
  external_labels:
    cluster: 'log-collection-system'
    environment: 'production'

# å‘Šè­¦ç®¡ç†å™¨é…ç½®
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

# å‘Šè­¦è¦å‰‡æ–‡ä»¶
rule_files:
  - "alerts/*.yml"

# æŠ“å–é…ç½®
scrape_configs:
  # FastAPI æ‡‰ç”¨ç¨‹å¼ç›£æ§
  - job_name: 'fastapi'
    static_configs:
      - targets: 
        - 'fastapi-1:8000'
        - 'fastapi-2:8000'
        - 'fastapi-3:8000'
    metrics_path: '/metrics'
    scrape_interval: 5s
    
  # Redis ç›£æ§
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']
    scrape_interval: 10s
  
  # PostgreSQL ç›£æ§
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    scrape_interval: 10s
  
  # Nginx ç›£æ§
  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']
    scrape_interval: 10s
  
  # Node Exporter (ç³»çµ±è³‡æºç›£æ§)
  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
    scrape_interval: 10s
  
  # Worker ç›£æ§
  - job_name: 'worker'
    static_configs:
      - targets:
        - 'worker-1:8001'
        - 'worker-2:8001'
    metrics_path: '/metrics'
    scrape_interval: 10s
```

#### Docker Compose é…ç½®

```yaml
# docker-compose.yml ä¸­åŠ å…¥ç›£æ§æœå‹™
version: '3.8'

services:
  # ... å…¶ä»–æœå‹™ ...
  
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./prometheus/alerts:/etc/prometheus/alerts
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - monitoring
    restart: unless-stopped
  
  # Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    depends_on:
      - prometheus
    networks:
      - monitoring
    restart: unless-stopped
  
  # AlertManager
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    networks:
      - monitoring
    restart: unless-stopped
  
  # Redis Exporter
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: redis-exporter
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis:6379
    depends_on:
      - redis
    networks:
      - monitoring
    restart: unless-stopped
  
  # PostgreSQL Exporter
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: postgres-exporter
    ports:
      - "9187:9187"
    environment:
      - DATA_SOURCE_NAME=postgresql://user:pass@postgres:5432/logs_db?sslmode=disable
    depends_on:
      - postgres
    networks:
      - monitoring
    restart: unless-stopped
  
  # Nginx Exporter
  nginx-exporter:
    image: nginx/nginx-prometheus-exporter:latest
    container_name: nginx-exporter
    ports:
      - "9113:9113"
    command:
      - '-nginx.scrape-uri=http://nginx:80/stub_status'
    depends_on:
      - nginx
    networks:
      - monitoring
    restart: unless-stopped
  
  # Node Exporter
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--path.rootfs=/rootfs'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - monitoring
    restart: unless-stopped

volumes:
  prometheus_data:
  grafana_data:
  alertmanager_data:

networks:
  monitoring:
    driver: bridge
```

### 2. FastAPI æ‡‰ç”¨ç¨‹å¼æŒ‡æ¨™æ•´åˆ

#### å®Œæ•´çš„ metrics.py æ¨¡çµ„

```python
# metrics.py
from prometheus_client import (
    Counter, Histogram, Gauge, Summary,
    generate_latest, CONTENT_TYPE_LATEST
)
from fastapi import Response
import time
import psutil
import functools

# ==================== HTTP è«‹æ±‚æŒ‡æ¨™ ====================
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration in seconds',
    ['method', 'endpoint'],
    buckets=(0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0)
)

http_request_size_bytes = Summary(
    'http_request_size_bytes',
    'HTTP request size in bytes',
    ['method', 'endpoint']
)

http_response_size_bytes = Summary(
    'http_response_size_bytes',
    'HTTP response size in bytes',
    ['method', 'endpoint']
)

# ==================== Redis æŒ‡æ¨™ ====================
redis_stream_messages_total = Counter(
    'redis_stream_messages_total',
    'Total messages written to Redis Stream',
    ['status']  # success, failed
)

redis_stream_size = Gauge(
    'redis_stream_size',
    'Current size of Redis Stream'
)

redis_cache_hits_total = Counter(
    'redis_cache_hits_total',
    'Total Redis cache hits'
)

redis_cache_misses_total = Counter(
    'redis_cache_misses_total',
    'Total Redis cache misses'
)

redis_operation_duration_seconds = Histogram(
    'redis_operation_duration_seconds',
    'Redis operation duration',
    ['operation'],  # xadd, get, set, xreadgroup
    buckets=(0.0001, 0.0005, 0.001, 0.0025, 0.005, 0.01, 0.025, 0.05, 0.1)
)

# ==================== è³‡æ–™åº«æŒ‡æ¨™ ====================
db_connections_active = Gauge(
    'db_connections_active',
    'Active database connections',
    ['pool']  # master, replica
)

db_connections_idle = Gauge(
    'db_connections_idle',
    'Idle database connections',
    ['pool']
)

db_query_duration_seconds = Histogram(
    'db_query_duration_seconds',
    'Database query duration',
    ['query_type', 'pool'],  # select, insert, update, delete
    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0)
)

db_queries_total = Counter(
    'db_queries_total',
    'Total database queries',
    ['query_type', 'status']  # success, error
)

# ==================== æ¥­å‹™æŒ‡æ¨™ ====================
logs_received_total = Counter(
    'logs_received_total',
    'Total logs received',
    ['device_id', 'log_level']
)

logs_processing_errors_total = Counter(
    'logs_processing_errors_total',
    'Total log processing errors',
    ['error_type']
)

batch_processing_duration_seconds = Histogram(
    'batch_processing_duration_seconds',
    'Batch processing duration',
    ['batch_size'],
    buckets=(0.01, 0.05, 0.1, 0.5, 1.0, 2.5, 5.0, 10.0)
)

active_devices_total = Gauge(
    'active_devices_total',
    'Total number of active devices'
)

# ==================== ç³»çµ±è³‡æºæŒ‡æ¨™ ====================
system_cpu_usage_percent = Gauge(
    'system_cpu_usage_percent',
    'System CPU usage percentage'
)

system_memory_usage_bytes = Gauge(
    'system_memory_usage_bytes',
    'System memory usage in bytes',
    ['type']  # used, available, total
)

system_disk_usage_bytes = Gauge(
    'system_disk_usage_bytes',
    'System disk usage in bytes',
    ['type']  # used, free, total
)

# ==================== Worker æŒ‡æ¨™ ====================
worker_active_tasks = Gauge(
    'worker_active_tasks',
    'Number of active worker tasks',
    ['worker_id']
)

worker_processed_logs_total = Counter(
    'worker_processed_logs_total',
    'Total logs processed by worker',
    ['worker_id', 'status']  # success, failed
)

worker_batch_size = Histogram(
    'worker_batch_size',
    'Worker batch size distribution',
    buckets=(10, 25, 50, 100, 200, 500, 1000)
)


# ==================== è£é£¾å™¨å’Œè¼”åŠ©å‡½æ•¸ ====================
def track_time(metric: Histogram, labels: dict = None):
    """è¿½è¹¤å‡½æ•¸åŸ·è¡Œæ™‚é–“çš„è£é£¾å™¨"""
    def decorator(func):
        @functools.wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                if labels:
                    metric.labels(**labels).observe(duration)
                else:
                    metric.observe(duration)
        
        @functools.wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                if labels:
                    metric.labels(**labels).observe(duration)
                else:
                    metric.observe(duration)
        
        # æ ¹æ“šå‡½æ•¸é¡å‹è¿”å›å°æ‡‰çš„åŒ…è£å™¨
        import asyncio
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        return sync_wrapper
    
    return decorator


def update_system_metrics():
    """æ›´æ–°ç³»çµ±è³‡æºæŒ‡æ¨™"""
    # CPU ä½¿ç”¨ç‡
    cpu_percent = psutil.cpu_percent(interval=1)
    system_cpu_usage_percent.set(cpu_percent)
    
    # è¨˜æ†¶é«”ä½¿ç”¨
    memory = psutil.virtual_memory()
    system_memory_usage_bytes.labels(type='used').set(memory.used)
    system_memory_usage_bytes.labels(type='available').set(memory.available)
    system_memory_usage_bytes.labels(type='total').set(memory.total)
    
    # ç£ç¢Ÿä½¿ç”¨
    disk = psutil.disk_usage('/')
    system_disk_usage_bytes.labels(type='used').set(disk.used)
    system_disk_usage_bytes.labels(type='free').set(disk.free)
    system_disk_usage_bytes.labels(type='total').set(disk.total)


class MetricsMiddleware:
    """FastAPI ä¸­é–“ä»¶ç”¨æ–¼è‡ªå‹•è¨˜éŒ„ HTTP æŒ‡æ¨™"""
    
    def __init__(self, app):
        self.app = app
    
    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            await self.app(scope, receive, send)
            return
        
        start_time = time.time()
        
        # è¨˜éŒ„è«‹æ±‚å¤§å°
        request_size = 0
        
        async def receive_with_size():
            nonlocal request_size
            message = await receive()
            if message["type"] == "http.request":
                body = message.get("body", b"")
                request_size += len(body)
            return message
        
        # è¨˜éŒ„å›æ‡‰å¤§å°
        response_size = 0
        status_code = 500
        
        async def send_with_size(message):
            nonlocal response_size, status_code
            if message["type"] == "http.response.start":
                status_code = message["status"]
            elif message["type"] == "http.response.body":
                body = message.get("body", b"")
                response_size += len(body)
            await send(message)
        
        try:
            await self.app(scope, receive_with_size, send_with_size)
        finally:
            # è¨˜éŒ„æŒ‡æ¨™
            duration = time.time() - start_time
            method = scope["method"]
            path = scope["path"]
            
            http_requests_total.labels(
                method=method,
                endpoint=path,
                status=status_code
            ).inc()
            
            http_request_duration_seconds.labels(
                method=method,
                endpoint=path
            ).observe(duration)
            
            http_request_size_bytes.labels(
                method=method,
                endpoint=path
            ).observe(request_size)
            
            http_response_size_bytes.labels(
                method=method,
                endpoint=path
            ).observe(response_size)
```

#### åœ¨ FastAPI ä¸­ä½¿ç”¨ Metrics

```python
# main.py
from fastapi import FastAPI, Response
from metrics import (
    MetricsMiddleware,
    generate_latest,
    CONTENT_TYPE_LATEST,
    logs_received_total,
    redis_stream_messages_total,
    update_system_metrics,
    track_time,
    redis_operation_duration_seconds
)
import asyncio

app = FastAPI(title="Log Collection API")

# åŠ å…¥ Metrics ä¸­é–“ä»¶
app.add_middleware(MetricsMiddleware)

# èƒŒæ™¯ä»»å‹™ï¼šå®šæœŸæ›´æ–°ç³»çµ±æŒ‡æ¨™
async def update_metrics_task():
    """èƒŒæ™¯ä»»å‹™ï¼šå®šæœŸæ›´æ–°ç³»çµ±æŒ‡æ¨™"""
    while True:
        update_system_metrics()
        await asyncio.sleep(15)  # æ¯ 15 ç§’æ›´æ–°ä¸€æ¬¡

@app.on_event("startup")
async def startup_event():
    """å•Ÿå‹•æ™‚é–‹å§‹èƒŒæ™¯ä»»å‹™"""
    asyncio.create_task(update_metrics_task())

@app.post("/api/log")
async def create_log(log: LogEntry):
    """æ¥æ”¶æ—¥èªŒä¸¦å¿«é€Ÿå¯«å…¥ Redis Stream"""
    # è¨˜éŒ„æ¥­å‹™æŒ‡æ¨™
    logs_received_total.labels(
        device_id=log.device_id,
        log_level=log.log_level
    ).inc()
    
    # æº–å‚™è³‡æ–™
    log_dict = {
        "device_id": log.device_id,
        "log_level": log.log_level,
        "message": log.message,
        "log_data": json.dumps(log.log_data),
        "timestamp": datetime.now().isoformat()
    }
    
    # è¿½è¹¤ Redis æ“ä½œæ™‚é–“
    start_time = time.time()
    try:
        # å¯«å…¥ Redis Stream
        message_id = await redis_client.xadd(
            "logs:stream",
            log_dict,
            maxlen=100000
        )
        
        # è¨˜éŒ„æˆåŠŸ
        redis_stream_messages_total.labels(status='success').inc()
        
        # è¨˜éŒ„æ“ä½œæ™‚é–“
        duration = time.time() - start_time
        redis_operation_duration_seconds.labels(operation='xadd').observe(duration)
        
        return LogResponse(status="queued", message_id=message_id)
    
    except Exception as e:
        redis_stream_messages_total.labels(status='failed').inc()
        raise

@app.get("/metrics")
async def metrics():
    """Prometheus metrics ç«¯é»"""
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {"status": "healthy"}
```

### 3. Grafana å„€è¡¨æ¿é…ç½®

#### è‡ªå‹•é…ç½® Datasource

```yaml
# grafana/provisioning/datasources/prometheus.yml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      timeInterval: "15s"
```

#### é è¨­å„€è¡¨æ¿é…ç½®

```yaml
# grafana/provisioning/dashboards/default.yml
apiVersion: 1

providers:
  - name: 'default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /var/lib/grafana/dashboards
```

#### å®Œæ•´å„€è¡¨æ¿ JSON

```json
{
  "dashboard": {
    "title": "æ—¥èªŒæ”¶é›†ç³»çµ±æ•ˆèƒ½å„€è¡¨æ¿",
    "timezone": "browser",
    "refresh": "10s",
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "panels": [
      {
        "id": 1,
        "title": "æ¯ç§’è«‹æ±‚æ•¸ (QPS)",
        "type": "graph",
        "gridPos": {
          "x": 0,
          "y": 0,
          "w": 12,
          "h": 8
        },
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[1m]))",
            "legendFormat": "ç¸½ QPS",
            "refId": "A"
          },
          {
            "expr": "sum(rate(http_requests_total{status=~\"2..\"}[1m]))",
            "legendFormat": "æˆåŠŸè«‹æ±‚",
            "refId": "B"
          },
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[1m]))",
            "legendFormat": "éŒ¯èª¤è«‹æ±‚",
            "refId": "C"
          }
        ],
        "yaxes": [
          {
            "format": "reqps",
            "label": "è«‹æ±‚/ç§’"
          }
        ]
      },
      {
        "id": 2,
        "title": "HTTP è«‹æ±‚å»¶é² (P50, P95, P99)",
        "type": "graph",
        "gridPos": {
          "x": 12,
          "y": 0,
          "w": 12,
          "h": 8
        },
        "targets": [
          {
            "expr": "histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P50",
            "refId": "A"
          },
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P95",
            "refId": "B"
          },
          {
            "expr": "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "P99",
            "refId": "C"
          }
        ],
        "yaxes": [
          {
            "format": "s",
            "label": "å»¶é²æ™‚é–“"
          }
        ]
      },
      {
        "id": 3,
        "title": "Redis Stream å¤§å°",
        "type": "graph",
        "gridPos": {
          "x": 0,
          "y": 8,
          "w": 8,
          "h": 8
        },
        "targets": [
          {
            "expr": "redis_stream_size",
            "legendFormat": "Stream é•·åº¦",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "short",
            "label": "è¨Šæ¯æ•¸é‡"
          }
        ]
      },
      {
        "id": 4,
        "title": "Redis å¿«å–å‘½ä¸­ç‡",
        "type": "graph",
        "gridPos": {
          "x": 8,
          "y": 8,
          "w": 8,
          "h": 8
        },
        "targets": [
          {
            "expr": "rate(redis_cache_hits_total[5m]) / (rate(redis_cache_hits_total[5m]) + rate(redis_cache_misses_total[5m])) * 100",
            "legendFormat": "å¿«å–å‘½ä¸­ç‡",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "percent",
            "label": "å‘½ä¸­ç‡",
            "max": 100,
            "min": 0
          }
        ]
      },
      {
        "id": 5,
        "title": "è³‡æ–™åº«é€£ç·šæ•¸",
        "type": "graph",
        "gridPos": {
          "x": 16,
          "y": 8,
          "w": 8,
          "h": 8
        },
        "targets": [
          {
            "expr": "db_connections_active",
            "legendFormat": "æ´»å‹•é€£ç·š - {{pool}}",
            "refId": "A"
          },
          {
            "expr": "db_connections_idle",
            "legendFormat": "é–’ç½®é€£ç·š - {{pool}}",
            "refId": "B"
          }
        ],
        "yaxes": [
          {
            "format": "short",
            "label": "é€£ç·šæ•¸"
          }
        ]
      },
      {
        "id": 6,
        "title": "ç³»çµ± CPU ä½¿ç”¨ç‡",
        "type": "graph",
        "gridPos": {
          "x": 0,
          "y": 16,
          "w": 8,
          "h": 8
        },
        "targets": [
          {
            "expr": "system_cpu_usage_percent",
            "legendFormat": "CPU ä½¿ç”¨ç‡",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "percent",
            "label": "ä½¿ç”¨ç‡",
            "max": 100,
            "min": 0
          }
        ]
      },
      {
        "id": 7,
        "title": "ç³»çµ±è¨˜æ†¶é«”ä½¿ç”¨",
        "type": "graph",
        "gridPos": {
          "x": 8,
          "y": 16,
          "w": 8,
          "h": 8
        },
        "targets": [
          {
            "expr": "system_memory_usage_bytes{type='used'}",
            "legendFormat": "å·²ä½¿ç”¨",
            "refId": "A"
          },
          {
            "expr": "system_memory_usage_bytes{type='available'}",
            "legendFormat": "å¯ç”¨",
            "refId": "B"
          }
        ],
        "yaxes": [
          {
            "format": "bytes",
            "label": "è¨˜æ†¶é«”"
          }
        ]
      },
      {
        "id": 8,
        "title": "æ¯ç§’æ—¥èªŒæ¥æ”¶æ•¸",
        "type": "graph",
        "gridPos": {
          "x": 16,
          "y": 16,
          "w": 8,
          "h": 8
        },
        "targets": [
          {
            "expr": "sum(rate(logs_received_total[1m])) by (log_level)",
            "legendFormat": "{{log_level}}",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "short",
            "label": "æ—¥èªŒ/ç§’"
          }
        ]
      },
      {
        "id": 9,
        "title": "Worker è™•ç†æ•ˆèƒ½",
        "type": "graph",
        "gridPos": {
          "x": 0,
          "y": 24,
          "w": 12,
          "h": 8
        },
        "targets": [
          {
            "expr": "rate(worker_processed_logs_total{status='success'}[1m])",
            "legendFormat": "Worker {{worker_id}} - æˆåŠŸ",
            "refId": "A"
          },
          {
            "expr": "rate(worker_processed_logs_total{status='failed'}[1m])",
            "legendFormat": "Worker {{worker_id}} - å¤±æ•—",
            "refId": "B"
          }
        ],
        "yaxes": [
          {
            "format": "short",
            "label": "è™•ç†é€Ÿåº¦"
          }
        ]
      },
      {
        "id": 10,
        "title": "è³‡æ–™åº«æŸ¥è©¢å»¶é²",
        "type": "graph",
        "gridPos": {
          "x": 12,
          "y": 24,
          "w": 12,
          "h": 8
        },
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(db_query_duration_seconds_bucket[5m])) by (le, query_type))",
            "legendFormat": "P95 - {{query_type}}",
            "refId": "A"
          },
          {
            "expr": "histogram_quantile(0.99, sum(rate(db_query_duration_seconds_bucket[5m])) by (le, query_type))",
            "legendFormat": "P99 - {{query_type}}",
            "refId": "B"
          }
        ],
        "yaxes": [
          {
            "format": "s",
            "label": "æŸ¥è©¢æ™‚é–“"
          }
        ]
      }
    ]
  }
}
```

---

## å³æ™‚ç³»çµ±ç›£æ§å·¥å…·

### 1. htop - äº’å‹•å¼ç¨‹åºç›£æ§

#### å®‰è£èˆ‡åŸºæœ¬ä½¿ç”¨

```bash
# å®‰è£
sudo apt-get update
sudo apt-get install htop

# å•Ÿå‹•
htop

# å¸¸ç”¨å¿«æ·éµ:
# F1: èªªæ˜
# F2: è¨­å®š
# F3: æœå°‹ç¨‹åº
# F4: éæ¿¾
# F5: æ¨¹ç‹€é¡¯ç¤º
# F6: æ’åº
# F9: çµ‚æ­¢ç¨‹åº
# F10: é›¢é–‹
```

#### htop é…ç½®æ–‡ä»¶

```bash
# ~/.config/htop/htoprc
fields=0 48 17 18 38 39 40 2 46 47 49 1
sort_key=46
sort_direction=1
hide_threads=0
hide_kernel_threads=1
hide_userland_threads=0
shadow_other_users=0
show_thread_names=0
show_program_path=1
highlight_base_name=1
highlight_megabytes=1
highlight_threads=1
tree_view=1
header_margin=1
detailed_cpu_time=0
cpu_count_from_zero=0
update_process_names=0
account_guest_in_cpu_meter=0
color_scheme=0
delay=15
left_meters=AllCPUs Memory Swap
left_meter_modes=1 1 1
right_meters=Tasks LoadAverage Uptime
right_meter_modes=2 2 2
```

### 2. glances - å…¨é¢çš„ç³»çµ±ç›£æ§

#### å®‰è£

```bash
# Ubuntu/Debian
sudo apt-get install glances

# æˆ–ä½¿ç”¨ pip
pip install glances[all]
```

#### åŸºæœ¬ä½¿ç”¨

```bash
# åŸºæœ¬å•Ÿå‹•
glances

# Web ä¼ºæœå™¨æ¨¡å¼ (å¯é€éç€è¦½å™¨å­˜å–)
glances -w

# è¨ªå•: http://localhost:61208

# å®¢æˆ¶ç«¯æ¨¡å¼ (é€£æ¥åˆ°é ç«¯ä¼ºæœå™¨)
glances -c <server_ip>

# åŒ¯å‡ºåˆ° CSV
glances --export csv --export-csv-file /tmp/glances.csv

# åŒ¯å‡ºåˆ° Prometheus
glances --export prometheus

# å¸¸ç”¨å¿«æ·éµ:
# h: èªªæ˜
# q: é›¢é–‹
# 1: åˆ‡æ› CPU é¡¯ç¤ºæ¨¡å¼
# m: æŒ‰è¨˜æ†¶é«”æ’åº
# c: æŒ‰ CPU æ’åº
# i: æŒ‰ I/O æ’åº
# a: è‡ªå‹•æ’åº
# d: é¡¯ç¤º/éš±è—ç£ç¢Ÿ I/O
# n: é¡¯ç¤º/éš±è—ç¶²è·¯
# s: é¡¯ç¤º/éš±è—æ„Ÿæ‡‰å™¨
# f: é¡¯ç¤º/éš±è—æª”æ¡ˆç³»çµ±
# /: æœå°‹ç¨‹åº
```

#### glances é…ç½®æ–‡ä»¶

```ini
# ~/.config/glances/glances.conf
[global]
check_update=false
refresh=2

[quicklook]
cpu_careful=50
cpu_warning=70
cpu_critical=90
mem_careful=50
mem_warning=70
mem_critical=90

[cpu]
user_careful=50
user_warning=70
user_critical=90
system_careful=50
system_warning=70
system_critical=90

[mem]
careful=50
warning=70
critical=90

[memswap]
careful=50
warning=70
critical=90

[load]
careful=0.7
warning=1.0
critical=5.0

[network]
hide=lo
rx_careful=70
rx_warning=80
rx_critical=90
tx_careful=70
tx_warning=80
tx_critical=90

[diskio]
hide=loop.*,ram.*
```

### 3. dstat - å³æ™‚ç³»çµ±çµ±è¨ˆ

```bash
# å®‰è£
sudo apt-get install dstat

# åŸºæœ¬ä½¿ç”¨
dstat

# è©³ç´°çš„ CPUã€è¨˜æ†¶é«”ã€ç¶²è·¯ã€ç£ç¢Ÿè³‡è¨Š
dstat -cdngy

# æ¯ 5 ç§’æ›´æ–°ä¸€æ¬¡,é¡¯ç¤º 10 æ¬¡
dstat 5 10

# è¼¸å‡ºåˆ° CSV
dstat --output /tmp/dstat.csv 5
```

### 4. å®¢è£½åŒ–ç›£æ§è…³æœ¬

#### ç³»çµ±è³‡æºç›£æ§è…³æœ¬

```python
# system_monitor.py
#!/usr/bin/env python3
import psutil
import time
from datetime import datetime
import json

def get_system_info():
    """ç²å–ç³»çµ±è³‡è¨Š"""
    cpu_percent = psutil.cpu_percent(interval=1, percpu=True)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    net_io = psutil.net_io_counters()
    
    return {
        'timestamp': datetime.now().isoformat(),
        'cpu': {
            'total': psutil.cpu_percent(interval=1),
            'per_core': cpu_percent,
            'count': psutil.cpu_count()
        },
        'memory': {
            'total': memory.total,
            'available': memory.available,
            'used': memory.used,
            'percent': memory.percent
        },
        'disk': {
            'total': disk.total,
            'used': disk.used,
            'free': disk.free,
            'percent': disk.percent
        },
        'network': {
            'bytes_sent': net_io.bytes_sent,
            'bytes_recv': net_io.bytes_recv,
            'packets_sent': net_io.packets_sent,
            'packets_recv': net_io.packets_recv
        }
    }

def format_bytes(bytes):
    """æ ¼å¼åŒ–ä½å…ƒçµ„é¡¯ç¤º"""
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes < 1024.0:
            return f"{bytes:.2f} {unit}"
        bytes /= 1024.0

def print_system_info():
    """æ¼‚äº®åœ°åˆ—å°ç³»çµ±è³‡è¨Š"""
    info = get_system_info()
    
    print("\n" + "="*60)
    print(f"æ™‚é–“: {info['timestamp']}")
    print("="*60)
    
    print(f"\nğŸ–¥ï¸  CPU:")
    print(f"  ç¸½ä½¿ç”¨ç‡: {info['cpu']['total']:.1f}%")
    print(f"  æ¯æ ¸å¿ƒä½¿ç”¨ç‡: {', '.join([f'{x:.1f}%' for x in info['cpu']['per_core']])}")
    
    print(f"\nğŸ’¾ è¨˜æ†¶é«”:")
    print(f"  ç¸½é‡: {format_bytes(info['memory']['total'])}")
    print(f"  å·²ä½¿ç”¨: {format_bytes(info['memory']['used'])} ({info['memory']['percent']:.1f}%)")
    print(f"  å¯ç”¨: {format_bytes(info['memory']['available'])}")
    
    print(f"\nğŸ’¿ ç£ç¢Ÿ:")
    print(f"  ç¸½é‡: {format_bytes(info['disk']['total'])}")
    print(f"  å·²ä½¿ç”¨: {format_bytes(info['disk']['used'])} ({info['disk']['percent']:.1f}%)")
    print(f"  å¯ç”¨: {format_bytes(info['disk']['free'])}")
    
    print(f"\nğŸŒ ç¶²è·¯:")
    print(f"  ç™¼é€: {format_bytes(info['network']['bytes_sent'])}")
    print(f"  æ¥æ”¶: {format_bytes(info['network']['bytes_recv'])}")

def monitor_loop(interval=5, output_file=None):
    """æŒçºŒç›£æ§ä¸¦å¯é¸æ“‡è¼¸å‡ºåˆ°æ–‡ä»¶"""
    try:
        while True:
            print_system_info()
            
            if output_file:
                with open(output_file, 'a') as f:
                    info = get_system_info()
                    f.write(json.dumps(info) + '\n')
            
            time.sleep(interval)
    
    except KeyboardInterrupt:
        print("\n\nç›£æ§å·²åœæ­¢")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ç³»çµ±è³‡æºç›£æ§å·¥å…·')
    parser.add_argument('-i', '--interval', type=int, default=5,
                       help='æ›´æ–°é–“éš”(ç§’), é è¨­: 5')
    parser.add_argument('-o', '--output', type=str,
                       help='è¼¸å‡ºæ–‡ä»¶è·¯å¾‘')
    
    args = parser.parse_args()
    
    print("ğŸš€ é–‹å§‹ç³»çµ±ç›£æ§...")
    print("æŒ‰ Ctrl+C åœæ­¢")
    
    monitor_loop(interval=args.interval, output_file=args.output)
```

ä½¿ç”¨æ–¹å¼:
```bash
# æ¯ 5 ç§’æ›´æ–°ä¸€æ¬¡
python system_monitor.py

# æ¯ 2 ç§’æ›´æ–°ä¸¦å„²å­˜åˆ°æ–‡ä»¶
python system_monitor.py -i 2 -o system_metrics.jsonl
```

---

## æ—¥èªŒåˆ†æå·¥å…·

### 1. ELK Stack (Elasticsearch + Logstash + Kibana)

#### Docker Compose é…ç½®

```yaml
version: '3.8'

services:
  elasticsearch:
    image: elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
      - "9300:9300"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - elk
    restart: unless-stopped

  logstash:
    image: logstash:8.11.0
    container_name: logstash
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    ports:
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    environment:
      LS_JAVA_OPTS: "-Xmx512m -Xms512m"
    networks:
      - elk
    depends_on:
      - elasticsearch
    restart: unless-stopped

  kibana:
    image: kibana:8.11.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'
    networks:
      - elk
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  elasticsearch_data:

networks:
  elk:
    driver: bridge
```

#### Logstash é…ç½®

```ruby
# logstash/pipeline/logstash.conf
input {
  # å¾ FastAPI æ¥æ”¶æ—¥èªŒ
  tcp {
    port => 5000
    codec => json
  }
  
  # å¾æ–‡ä»¶è®€å–æ—¥èªŒ
  file {
    path => "/var/log/app/*.log"
    start_position => "beginning"
    codec => json
  }
}

filter {
  # è§£ææ™‚é–“æˆ³
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }
  
  # æ·»åŠ æ¨™ç±¤
  if [log_level] == "ERROR" {
    mutate {
      add_tag => [ "error" ]
    }
  }
  
  # è§£æ JSON æ¬„ä½
  if [log_data] {
    json {
      source => "log_data"
      target => "parsed_data"
    }
  }
  
  # Geo IP æŸ¥æ‰¾ (å¦‚æœæœ‰ IP æ¬„ä½)
  if [client_ip] {
    geoip {
      source => "client_ip"
    }
  }
}

output {
  # è¼¸å‡ºåˆ° Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # é™¤éŒ¯ç”¨: è¼¸å‡ºåˆ°æ¨™æº–è¼¸å‡º
  stdout {
    codec => rubydebug
  }
}
```

### 2. Grafana Loki

#### Docker Compose é…ç½®

```yaml
version: '3.8'

services:
  loki:
    image: grafana/loki:latest
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - ./loki/loki-config.yml:/etc/loki/loki-config.yml
      - loki_data:/loki
    command: -config.file=/etc/loki/loki-config.yml
    networks:
      - monitoring
    restart: unless-stopped

  promtail:
    image: grafana/promtail:latest
    container_name: promtail
    volumes:
      - ./promtail/promtail-config.yml:/etc/promtail/promtail-config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/promtail-config.yml
    networks:
      - monitoring
    depends_on:
      - loki
    restart: unless-stopped

volumes:
  loki_data:

networks:
  monitoring:
    driver: bridge
```

#### Loki é…ç½®

```yaml
# loki/loki-config.yml
auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 5m
  chunk_retain_period: 30s

schema_config:
  configs:
    - from: 2020-05-15
      store: boltdb
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 168h

storage_config:
  boltdb:
    directory: /loki/index
  filesystem:
    directory: /loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h

chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: false
  retention_period: 0s
```

#### Promtail é…ç½®

```yaml
# promtail/promtail-config.yml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # æŠ“å– Docker å®¹å™¨æ—¥èªŒ
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'stream'
    
  # æŠ“å–ç³»çµ±æ—¥èªŒ
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*.log
```

---

## å®¹å™¨ç›£æ§

### 1. Docker Stats

#### åŸºæœ¬ä½¿ç”¨

```bash
# é¡¯ç¤ºæ‰€æœ‰å®¹å™¨çš„çµ±è¨ˆè³‡è¨Š
docker stats

# é¡¯ç¤ºç‰¹å®šå®¹å™¨
docker stats fastapi-1 fastapi-2 fastapi-3

# ä¸æŒçºŒé¡¯ç¤º,åªé¡¯ç¤ºä¸€æ¬¡
docker stats --no-stream

# æ ¼å¼åŒ–è¼¸å‡º
docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}\t{{.BlockIO}}"
```

#### ç›£æ§è…³æœ¬

```bash
#!/bin/bash
# docker_monitor.sh

# é¡è‰²å®šç¾©
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo "ğŸ³ Docker å®¹å™¨ç›£æ§"
echo "=================="
echo ""

# ç²å–å®¹å™¨çµ±è¨ˆè³‡è¨Š
docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}\t{{.NetIO}}\t{{.BlockIO}}" | while read line; do
    # è·³éæ¨™é¡Œè¡Œ
    if [[ $line == CONTAINER* ]]; then
        echo -e "${GREEN}$line${NC}"
        continue
    fi
    
    # è§£æ CPU ä½¿ç”¨ç‡
    cpu=$(echo $line | awk '{print $2}' | sed 's/%//')
    
    # æ ¹æ“š CPU ä½¿ç”¨ç‡è‘—è‰²
    if (( $(echo "$cpu > 80" | bc -l) )); then
        echo -e "${RED}$line${NC}"
    elif (( $(echo "$cpu > 50" | bc -l) )); then
        echo -e "${YELLOW}$line${NC}"
    else
        echo "$line"
    fi
done
```

### 2. cAdvisor (Container Advisor)

```yaml
# docker-compose.yml
cadvisor:
  image: gcr.io/cadvisor/cadvisor:latest
  container_name: cadvisor
  ports:
    - "8080:8080"
  volumes:
    - /:/rootfs:ro
    - /var/run:/var/run:ro
    - /sys:/sys:ro
    - /var/lib/docker/:/var/lib/docker:ro
    - /dev/disk/:/dev/disk:ro
  privileged: true
  devices:
    - /dev/kmsg
  networks:
    - monitoring
  restart: unless-stopped
```

è¨ªå• cAdvisor UI: http://localhost:8080

---

## å®Œæ•´ç›£æ§æ¶æ§‹

### æ•´åˆçš„ Docker Compose

```yaml
# complete-monitoring-stack.yml
version: '3.8'

services:
  # ==================== æ‡‰ç”¨æœå‹™ ====================
  nginx:
    image: nginx:latest
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    networks:
      - app
      - monitoring

  fastapi-1:
    build: .
    networks:
      - app
      - monitoring

  # ... å…¶ä»–æ‡‰ç”¨æœå‹™ ...

  # ==================== Prometheus ç›£æ§ ====================
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=30d'
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    networks:
      - monitoring

  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager:/etc/alertmanager
    networks:
      - monitoring

  # ==================== Exporters ====================
  redis-exporter:
    image: oliver006/redis_exporter:latest
    environment:
      - REDIS_ADDR=redis:6379
    networks:
      - monitoring

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    environment:
      - DATA_SOURCE_NAME=postgresql://user:pass@postgres:5432/logs_db?sslmode=disable
    networks:
      - monitoring

  node-exporter:
    image: prom/node-exporter:latest
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - monitoring

  # ==================== æ—¥èªŒæ”¶é›† ====================
  loki:
    image: grafana/loki:latest
    ports:
      - "3100:3100"
    volumes:
      - ./loki:/etc/loki
      - loki_data:/loki
    networks:
      - monitoring

  promtail:
    image: grafana/promtail:latest
    volumes:
      - ./promtail:/etc/promtail
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks:
      - monitoring

  # ==================== å®¹å™¨ç›£æ§ ====================
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    privileged: true
    networks:
      - monitoring

volumes:
  prometheus_data:
  grafana_data:
  loki_data:

networks:
  app:
    driver: bridge
  monitoring:
    driver: bridge
```

---

## å‘Šè­¦é…ç½®

### AlertManager é…ç½®

```yaml
# alertmanager/alertmanager.yml
global:
  resolve_timeout: 5m
  
  # SMTP é…ç½® (Email é€šçŸ¥)
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@example.com'
  smtp_auth_username: 'alerts@example.com'
  smtp_auth_password: 'your_password'

# è·¯ç”±é…ç½®
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
    - match:
        severity: critical
      receiver: 'critical'
      continue: true
    
    - match:
        severity: warning
      receiver: 'warning'

# æ¥æ”¶å™¨é…ç½®
receivers:
  - name: 'default'
    email_configs:
      - to: 'team@example.com'
        headers:
          Subject: '[ç›£æ§å‘Šè­¦] {{ .GroupLabels.alertname }}'
  
  - name: 'critical'
    email_configs:
      - to: 'oncall@example.com'
        headers:
          Subject: '[ç·Šæ€¥å‘Šè­¦] {{ .GroupLabels.alertname }}'
    
    # Slack é€šçŸ¥
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts-critical'
        title: 'ğŸš¨ ç·Šæ€¥å‘Šè­¦'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
  
  - name: 'warning'
    email_configs:
      - to: 'team@example.com'
        headers:
          Subject: '[è­¦å‘Š] {{ .GroupLabels.alertname }}'

# æŠ‘åˆ¶è¦å‰‡
inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
```

### Prometheus å‘Šè­¦è¦å‰‡

```yaml
# prometheus/alerts/app_alerts.yml
groups:
  - name: app_alerts
    interval: 30s
    rules:
      # API å›æ‡‰æ™‚é–“å‘Šè­¦
      - alert: HighAPILatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "API å›æ‡‰æ™‚é–“éé«˜"
          description: "P95 å›æ‡‰æ™‚é–“ {{ $value }}s è¶…é 500ms"
      
      # éŒ¯èª¤ç‡å‘Šè­¦
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "éŒ¯èª¤ç‡éé«˜"
          description: "éŒ¯èª¤ç‡ {{ $value | humanizePercentage }} è¶…é 5%"
      
      # Redis Stream å †ç©å‘Šè­¦
      - alert: RedisStreamBacklog
        expr: redis_stream_size > 50000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Redis Stream è¨Šæ¯å †ç©"
          description: "Stream å¤§å° {{ $value }} è¶…é 50000"
      
      # è³‡æ–™åº«é€£ç·šæ•¸å‘Šè­¦
      - alert: HighDatabaseConnections
        expr: db_connections_active > 150
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "è³‡æ–™åº«é€£ç·šæ•¸éé«˜"
          description: "æ´»å‹•é€£ç·šæ•¸ {{ $value }} è¶…é 150"
      
      # ç³»çµ± CPU å‘Šè­¦
      - alert: HighCPUUsage
        expr: system_cpu_usage_percent > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "ç³»çµ± CPU ä½¿ç”¨ç‡éé«˜"
          description: "CPU ä½¿ç”¨ç‡ {{ $value }}% è¶…é 80%"
      
      # ç³»çµ±è¨˜æ†¶é«”å‘Šè­¦
      - alert: HighMemoryUsage
        expr: (system_memory_usage_bytes{type='used'} / system_memory_usage_bytes{type='total'}) * 100 > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "ç³»çµ±è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜"
          description: "è¨˜æ†¶é«”ä½¿ç”¨ç‡ {{ $value }}% è¶…é 85%"
      
      # æœå‹™åœæ©Ÿå‘Šè­¦
      - alert: ServiceDown
        expr: up{job=~"fastapi|redis|postgres"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "æœå‹™åœæ©Ÿ"
          description: "{{ $labels.job }} æœå‹™å·²åœæ©Ÿ"
```

### å¿«é€Ÿå•Ÿå‹•è…³æœ¬

```bash
#!/bin/bash
# start_monitoring.sh

echo "ğŸš€ å•Ÿå‹•å®Œæ•´ç›£æ§æ¶æ§‹..."

# å‰µå»ºå¿…è¦çš„ç›®éŒ„
mkdir -p prometheus/alerts
mkdir -p grafana/provisioning/{datasources,dashboards}
mkdir -p alertmanager
mkdir -p loki
mkdir -p promtail

# å•Ÿå‹•æ‰€æœ‰æœå‹™
docker-compose -f complete-monitoring-stack.yml up -d

echo "âœ… ç›£æ§æœå‹™å·²å•Ÿå‹•"
echo ""
echo "ğŸ“Š è¨ªå•ä»¥ä¸‹ URL:"
echo "  - Prometheus: http://localhost:9090"
echo "  - Grafana: http://localhost:3000 (admin/admin123)"
echo "  - AlertManager: http://localhost:9093"
echo "  - cAdvisor: http://localhost:8080"
echo ""
echo "ğŸ” æŸ¥çœ‹æœå‹™ç‹€æ…‹:"
docker-compose -f complete-monitoring-stack.yml ps
```

---

## ç¸½çµ

é€™å€‹å®Œæ•´çš„ç›£æ§æ–¹æ¡ˆæä¾›äº†:

âœ… **å¤šå±¤æ¬¡ç›£æ§**
- æ‡‰ç”¨å±¤æŒ‡æ¨™ (HTTP è«‹æ±‚ã€æ¥­å‹™é‚è¼¯)
- ç³»çµ±å±¤æŒ‡æ¨™ (CPUã€è¨˜æ†¶é«”ã€ç£ç¢Ÿ)
- å®¹å™¨å±¤æŒ‡æ¨™ (Docker å®¹å™¨è³‡æº)
- æœå‹™å±¤æŒ‡æ¨™ (Redisã€PostgreSQLã€Nginx)

âœ… **å³æ™‚å¯è¦–åŒ–**
- Grafana å„€è¡¨æ¿
- Prometheus æŸ¥è©¢ä»‹é¢
- cAdvisor å®¹å™¨ç›£æ§

âœ… **æ™ºèƒ½å‘Šè­¦**
- å¤šç´šåˆ¥å‘Šè­¦ (Criticalã€Warning)
- å¤šé€šé“é€šçŸ¥ (Emailã€Slack)
- å‘Šè­¦æŠ‘åˆ¶å’Œåˆ†çµ„

âœ… **æ—¥èªŒåˆ†æ**
- ELK Stack æˆ– Loki
- é›†ä¸­å¼æ—¥èªŒæ”¶é›†
- å¼·å¤§çš„æœå°‹å’Œåˆ†æèƒ½åŠ›

é€™å¥—ç›£æ§ç³»çµ±å¯ä»¥å¹«åŠ©ä½ å…¨é¢æŒæ¡ç³»çµ±ç‹€æ…‹,å¿«é€Ÿç™¼ç¾å’Œè§£æ±ºå•é¡Œ! ğŸ¯