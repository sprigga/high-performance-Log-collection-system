"""
FastAPI ä¸»æ‡‰ç”¨ç¨‹å¼
"""
import os
import json
import time
import asyncio
from datetime import datetime
from typing import Optional
from zoneinfo import ZoneInfo
from fastapi import FastAPI, Depends, HTTPException, Query, Response
from fastapi.responses import JSONResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, func, text
import redis.asyncio as redis

from database import get_async_db, async_engine, test_db_connection
from models import (
    LogEntryRequest, LogEntryResponse, LogQueryResponse,
    BatchLogQueryResponse, HealthCheckResponse, StatsResponse, Log,
    BatchLogEntryRequest, BatchLogEntryResponse  # æ–°å¢æ‰¹é‡æ¨¡å‹
)

# åŒ¯å…¥ Prometheus æŒ‡æ¨™æ¨¡çµ„
from metrics import (
    MetricsMiddleware,
    generate_latest,
    CONTENT_TYPE_LATEST,
    logs_received_total,
    redis_stream_messages_total,
    redis_cache_hits_total,
    redis_cache_misses_total,
    redis_operation_duration_seconds,
    redis_stream_size,
    update_system_metrics,
    batch_processing_duration_seconds,
    logs_processing_errors_total
)

# ==========================================
# æ‡‰ç”¨ç¨‹å¼åˆå§‹åŒ–
# ==========================================
app = FastAPI(
    title="é«˜æ•ˆèƒ½æ—¥èªŒæ”¶é›†ç³»çµ±",
    description="åŸºæ–¼ FastAPI + Redis + PostgreSQL çš„æ—¥èªŒæ”¶é›†ç³»çµ±",
    version="1.0.0"
)

# åŠ å…¥ Prometheus Metrics ä¸­é–“ä»¶
app.add_middleware(MetricsMiddleware)

# å¯¦ä¾‹åç¨±ï¼ˆç”¨æ–¼è­˜åˆ¥ä¸åŒçš„ FastAPI å¯¦ä¾‹ï¼‰
INSTANCE_NAME = os.getenv('INSTANCE_NAME', 'fastapi-unknown')

# Redis é…ç½®
REDIS_HOST = os.getenv('REDIS_HOST', 'localhost')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))

# Redis é€£ç·šæ± 
redis_client: Optional[redis.Redis] = None

# ==========================================
# æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸ
# ==========================================
async def update_metrics_task():
    """èƒŒæ™¯ä»»å‹™ï¼šå®šæœŸæ›´æ–°ç³»çµ±æŒ‡æ¨™"""
    while True:
        try:
            update_system_metrics()
            # æ›´æ–° Redis Stream å¤§å°
            if redis_client:
                stream_len = await redis_client.xlen('logs:stream')
                redis_stream_size.set(stream_len)
        except Exception as e:
            print(f"æ›´æ–°æŒ‡æ¨™å¤±æ•—: {e}")
        await asyncio.sleep(15)  # æ¯ 15 ç§’æ›´æ–°ä¸€æ¬¡


@app.on_event("startup")
async def startup_event():
    """
    æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚åŸ·è¡Œ
    """
    global redis_client

    print(f"ğŸš€ å•Ÿå‹• FastAPI å¯¦ä¾‹: {INSTANCE_NAME}")

    # å»ºç«‹ Redis é€£ç·šï¼ˆä½¿ç”¨é€£ç·šæ± ï¼‰
    pool = redis.ConnectionPool(
        host=REDIS_HOST,
        port=REDIS_PORT,
        decode_responses=True,
        # max_connections=50  # åŸå§‹é€£ç·šæ± å¤§å°
        max_connections=200   # æå‡é€£ç·šæ± ä»¥æ”¯æ´æ›´é«˜ä¸¦ç™¼
    )
    redis_client = redis.Redis(connection_pool=pool)

    # æ¸¬è©¦ Redis é€£ç·š
    try:
        await redis_client.ping()
        print("âœ… Redis é€£ç·šæˆåŠŸ")
    except Exception as e:
        print(f"âŒ Redis é€£ç·šå¤±æ•—: {e}")

    # æ¸¬è©¦è³‡æ–™åº«é€£ç·š
    if await test_db_connection():
        print("âœ… PostgreSQL é€£ç·šæˆåŠŸ")
    else:
        print("âŒ PostgreSQL é€£ç·šå¤±æ•—")

    # ç¢ºä¿ Redis Stream æ¶ˆè²»è€…ç¾¤çµ„å­˜åœ¨
    try:
        await redis_client.xgroup_create(
            name='logs:stream',
            groupname='log_workers',
            id='0',
            mkstream=True
        )
        print("âœ… Redis Stream ç¾¤çµ„å»ºç«‹æˆåŠŸ")
    except redis.ResponseError as e:
        if "BUSYGROUP" in str(e):
            print("â„¹ï¸ Redis Stream ç¾¤çµ„å·²å­˜åœ¨")
        else:
            print(f"âŒ Redis Stream ç¾¤çµ„å»ºç«‹å¤±æ•—: {e}")

    # å•Ÿå‹•èƒŒæ™¯ä»»å‹™ï¼šå®šæœŸæ›´æ–°ç³»çµ±æŒ‡æ¨™
    asyncio.create_task(update_metrics_task())
    print("âœ… ç³»çµ±æŒ‡æ¨™ç›£æ§å·²å•Ÿå‹•")

@app.on_event("shutdown")
async def shutdown_event():
    """
    æ‡‰ç”¨ç¨‹å¼é—œé–‰æ™‚åŸ·è¡Œ
    """
    global redis_client

    print(f"ğŸ›‘ é—œé–‰ FastAPI å¯¦ä¾‹: {INSTANCE_NAME}")

    if redis_client:
        await redis_client.close()
        print("âœ… Redis é€£ç·šå·²é—œé–‰")

# ==========================================
# API ç«¯é» - å¥åº·æª¢æŸ¥
# ==========================================
@app.get("/health", response_model=HealthCheckResponse)
async def health_check():
    """
    å¥åº·æª¢æŸ¥ç«¯é»

    æª¢æŸ¥é …ç›®ï¼š
    - Redis é€£ç·šç‹€æ…‹
    - PostgreSQL é€£ç·šç‹€æ…‹
    """
    checks = {
        "redis": False,
        "postgres": False
    }

    # æª¢æŸ¥ Redis
    try:
        await redis_client.ping()
        checks["redis"] = True
    except Exception as e:
        print(f"Redis å¥åº·æª¢æŸ¥å¤±æ•—: {e}")

    # æª¢æŸ¥ PostgreSQL
    try:
        async with async_engine.connect() as conn:
            await conn.execute(text("SELECT 1"))
        checks["postgres"] = True
    except Exception as e:
        print(f"PostgreSQL å¥åº·æª¢æŸ¥å¤±æ•—: {e}")

    status = "healthy" if all(checks.values()) else "unhealthy"

    return HealthCheckResponse(
        status=status,
        instance=INSTANCE_NAME,
        checks=checks,
        timestamp=datetime.now()
    )

# ==========================================
# API ç«¯é» - å»ºç«‹æ—¥èªŒï¼ˆå¯«å…¥ Redis Streamï¼‰
# ==========================================
@app.post("/api/log", response_model=LogEntryResponse)
async def create_log(log: LogEntryRequest):
    """
    æ¥æ”¶æ—¥èªŒä¸¦å¿«é€Ÿå¯«å…¥ Redis Stream

    æµç¨‹ï¼š
    1. é©—è­‰æ—¥èªŒæ ¼å¼
    2. å¯«å…¥ Redis Stream
    3. ç«‹å³è¿”å›ï¼ˆéåŒæ­¥è™•ç†ï¼‰

    é æœŸå›æ‡‰æ™‚é–“ï¼š< 5ms
    """
    # è¨˜éŒ„æ¥­å‹™æŒ‡æ¨™
    logs_received_total.labels(
        device_id=log.device_id,
        log_level=log.log_level
    ).inc()

    try:
        # æº–å‚™æ—¥èªŒè³‡æ–™
        log_dict = {
            "device_id": log.device_id,
            "log_level": log.log_level,
            "message": log.message,
            "log_data": json.dumps(log.log_data) if log.log_data else "{}",
            "timestamp": datetime.now(ZoneInfo("Asia/Taipei")).isoformat()
        }

        # è¿½è¹¤ Redis æ“ä½œæ™‚é–“
        start_time = time.time()

        # å¯«å…¥ Redis Streamï¼ˆè¨­å®šæœ€å¤§é•·åº¦é¿å…è¨˜æ†¶é«”æº¢å‡ºï¼‰
        message_id = await redis_client.xadd(
            name="logs:stream",
            fields=log_dict,
            maxlen=100000,  # ä¿ç•™æœ€è¿‘ 10 è¬ç­†
            approximate=True  # ä½¿ç”¨è¿‘ä¼¼å€¼æå‡æ•ˆèƒ½
        )

        # è¨˜éŒ„æˆåŠŸ
        redis_stream_messages_total.labels(status='success').inc()

        # è¨˜éŒ„æ“ä½œæ™‚é–“
        duration = time.time() - start_time
        redis_operation_duration_seconds.labels(operation='xadd').observe(duration)

        return LogEntryResponse(
            status="queued",
            message_id=message_id,
            received_at=datetime.now()
        )

    except Exception as e:
        redis_stream_messages_total.labels(status='failed').inc()
        logs_processing_errors_total.labels(error_type='redis_write').inc()
        print(f"å¯«å…¥ Redis Stream å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to queue log: {str(e)}")

# ==========================================
# API ç«¯é» - æ‰¹é‡å»ºç«‹æ—¥èªŒï¼ˆæ–°å¢é«˜æ•ˆèƒ½ç«¯é»ï¼‰
# ==========================================
@app.post("/api/logs/batch", response_model=BatchLogEntryResponse)
async def create_batch_logs(batch: BatchLogEntryRequest):
    """
    æ‰¹é‡æ¥æ”¶æ—¥èªŒä¸¦å¿«é€Ÿå¯«å…¥ Redis Streamï¼ˆä½¿ç”¨ pipelineï¼‰

    æµç¨‹ï¼š
    1. é©—è­‰æ—¥èªŒæ ¼å¼
    2. ä½¿ç”¨ Redis Pipeline æ‰¹é‡å¯«å…¥
    3. ç«‹å³è¿”å›ï¼ˆéåŒæ­¥è™•ç†ï¼‰

    é æœŸæ•ˆèƒ½ï¼šå¯è™•ç† 10,000+ logs/ç§’
    """
    batch_size = len(batch.logs)
    start_time = time.time()

    try:
        message_ids = []
        # ä¿®æ­£: ä½¿ç”¨ Asia/Taipei æ™‚å€
        # current_time = datetime.now().isoformat()  # åŸå§‹å¯«æ³•æ²’æœ‰æŒ‡å®šæ™‚å€
        current_time = datetime.now(ZoneInfo("Asia/Taipei")).isoformat()

        # ä½¿ç”¨ Redis Pipeline æ‰¹é‡æ“ä½œï¼ˆå¤§å¹…æ¸›å°‘ç¶²è·¯å¾€è¿”ï¼‰
        pipe = redis_client.pipeline()

        for log in batch.logs:
            # è¨˜éŒ„æ¥­å‹™æŒ‡æ¨™
            logs_received_total.labels(
                device_id=log.device_id,
                log_level=log.log_level
            ).inc()

            log_dict = {
                "device_id": log.device_id,
                "log_level": log.log_level,
                "message": log.message,
                "log_data": json.dumps(log.log_data) if log.log_data else "{}",
                "timestamp": current_time
            }
            pipe.xadd(
                name="logs:stream",
                fields=log_dict,
                maxlen=100000,
                approximate=True
            )

        # åŸ·è¡Œæ‰¹é‡æ“ä½œ
        results = await pipe.execute()
        message_ids = [str(r) for r in results]

        # è¨˜éŒ„æˆåŠŸ
        redis_stream_messages_total.labels(status='success').inc()

        # è¨˜éŒ„æ‰¹é‡è™•ç†æ™‚é–“
        duration = time.time() - start_time
        batch_processing_duration_seconds.labels(batch_size=str(batch_size)).observe(duration)

        return BatchLogEntryResponse(
            status="queued",
            count=batch_size,
            message_ids=message_ids,
            received_at=datetime.now()
        )

    except Exception as e:
        redis_stream_messages_total.labels(status='failed').inc()
        logs_processing_errors_total.labels(error_type='batch_redis_write').inc()
        print(f"æ‰¹é‡å¯«å…¥ Redis Stream å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to queue batch logs: {str(e)}")

# ==========================================
# API ç«¯é» - æŸ¥è©¢æ—¥èªŒï¼ˆä½¿ç”¨å¿«å–ï¼‰
# ==========================================
@app.get("/api/logs/{device_id}", response_model=BatchLogQueryResponse)
async def get_logs(
    device_id: str,
    limit: int = Query(default=100, ge=1, le=1000, description="æŸ¥è©¢ç­†æ•¸"),
    db: AsyncSession = Depends(get_async_db)
):
    """
    æŸ¥è©¢æŒ‡å®šè¨­å‚™çš„æ—¥èªŒ

    æµç¨‹ï¼š
    1. å…ˆæŸ¥è©¢ Redis å¿«å–
    2. Cache Miss æ™‚æŸ¥è©¢ PostgreSQL
    3. å°‡çµæœå¯«å…¥å¿«å–ï¼ˆTTL 5åˆ†é˜ï¼‰

    åƒæ•¸ï¼š
    - device_id: è¨­å‚™ID
    - limit: æŸ¥è©¢ç­†æ•¸ï¼ˆé è¨­100ï¼Œæœ€å¤š1000ï¼‰
    """
    cache_key = f"cache:logs:{device_id}:{limit}"

    # 1. æª¢æŸ¥ Redis å¿«å–
    try:
        start_time = time.time()
        cached_data = await redis_client.get(cache_key)
        duration = time.time() - start_time
        redis_operation_duration_seconds.labels(operation='get').observe(duration)

        if cached_data:
            redis_cache_hits_total.inc()
            logs_data = json.loads(cached_data)
            return BatchLogQueryResponse(
                total=len(logs_data),
                source="cache",
                data=[LogQueryResponse(**log) for log in logs_data]
            )
        else:
            redis_cache_misses_total.inc()
    except Exception as e:
        print(f"Redis å¿«å–è®€å–å¤±æ•—: {e}")

    # 2. Cache Miss - æŸ¥è©¢è³‡æ–™åº«
    try:
        # æŸ¥è©¢æ—¥èªŒ
        query = (
            select(Log)
            .where(Log.device_id == device_id)
            .order_by(Log.created_at.desc())
            .limit(limit)
        )
        result = await db.execute(query)
        logs = result.scalars().all()

        # è½‰æ›ç‚ºå›æ‡‰æ ¼å¼
        logs_data = [
            {
                "id": log.id,
                "device_id": log.device_id,
                "log_level": log.log_level,
                "message": log.message,
                "log_data": log.log_data,
                "created_at": log.created_at.isoformat()
            }
            for log in logs
        ]

        # 3. å¯«å…¥å¿«å–ï¼ˆTTL 5åˆ†é˜ï¼‰
        try:
            start_time = time.time()
            await redis_client.setex(
                name=cache_key,
                time=300,  # 5åˆ†é˜
                value=json.dumps(logs_data, default=str)
            )
            duration = time.time() - start_time
            redis_operation_duration_seconds.labels(operation='set').observe(duration)
        except Exception as e:
            print(f"Redis å¿«å–å¯«å…¥å¤±æ•—: {e}")

        return BatchLogQueryResponse(
            total=len(logs_data),
            source="database",
            data=[LogQueryResponse(**log) for log in logs_data]
        )

    except Exception as e:
        print(f"è³‡æ–™åº«æŸ¥è©¢å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to query logs: {str(e)}")

# ==========================================
# API ç«¯é» - çµ±è¨ˆè³‡æ–™
# ==========================================
@app.get("/api/stats", response_model=StatsResponse)
async def get_stats(db: AsyncSession = Depends(get_async_db)):
    """
    å–å¾—ç³»çµ±çµ±è¨ˆè³‡æ–™

    çµ±è¨ˆé …ç›®ï¼š
    - ç¸½æ—¥èªŒæ•¸
    - å„ç­‰ç´šæ—¥èªŒæ•¸é‡
    - æœ€è¿‘æ´»èºçš„è¨­å‚™
    """
    cache_key = "cache:stats"

    # æª¢æŸ¥å¿«å–
    try:
        cached_stats = await redis_client.get(cache_key)
        if cached_stats:
            return StatsResponse(**json.loads(cached_stats))
    except Exception as e:
        print(f"çµ±è¨ˆå¿«å–è®€å–å¤±æ•—: {e}")

    try:
        # ç¸½æ—¥èªŒæ•¸
        total_query = select(func.count(Log.id))
        total_result = await db.execute(total_query)
        total_logs = total_result.scalar()

        # æŒ‰ç­‰ç´šçµ±è¨ˆ
        level_query = (
            select(Log.log_level, func.count(Log.id))
            .group_by(Log.log_level)
        )
        level_result = await db.execute(level_query)
        logs_by_level = {row[0]: row[1] for row in level_result}

        # æœ€è¿‘æ´»èºçš„è¨­å‚™
        # åŸå§‹å¯«æ³•æœ‰èª¤ï¼šSELECT DISTINCT éœ€è¦ ORDER BY æ¬„ä½ä¹Ÿåœ¨ SELECT ä¸­
        # device_query = (
        #     select(Log.device_id)
        #     .distinct()
        #     .order_by(Log.created_at.desc())
        #     .limit(10)
        # )
        device_query = (
            select(Log.device_id, func.max(Log.created_at).label('last_activity'))
            .group_by(Log.device_id)
            .order_by(func.max(Log.created_at).desc())
            .limit(10)
        )
        device_result = await db.execute(device_query)
        recent_devices = [row[0] for row in device_result]

        stats = {
            "total_logs": total_logs or 0,
            "logs_by_level": logs_by_level,
            "recent_devices": recent_devices
        }

        # å¯«å…¥å¿«å–ï¼ˆTTL 60ç§’ï¼‰
        try:
            await redis_client.setex(
                name=cache_key,
                time=60,
                value=json.dumps(stats)
            )
        except Exception as e:
            print(f"çµ±è¨ˆå¿«å–å¯«å…¥å¤±æ•—: {e}")

        return StatsResponse(**stats)

    except Exception as e:
        print(f"çµ±è¨ˆæŸ¥è©¢å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get stats: {str(e)}")

# ==========================================
# API ç«¯é» - æ ¹è·¯å¾‘
# ==========================================
@app.get("/metrics")
async def metrics():
    """Prometheus metrics ç«¯é»"""
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )


@app.get("/")
async def root():
    """
    API æ ¹è·¯å¾‘
    """
    return {
        "service": "é«˜æ•ˆèƒ½æ—¥èªŒæ”¶é›†ç³»çµ±",
        "version": "1.0.0",
        "instance": INSTANCE_NAME,
        "endpoints": {
            "health": "/health",
            "create_log": "POST /api/log",
            "get_logs": "GET /api/logs/{device_id}",
            "stats": "GET /api/stats",
            "metrics": "/metrics",
            "docs": "/docs"
        }
    }

# ==========================================
# å…¨åŸŸä¾‹å¤–è™•ç†
# ==========================================
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """
    å…¨åŸŸä¾‹å¤–è™•ç†å™¨
    """
    print(f"æœªè™•ç†çš„ä¾‹å¤–: {exc}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal Server Error",
            "detail": str(exc),
            "instance": INSTANCE_NAME
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        workers=1
    )
