"""
å°ç…§çµ„ - ç°¡åŒ–ç‰ˆ FastAPI æ‡‰ç”¨
ç›´æ¥å¯«å…¥ PostgreSQLï¼Œç„¡è² è¼‰å¹³è¡¡ã€é€£æ¥æ± ã€Redisã€Worker
"""
import os
import time
from datetime import datetime
from typing import Optional
from zoneinfo import ZoneInfo
from fastapi import FastAPI, HTTPException, Response
from pydantic import BaseModel, Field
import psycopg2
from psycopg2.extras import Json
# æ–°å¢: Prometheus ç›£æ§æ”¯æ´
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
import psutil

# ==========================================
# æ‡‰ç”¨ç¨‹å¼åˆå§‹åŒ–
# ==========================================
app = FastAPI(
    title="å°ç…§çµ„ - ç°¡åŒ–æ—¥èªŒæ”¶é›†ç³»çµ±",
    description="ç›´æ¥å¯«å…¥ PostgreSQLï¼Œç„¡å„ªåŒ–æ©Ÿåˆ¶",
    version="1.0.0"
)

# ==========================================
# Prometheus ç›£æ§æŒ‡æ¨™å®šç¾©
# ==========================================
# HTTP è«‹æ±‚ç›¸é—œ
http_requests_total = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)
http_request_duration_seconds = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

# æ—¥èªŒæ¥­å‹™æŒ‡æ¨™
logs_received_total = Counter(
    'logs_received_total',
    'Total logs received',
    ['device_id', 'log_level']
)
logs_processing_errors_total = Counter(
    'logs_processing_errors_total',
    'Total log processing errors',
    ['error_type']
)
batch_processing_duration_seconds = Histogram(
    'batch_processing_duration_seconds',
    'Batch processing duration',
    ['batch_size']
)

# PostgreSQL æŒ‡æ¨™
postgres_connection_duration_seconds = Histogram(
    'postgres_connection_duration_seconds',
    'PostgreSQL connection duration'
)
postgres_query_duration_seconds = Histogram(
    'postgres_query_duration_seconds',
    'PostgreSQL query duration',
    ['operation']
)

# ç³»çµ±è³‡æºæŒ‡æ¨™
system_cpu_usage_percent = Gauge(
    'system_cpu_usage_percent',
    'System CPU usage percentage'
)
system_memory_usage_bytes = Gauge(
    'system_memory_usage_bytes',
    'System memory usage in bytes',
    ['type']
)
system_disk_usage_bytes = Gauge(
    'system_disk_usage_bytes',
    'System disk usage in bytes',
    ['type']
)

# ==========================================
# æ›´æ–°ç³»çµ±æŒ‡æ¨™å‡½æ•¸
# ==========================================
def update_system_metrics():
    """æ›´æ–°ç³»çµ±è³‡æºæŒ‡æ¨™"""
    try:
        # CPU ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=0.1)
        system_cpu_usage_percent.set(cpu_percent)

        # è¨˜æ†¶é«”ä½¿ç”¨
        memory = psutil.virtual_memory()
        system_memory_usage_bytes.labels(type='used').set(memory.used)
        system_memory_usage_bytes.labels(type='available').set(memory.available)
        system_memory_usage_bytes.labels(type='total').set(memory.total)

        # ç£ç¢Ÿä½¿ç”¨
        disk = psutil.disk_usage('/')
        system_disk_usage_bytes.labels(type='used').set(disk.used)
        system_disk_usage_bytes.labels(type='free').set(disk.free)
        system_disk_usage_bytes.labels(type='total').set(disk.total)
    except Exception as e:
        print(f"æ›´æ–°ç³»çµ±æŒ‡æ¨™å¤±æ•—: {e}")

# ==========================================
# PostgreSQL é…ç½®ï¼ˆç„¡é€£æ¥æ± ï¼‰
# ==========================================
POSTGRES_HOST = os.getenv('POSTGRES_HOST', 'localhost')
POSTGRES_PORT = os.getenv('POSTGRES_PORT', '5432')
POSTGRES_USER = os.getenv('POSTGRES_USER', 'loguser')
POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'logpass')
POSTGRES_DB = os.getenv('POSTGRES_DB', 'logsdb')

# ==========================================
# Pydantic æ¨¡å‹
# ==========================================
class LogEntryRequest(BaseModel):
    """æ—¥èªŒè«‹æ±‚æ¨¡å‹"""
    device_id: str = Field(..., min_length=1, max_length=50)
    log_level: str
    message: str = Field(..., min_length=1, max_length=5000)
    log_data: Optional[dict] = Field(default={})

class BatchLogEntryRequest(BaseModel):
    """æ‰¹é‡æ—¥èªŒè«‹æ±‚æ¨¡å‹"""
    logs: list[LogEntryRequest] = Field(..., min_length=1, max_length=1000)

class LogEntryResponse(BaseModel):
    """æ—¥èªŒå›æ‡‰æ¨¡å‹"""
    status: str
    log_id: Optional[int] = None
    received_at: datetime

class BatchLogEntryResponse(BaseModel):
    """æ‰¹é‡æ—¥èªŒå›æ‡‰æ¨¡å‹"""
    status: str
    count: int
    received_at: datetime

# ==========================================
# è³‡æ–™åº«é€£ç·šå‡½æ•¸ï¼ˆæ¯æ¬¡è«‹æ±‚å‰µå»ºæ–°é€£ç·šï¼‰
# ==========================================
def get_db_connection():
    """
    å‰µå»ºæ–°çš„è³‡æ–™åº«é€£ç·šï¼ˆç„¡é€£æ¥æ± ï¼‰
    æ¯æ¬¡è«‹æ±‚éƒ½æœƒå‰µå»ºå’Œé—œé–‰é€£ç·š
    """
    return psycopg2.connect(
        host=POSTGRES_HOST,
        port=POSTGRES_PORT,
        user=POSTGRES_USER,
        password=POSTGRES_PASSWORD,
        database=POSTGRES_DB
    )

# ==========================================
# API ç«¯é» - å–®ç­†æ—¥èªŒï¼ˆç›´æ¥å¯«å…¥ï¼‰
# ==========================================
@app.post("/api/log", response_model=LogEntryResponse)
async def create_log(log: LogEntryRequest):
    """
    æ¥æ”¶æ—¥èªŒä¸¦ç›´æ¥å¯«å…¥ PostgreSQL

    æµç¨‹ï¼š
    1. é©—è­‰æ—¥èªŒæ ¼å¼
    2. å‰µå»ºè³‡æ–™åº«é€£ç·š
    3. ç›´æ¥ INSERT åˆ°è³‡æ–™åº«
    4. é—œé–‰é€£ç·š
    5. è¿”å›çµæœ

    ç„¡å„ªåŒ–ï¼šæ¯æ¬¡è«‹æ±‚éƒ½å‰µå»ºæ–°é€£ç·š
    """
    # è¨˜éŒ„æ¥­å‹™æŒ‡æ¨™
    logs_received_total.labels(
        device_id=log.device_id,
        log_level=log.log_level
    ).inc()

    # è¨˜éŒ„è«‹æ±‚æ™‚é–“
    start_time = time.time()

    try:
        # æ¯æ¬¡è«‹æ±‚å‰µå»ºæ–°é€£ç·šï¼ˆç„¡é€£æ¥æ± ï¼‰
        # è¨˜éŒ„é€£ç·šæ™‚é–“
        conn_start = time.time()
        conn = get_db_connection()
        postgres_connection_duration_seconds.observe(time.time() - conn_start)

        cursor = conn.cursor()

        # ç›´æ¥ INSERT åˆ°è³‡æ–™åº«
        insert_query = """
            INSERT INTO logs (device_id, log_level, message, log_data, created_at)
            VALUES (%s, %s, %s, %s, %s)
            RETURNING id
        """

        # è¨˜éŒ„æŸ¥è©¢æ™‚é–“
        query_start = time.time()
        cursor.execute(
            insert_query,
            (
                log.device_id,
                log.log_level,
                log.message,
                Json(log.log_data) if log.log_data else Json({}),
                datetime.now(ZoneInfo("Asia/Taipei"))
            )
        )
        postgres_query_duration_seconds.labels(operation='insert').observe(time.time() - query_start)

        log_id = cursor.fetchone()[0]

        # æäº¤äº‹å‹™
        conn.commit()

        # é—œé–‰é€£ç·š
        cursor.close()
        conn.close()

        # è¨˜éŒ„HTTPè«‹æ±‚æŒ‡æ¨™
        http_requests_total.labels(method='POST', endpoint='/api/log', status='200').inc()
        http_request_duration_seconds.labels(method='POST', endpoint='/api/log').observe(time.time() - start_time)

        return LogEntryResponse(
            status="saved",
            log_id=log_id,
            received_at=datetime.now()
        )

    except Exception as e:
        logs_processing_errors_total.labels(error_type='database_write').inc()
        http_requests_total.labels(method='POST', endpoint='/api/log', status='500').inc()
        print(f"å¯«å…¥è³‡æ–™åº«å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save log: {str(e)}")

# ==========================================
# API ç«¯é» - æ‰¹é‡æ—¥èªŒï¼ˆç›´æ¥å¯«å…¥ï¼‰
# ==========================================
@app.post("/api/logs/batch", response_model=BatchLogEntryResponse)
async def create_batch_logs(batch: BatchLogEntryRequest):
    """
    æ‰¹é‡æ¥æ”¶æ—¥èªŒä¸¦ç›´æ¥å¯«å…¥ PostgreSQL

    æµç¨‹ï¼š
    1. é©—è­‰æ—¥èªŒæ ¼å¼
    2. å‰µå»ºè³‡æ–™åº«é€£ç·š
    3. ä½¿ç”¨ executemany æ‰¹é‡æ’å…¥
    4. é—œé–‰é€£ç·š
    5. è¿”å›çµæœ

    ç„¡å„ªåŒ–ï¼šæ¯æ¬¡è«‹æ±‚éƒ½å‰µå»ºæ–°é€£ç·šï¼Œç„¡éåŒæ­¥è™•ç†
    """
    batch_size = len(batch.logs)
    start_time = time.time()

    try:
        # è¨˜éŒ„æ¥­å‹™æŒ‡æ¨™
        for log in batch.logs:
            logs_received_total.labels(
                device_id=log.device_id,
                log_level=log.log_level
            ).inc()

        # æ¯æ¬¡è«‹æ±‚å‰µå»ºæ–°é€£ç·šï¼ˆç„¡é€£æ¥æ± ï¼‰
        conn_start = time.time()
        conn = get_db_connection()
        postgres_connection_duration_seconds.observe(time.time() - conn_start)

        cursor = conn.cursor()

        # æº–å‚™æ‰¹é‡æ’å…¥çš„è³‡æ–™
        current_time = datetime.now(ZoneInfo("Asia/Taipei"))
        values = [
            (
                log.device_id,
                log.log_level,
                log.message,
                Json(log.log_data) if log.log_data else Json({}),
                current_time
            )
            for log in batch.logs
        ]

        # æ‰¹é‡ INSERT
        insert_query = """
            INSERT INTO logs (device_id, log_level, message, log_data, created_at)
            VALUES (%s, %s, %s, %s, %s)
        """

        # è¨˜éŒ„æŸ¥è©¢æ™‚é–“
        query_start = time.time()
        cursor.executemany(insert_query, values)
        postgres_query_duration_seconds.labels(operation='batch_insert').observe(time.time() - query_start)

        # æäº¤äº‹å‹™
        conn.commit()

        # é—œé–‰é€£ç·š
        cursor.close()
        conn.close()

        # è¨˜éŒ„æ‰¹é‡è™•ç†æ™‚é–“
        duration = time.time() - start_time
        batch_processing_duration_seconds.labels(batch_size=str(batch_size)).observe(duration)

        # è¨˜éŒ„HTTPè«‹æ±‚æŒ‡æ¨™
        http_requests_total.labels(method='POST', endpoint='/api/logs/batch', status='200').inc()
        http_request_duration_seconds.labels(method='POST', endpoint='/api/logs/batch').observe(duration)

        return BatchLogEntryResponse(
            status="saved",
            count=batch_size,
            received_at=datetime.now()
        )

    except Exception as e:
        logs_processing_errors_total.labels(error_type='batch_database_write').inc()
        http_requests_total.labels(method='POST', endpoint='/api/logs/batch', status='500').inc()
        print(f"æ‰¹é‡å¯«å…¥è³‡æ–™åº«å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to save batch logs: {str(e)}")

# ==========================================
# API ç«¯é» - å¥åº·æª¢æŸ¥
# ==========================================
@app.get("/health")
async def health_check():
    """ç°¡å–®çš„å¥åº·æª¢æŸ¥"""
    try:
        conn = get_db_connection()
        conn.close()
        return {
            "status": "healthy",
            "instance": "simple-fastapi",
            "checks": {"postgres": True}
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "instance": "simple-fastapi",
            "checks": {"postgres": False},
            "error": str(e)
        }

# ==========================================
# API ç«¯é» - Prometheus Metrics
# ==========================================
@app.get("/metrics")
async def metrics():
    """Prometheus metrics ç«¯é»"""
    # æ›´æ–°ç³»çµ±æŒ‡æ¨™
    update_system_metrics()
    return Response(
        content=generate_latest(),
        media_type=CONTENT_TYPE_LATEST
    )

# ==========================================
# API ç«¯é» - æ ¹è·¯å¾‘
# ==========================================
@app.get("/")
async def root():
    """API æ ¹è·¯å¾‘"""
    return {
        "service": "å°ç…§çµ„ - ç°¡åŒ–æ—¥èªŒæ”¶é›†ç³»çµ±",
        "version": "1.0.0",
        "description": "ç›´æ¥å¯«å…¥ PostgreSQLï¼Œç„¡è² è¼‰å¹³è¡¡ã€é€£æ¥æ± ã€Redisã€Worker",
        "endpoints": {
            "health": "/health",
            "create_log": "POST /api/log",
            "create_batch_logs": "POST /api/logs/batch",
            "metrics": "/metrics",
            "docs": "/docs"
        }
    }

# ==========================================
# æ‡‰ç”¨ç¨‹å¼ç”Ÿå‘½é€±æœŸ
# ==========================================
@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚åŸ·è¡Œ"""
    print("ğŸš€ å•Ÿå‹•å°ç…§çµ„ FastAPI å¯¦ä¾‹")
    # åˆå§‹åŒ–ç³»çµ±æŒ‡æ¨™
    update_system_metrics()
    print("âœ… ç³»çµ±æŒ‡æ¨™ç›£æ§å·²å•Ÿå‹•")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main_simple:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        workers=1
    )
